---
description: Configura CLAUDE.md do projeto com padrÃµes de testes Python, frameworks, mocks e estrutura de diretÃ³rios
---

# Setup Project for Python Testing

Este comando configura o arquivo `CLAUDE.md` do projeto atual com instruÃ§Ãµes sobre testes Python, padrÃµes de mock, fixtures e convenÃ§Ãµes de teste.

## ğŸ¯ Objetivo

Adicionar ao `CLAUDE.md` do projeto instruÃ§Ãµes claras para que Claude:
- Gere testes Python seguindo padrÃµes do projeto
- Utilize frameworks de teste corretos (pytest, unittest, nose)
- Crie mocks adequados (LangChain, FastAPI, Django, AWS, etc.)
- Reutilize fixtures existentes (conftest.py)
- Siga estrutura de diretÃ³rios de testes do projeto
- Aplique AAA pattern (Arrange-Act-Assert)
- Garanta testes paralelos seguros (pytest-xdist)

## ğŸ“‹ Como usar

```bash
/setup-project-tests
```

Ou com descriÃ§Ã£o da stack Python:

```bash
/setup-project-tests "API FastAPI com LangChain + PostgreSQL"
```

## ğŸ” Processo de ExecuÃ§Ã£o

### 1. Detectar ou Criar CLAUDE.md

**Se CLAUDE.md existe**:
- Ler arquivo atual
- Adicionar seÃ§Ã£o "Python Testing Standards" ao final
- Preservar conteÃºdo existente

**Se CLAUDE.md NÃƒO existe**:
- Criar arquivo na raiz do projeto
- Adicionar template completo de padrÃµes de testes Python

### 2. Adicionar InstruÃ§Ãµes de Testes Python

O comando deve adicionar a seguinte seÃ§Ã£o ao `CLAUDE.md`:

```markdown
# Python Testing Standards

**IMPORTANTE**: Este projeto utiliza o plugin `python-test-generator` para criaÃ§Ã£o automÃ¡tica de testes Python com padrÃµes consistentes.

## ğŸ“‹ PadrÃµes de Testes

### âœ… Framework de Testes

**Framework Principal**: [pytest/unittest/nose - detectado automaticamente]

**Bibliotecas de Teste**:
- Testing framework: pytest/unittest
- Mocking: pytest-mock, unittest.mock
- Coverage: pytest-cov
- Async: pytest-asyncio (se aplicÃ¡vel)
- Parallel: pytest-xdist (se aplicÃ¡vel)

### ğŸ“ Estrutura de DiretÃ³rios

```
projeto/
â”œâ”€â”€ src/                    # CÃ³digo fonte
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ services/
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ tests/                  # Testes
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py        # Fixtures globais
â”‚   â”œâ”€â”€ unit/              # Testes unitÃ¡rios
â”‚   â”‚   â”œâ”€â”€ test_models.py
â”‚   â”‚   â”œâ”€â”€ test_services.py
â”‚   â”‚   â””â”€â”€ test_utils.py
â”‚   â”œâ”€â”€ integration/       # Testes de integraÃ§Ã£o
â”‚   â””â”€â”€ e2e/              # Testes end-to-end
â””â”€â”€ pytest.ini            # ConfiguraÃ§Ã£o pytest
```

### ğŸ¯ ConvenÃ§Ãµes de Nomenclatura

**Arquivos de Teste**:
- Pattern: `test_*.py` ou `*_test.py`
- Espelhar estrutura do cÃ³digo: `src/services/user.py` â†’ `tests/unit/test_services_user.py`

**FunÃ§Ãµes de Teste**:
- Pattern: `test_<funcao>_<cenario>()`
- Exemplos:
  - `test_calculate_discount_valid_input()`
  - `test_calculate_discount_invalid_percentage()`
  - `test_calculate_discount_zero_price()`

**Classes de Teste**:
- Pattern: `Test<NomeDaClasse>`
- Exemplo: `TestUserService`, `TestOrderModel`

### ğŸ“ AAA Pattern (Arrange-Act-Assert)

**Sempre seguir AAA pattern**:

```python
def test_process_order_with_valid_data():
    """Deve processar pedido vÃ¡lido com sucesso"""
    # Arrange - Preparar dados e mocks
    order_data = {"id": 1, "total": 100}
    mock_db = MagicMock()
    mock_db.save.return_value = True

    # Act - Executar aÃ§Ã£o
    result = process_order(order_data, db=mock_db)

    # Assert - Validar resultado
    assert result.status == "success"
    assert result.order_id == 1
    mock_db.save.assert_called_once_with(order_data)
```

### ğŸ­ PadrÃµes de Mock

#### Mock de APIs Externas

```python
@pytest.fixture
def mock_external_api(mocker):
    """Mock de API externa"""
    mock = mocker.patch('module.external_api.call')
    mock.return_value = {"status": "ok", "data": []}
    return mock

def test_fetch_data_from_api(mock_external_api):
    """Deve buscar dados da API mockada"""
    # Arrange
    mock_external_api.return_value = {"data": [1, 2, 3]}

    # Act
    result = fetch_data()

    # Assert
    assert len(result) == 3
    mock_external_api.assert_called_once()
```

#### Mock de Banco de Dados

```python
@pytest.fixture
def mock_db_session(mocker):
    """Mock de sessÃ£o de banco de dados"""
    mock_session = mocker.MagicMock()
    mock_session.query.return_value.filter.return_value.first.return_value = None
    return mock_session

def test_create_user_new_email(mock_db_session):
    """Deve criar usuÃ¡rio com email novo"""
    # Arrange
    user_data = {"email": "test@example.com", "name": "Test"}

    # Act
    result = create_user(user_data, db=mock_db_session)

    # Assert
    assert result.email == "test@example.com"
    mock_db_session.add.assert_called_once()
    mock_db_session.commit.assert_called_once()
```

#### Mock de VariÃ¡veis Module-Level

```python
# âš ï¸ IMPORTANTE: VariÃ¡veis module-level requerem patch especial

# src/config.py
MAX_RETRIES = 3  # VariÃ¡vel module-level

# tests/test_config.py
@patch("src.config.MAX_RETRIES", 5)  # âœ… Correto
def test_with_custom_retries():
    from src.config import MAX_RETRIES
    assert MAX_RETRIES == 5

# âŒ ERRADO: NÃ£o funciona com variÃ¡veis module-level
@patch.dict(os.environ, {"MAX_RETRIES": "5"})
def test_wrong_approach():
    # Isso NÃƒO altera a variÃ¡vel module-level
    pass
```

### ğŸ”— PadrÃµes EspecÃ­ficos por Framework

#### LangChain / LangGraph

**Mock de Chains com Pipe Operators**:

```python
# âš ï¸ CRÃTICO: Cada | requer um __or__ mock

def test_langchain_chain_with_pipes(mocker):
    """Deve executar chain com mÃºltiplos pipes"""
    # Arrange - Mock CADA componente
    mock_prompt = mocker.MagicMock()
    mock_llm = mocker.MagicMock()
    mock_parser = mocker.MagicMock()

    # Mock do pipe operator (|)
    mock_prompt.__or__ = mocker.MagicMock(return_value=mock_llm)
    mock_llm.__or__ = mocker.MagicMock(return_value=mock_parser)

    # Mock do invoke
    mock_parser.invoke.return_value = {"output": "result"}

    # Act
    chain = mock_prompt | mock_llm | mock_parser
    result = chain.invoke({"input": "test"})

    # Assert
    assert result["output"] == "result"
```

**Mock de LangGraph StateGraph**:

```python
@pytest.fixture
def mock_state_graph(mocker):
    """Mock de StateGraph do LangGraph"""
    mock_graph = mocker.MagicMock()
    mock_graph.invoke.return_value = {"final_state": "completed"}
    return mock_graph

def test_graph_execution(mock_state_graph):
    """Deve executar graph com sucesso"""
    result = mock_state_graph.invoke({"initial_state": "start"})
    assert result["final_state"] == "completed"
```

#### FastAPI

**Mock de DependÃªncias**:

```python
from fastapi.testclient import TestClient

def override_get_db():
    """Override de dependÃªncia de DB"""
    db = MagicMock()
    yield db

app.dependency_overrides[get_db] = override_get_db

def test_create_item_endpoint():
    """Deve criar item via endpoint"""
    client = TestClient(app)
    response = client.post("/items", json={"name": "test"})
    assert response.status_code == 201
```

#### AWS / Boto3

**Mock de ServiÃ§os AWS**:

```python
@pytest.fixture
def mock_s3_client(mocker):
    """Mock de cliente S3"""
    mock_client = mocker.MagicMock()
    mock_client.upload_file.return_value = None
    mock_client.download_file.return_value = None
    mocker.patch('boto3.client', return_value=mock_client)
    return mock_client

def test_upload_to_s3(mock_s3_client):
    """Deve fazer upload para S3"""
    upload_to_s3("file.txt", "bucket", "key")
    mock_s3_client.upload_file.assert_called_once()
```

### ğŸ§ª Fixtures e ReutilizaÃ§Ã£o

**conftest.py - Fixtures Globais**:

```python
# tests/conftest.py

import pytest

@pytest.fixture
def sample_user():
    """UsuÃ¡rio de exemplo para testes"""
    return {
        "id": 1,
        "email": "test@example.com",
        "name": "Test User"
    }

@pytest.fixture
def mock_db_session(mocker):
    """Mock de sessÃ£o de banco de dados"""
    return mocker.MagicMock()

@pytest.fixture(autouse=True)
def reset_global_state():
    """Reset automÃ¡tico de estado global"""
    # Setup
    yield
    # Teardown
    # Limpar cache, variÃ¡veis globais, etc.
```

**Reutilizar Fixtures**:

```python
def test_create_user(sample_user, mock_db_session):
    """Deve criar usuÃ¡rio usando fixtures reutilizÃ¡veis"""
    result = create_user(sample_user, db=mock_db_session)
    assert result.email == sample_user["email"]
```

### ğŸ”„ Testes Paralelos (pytest-xdist)

**Garantir seguranÃ§a em execuÃ§Ã£o paralela**:

```python
# âœ… BOM: Cada teste independente
def test_isolated_function():
    result = pure_function(input_data)
    assert result == expected

# âŒ RUIM: Compartilha estado global
global_counter = 0

def test_with_global_state():
    global global_counter
    global_counter += 1  # âš ï¸ Race condition!
    assert global_counter == 1

# âœ… CORRETO: Usar fixtures autouse para isolar
@pytest.fixture(autouse=True)
def reset_counter():
    global global_counter
    global_counter = 0
    yield
    global_counter = 0
```

### ğŸ§¹ Cleanup de Recursos

**Sempre fazer cleanup de recursos**:

```python
@pytest.fixture
def db_connection():
    """ConexÃ£o de DB com cleanup automÃ¡tico"""
    # Setup
    conn = create_connection()
    yield conn
    # Teardown
    conn.close()  # âœ… SEMPRE fechar

@pytest.fixture
def temp_file():
    """Arquivo temporÃ¡rio com cleanup"""
    # Setup
    file_path = "/tmp/test_file.txt"
    with open(file_path, "w") as f:
        f.write("test")
    yield file_path
    # Teardown
    if os.path.exists(file_path):
        os.remove(file_path)  # âœ… SEMPRE remover
```

### ğŸ“Š Cobertura de Testes

**Threshold**: â‰¥ 80% (configurÃ¡vel)

**Executar com cobertura**:

```bash
# Executar testes com relatÃ³rio de cobertura
pytest --cov=src --cov-report=term-missing

# Gerar relatÃ³rio HTML
pytest --cov=src --cov-report=html

# Falhar se cobertura < 80%
pytest --cov=src --cov-fail-under=80
```

**pytest.ini**:

```ini
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts =
    --cov=src
    --cov-report=term-missing
    --cov-fail-under=80
    -v
```

## âš™ï¸ ConfiguraÃ§Ã£o Pytest

**RECOMENDAÃ‡ÃƒO**: ApÃ³s configurar CLAUDE.md, execute `/setup-pytest-config` para configurar pytest automaticamente.

```bash
# Configurar pytest (pyproject.toml ou pytest.ini)
/setup-pytest-config
```

Este comando:
- âœ… Cria/atualiza `[tool.pytest.ini_options]` em pyproject.toml (preferencial)
- âœ… Cria `pytest.ini` se pyproject.toml nÃ£o existir
- âœ… Configura coverage, parallel, markers automaticamente
- âœ… Detecta stack Python e customiza configuraÃ§Ã£o

**Exemplo de configuraÃ§Ã£o gerada**:

```toml
# pyproject.toml (PREFERENCIAL)
[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]

addopts = [
    "--cov=src",
    "--cov-report=term-missing",
    "--cov-report=html",
    "--cov-fail-under=80",
    "-v",
    "-n auto",  # Parallel com pytest-xdist
]

markers = [
    "unit: Unit tests",
    "integration: Integration tests",
    "slow: Slow tests",
]

asyncio_mode = "auto"  # Se async detectado
```

```ini
# pytest.ini (FALLBACK - sÃ³ se pyproject.toml nÃ£o existir)
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

addopts =
    --cov=src
    --cov-report=term-missing
    --cov-fail-under=80
    -v
    -n auto

markers =
    unit: Unit tests
    integration: Integration tests

asyncio_mode = auto
```

**Ordem de Prioridade**:
1. âœ… **pyproject.toml** (recomendado - padrÃ£o moderno Python PEP 518)
2. pytest.ini (fallback - sÃ³ se pyproject.toml nÃ£o existir)
3. setup.cfg (legado)

## ğŸ¯ Plugin Python Test Generator

Este projeto usa o plugin `python-test-generator` com os seguintes recursos:

**Comandos**:
- `/py-test` - Gera testes Python automaticamente em paralelo
- `/setup-pytest-config` - Configura pytest (pyproject.toml ou pytest.ini)
- `/setup-project-tests` - Configura CLAUDE.md com padrÃµes de testes

**Agente**:
- `test-assistant` - Especialista em testes Python com mocks avanÃ§ados

**Skill**:
- `langchain-test-specialist` - Expertise em mocks de LangChain/LangGraph

**PadrÃµes AvanÃ§ados**:
- Mock de chains LangChain com pipe operators (`|`)
- Mock de variÃ¡veis module-level
- Fixtures reutilizÃ¡veis (conftest.py)
- Testes paralelos seguros (pytest-xdist)
- Cleanup automÃ¡tico de recursos
- AAA pattern (Arrange-Act-Assert)

**Performance**:
- âš¡ CriaÃ§Ã£o paralela de mÃºltiplos arquivos de teste
- ğŸ“Š AtÃ© 80% mais rÃ¡pido que criaÃ§Ã£o sequencial
- ğŸ”„ ValidaÃ§Ã£o automÃ¡tica de cobertura

---

**Filosofia**: Testes RÃ¡pidos > Testes Lentos | Mocks > Chamadas Reais | AAA Pattern > CÃ³digo Confuso
```

### 3. Adicionar Contexto do Projeto (Se Fornecido)

Se o usuÃ¡rio fornecer descriÃ§Ã£o da stack, adicionar seÃ§Ã£o customizada:

```markdown
## ğŸ“Š Contexto Deste Projeto

**Stack Python**: [stack fornecida pelo usuÃ¡rio]

**Frameworks de Teste Recomendados**:
- Framework principal: [pytest/unittest]
- Mocking: [pytest-mock/unittest.mock]
- Coverage: [pytest-cov/coverage.py]
- Async: [pytest-asyncio]

**Mocks EspecÃ­ficos da Stack**:
- [LangChain]: Mock de chains, agents, tools
- [FastAPI]: Mock de dependÃªncias, TestClient
- [Django]: Mock de ORM, views, middleware
- [AWS]: Mock de boto3, serviÃ§os AWS

**Fixtures Recomendadas**:
- Fixtures de banco de dados
- Fixtures de autenticaÃ§Ã£o
- Fixtures de APIs externas
```

### 4. Detectar Stack Python do Projeto

Analisar projeto para customizar instruÃ§Ãµes:

- Verificar `requirements.txt`, `pyproject.toml`, `Pipfile`
- Detectar frameworks: FastAPI, Django, Flask, LangChain, LangGraph
- Identificar gerenciador de pacotes: pip, poetry, pipenv, uv
- Localizar `conftest.py` e fixtures existentes
- Verificar `pytest.ini` ou `setup.cfg`

**Adicionar ao CLAUDE.md**:

```markdown
## ğŸ”§ Stack Python Detectada

**Gerenciador de Pacotes**: [pip/poetry/pipenv/uv]
**Framework de Testes**: [pytest/unittest/nose]
**Framework Web**: [FastAPI/Django/Flask]
**Bibliotecas Especiais**: [LangChain, boto3, sqlalchemy, etc.]

**Fixtures Existentes** (conftest.py):
- [listar fixtures encontradas]

**PadrÃµes de Mock Detectados**:
- [analisar testes existentes]
```

### 5. Confirmar com UsuÃ¡rio

Mostrar preview do que serÃ¡ adicionado:

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“ SETUP PYTHON TESTING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Arquivo: CLAUDE.md

AÃ§Ã£o: [CRIAR NOVO / ADICIONAR SEÃ‡ÃƒO]

Stack Python Detectada:
- Gerenciador: [poetry/pip/pipenv/uv]
- Framework: [pytest/unittest]
- Web Framework: [FastAPI/Django/Flask]
- Bibliotecas: [LangChain, boto3, etc.]

Fixtures Existentes:
- conftest.py encontrado
- [X] fixtures detectadas

ConteÃºdo a ser adicionado:
---
[Preview das instruÃ§Ãµes]
---

Adicionar ao CLAUDE.md? (s/n)
```

### 6. Criar/Atualizar Arquivo

Se usuÃ¡rio confirmar:
- Criar ou atualizar CLAUDE.md
- Adicionar instruÃ§Ãµes completas
- Preservar conteÃºdo existente (NUNCA sobrescrever)
- Validar que arquivo foi criado corretamente

```
âœ… CLAUDE.md configurado com sucesso!

InstruÃ§Ãµes de testes Python adicionadas.

Stack Python detectada:
- Gerenciador: poetry
- Framework: pytest
- Web Framework: FastAPI
- Bibliotecas: LangChain, boto3, sqlalchemy

Fixtures encontradas:
- conftest.py com 5 fixtures

PrÃ³ximos passos:
1. Revisar CLAUDE.md
2. Customizar fixtures (se necessÃ¡rio)
3. Executar: /py-test
4. Validar cobertura: pytest --cov

Claude agora estÃ¡ orientado a:
âœ“ Gerar testes Python seguindo padrÃµes do projeto
âœ“ Reutilizar fixtures existentes (conftest.py)
âœ“ Criar mocks adequados (LangChain, FastAPI, AWS)
âœ“ Aplicar AAA pattern consistentemente
âœ“ Garantir testes paralelos seguros
```

## ğŸ“š Exemplos de Uso

### Exemplo 1: Novo Projeto Python

```bash
/setup-project-tests "API FastAPI com PostgreSQL"
```

**Resultado**:
- Cria `CLAUDE.md` na raiz do projeto
- Adiciona padrÃµes pytest + FastAPI
- Configura mocks de banco de dados
- Define estrutura de diretÃ³rios de testes
- Orienta sobre TestClient FastAPI

### Exemplo 2: Projeto LangChain Existente

```bash
/setup-project-tests "LangChain + LangGraph com OpenAI"
```

**Resultado**:
- Detecta `CLAUDE.md` existente
- Adiciona seÃ§Ã£o de testes ao final
- Preserva conteÃºdo existente
- Inclui padrÃµes de mock LangChain (pipe operators)
- Configura mock de chains e agents

### Exemplo 3: Projeto Django

```bash
/setup-project-tests "Django REST Framework + Celery"
```

**Resultado**:
- Adiciona padrÃµes de teste Django
- Configura fixtures de banco de dados Django
- Orienta sobre mock de tasks Celery
- Define testes de views/serializers/models

## âš ï¸ Importante

### NÃ£o Sobrescrever ConteÃºdo Existente

Se `CLAUDE.md` jÃ¡ existe:
- âŒ NUNCA sobrescrever conteÃºdo
- âœ… SEMPRE adicionar ao final
- âœ… Usar separador claro: `---`

### Detectar Stack Python

Analisar projeto para customizar instruÃ§Ãµes:
- Verificar `requirements.txt`, `pyproject.toml`, `Pipfile`
- Detectar frameworks de teste em uso
- Identificar bibliotecas que requerem mocks especiais
- Localizar fixtures existentes (conftest.py)

### Validar Sintaxe Markdown

ApÃ³s criar/atualizar:
- Verificar que markdown estÃ¡ vÃ¡lido
- Headers bem formatados
- Code blocks Python com syntax highlighting
- Links funcionando

## ğŸš€ ApÃ³s Executar Este Comando

O usuÃ¡rio terÃ¡:

1. âœ… `CLAUDE.md` configurado com padrÃµes de testes Python
2. âœ… Claude orientado a seguir AAA pattern
3. âœ… Mocks adequados para frameworks usados
4. âœ… ReutilizaÃ§Ã£o de fixtures existentes
5. âœ… Estrutura de diretÃ³rios padronizada

**PrÃ³ximo passo**: Executar `/py-test` para gerar testes automaticamente!

## ğŸ’¡ Dica

ApÃ³s configurar o projeto, sempre valide cobertura de testes:

```bash
# Gerar testes automaticamente
/py-test

# Executar com cobertura
pytest --cov=src --cov-report=term-missing

# Verificar que cobertura â‰¥ 80%
pytest --cov-fail-under=80
```

Isso garantirÃ¡ qualidade consistente e detecÃ§Ã£o precoce de bugs.