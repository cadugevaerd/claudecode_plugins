# ğŸ Python Test Generator

Plugin de geraÃ§Ã£o automÃ¡tica de testes unitÃ¡rios Python com anÃ¡lise de cobertura e **criaÃ§Ã£o paralela** de arquivos para mÃ¡xima performance.

## ğŸ“‹ DescriÃ§Ã£o

O **Python Test Generator** Ã© um plugin especializado que analisa a cobertura de testes do seu projeto Python e **cria automaticamente mÃºltiplos testes em paralelo**, gerando testes unitÃ¡rios completos, bem estruturados e com alta qualidade, seguindo os padrÃµes e frameworks jÃ¡ utilizados no projeto.

### ğŸ¯ Principais Recursos

- âš¡ **CriaÃ§Ã£o Paralela**: Gera mÃºltiplos arquivos de teste simultaneamente (atÃ© 80% mais rÃ¡pido)
- âœ… **DetecÃ§Ã£o AutomÃ¡tica Python**: Identifica frameworks, estrutura e padrÃµes do projeto Python
- âœ… **AnÃ¡lise de Cobertura**: Executa e analisa cobertura atual automaticamente
- âœ… **CriaÃ§Ã£o Inteligente**: Gera testes Python seguindo AAA pattern e padrÃµes do projeto
- âœ… **Suporte Multi-Framework Python**: pytest, unittest, nose, FastAPI, Django, LangChain, etc.
- âœ… **Mocks AutomÃ¡ticos Python**: Cria mocks corretos de APIs, DB, LLM e outras dependÃªncias
- âœ… **Modo EmpÃ­rico**: Executa sem perguntas, totalmente automatizado
- âœ… **ReutilizaÃ§Ã£o**: Aproveita fixtures e factories Python existentes
- âœ… **ValidaÃ§Ã£o**: Executa testes criados e valida cobertura alcanÃ§ada

---

## ğŸš€ InstalaÃ§Ã£o

Este plugin faz parte do repositÃ³rio **claudecode_plugins**. Para usÃ¡-lo:

```bash
# 1. Clone ou atualize o repositÃ³rio
git pull origin main

# 2. Recarregue plugins no Claude Code
/plugin refresh

# 3. Verifique se o plugin estÃ¡ disponÃ­vel
/plugin list
```

---

## ğŸ“– Uso

### `/setup-project-tests`

**Configura CLAUDE.md do projeto** com padrÃµes de testes Python.

**O que faz**:
- âœ… Cria ou atualiza `CLAUDE.md` na raiz do projeto
- âœ… Adiciona padrÃµes de testes Python (AAA pattern, mocks, fixtures)
- âœ… Configura frameworks detectados (pytest, unittest, nose)
- âœ… Documenta padrÃµes de mock (LangChain, FastAPI, Django, AWS)
- âœ… Orienta sobre fixtures reutilizÃ¡veis (conftest.py)
- âœ… Preserva conteÃºdo existente (nÃ£o sobrescreve)
- âœ… Detecta stack Python automaticamente

**Uso**:
```bash
# Setup bÃ¡sico (detecta stack automaticamente)
/setup-project-tests

# Ou com descriÃ§Ã£o da stack
/setup-project-tests "API FastAPI com LangChain + PostgreSQL"
```

**Resultado**:
Claude ficarÃ¡ automaticamente orientado a:
- Gerar testes Python seguindo padrÃµes do projeto
- Reutilizar fixtures existentes (conftest.py)
- Criar mocks adequados (LangChain chains, FastAPI, AWS boto3)
- Aplicar AAA pattern consistentemente
- Garantir testes paralelos seguros (pytest-xdist)

**Quando usar**:
- âœ… No inÃ­cio de novos projetos Python
- âœ… Ao adicionar este plugin em projetos existentes
- âœ… Quando quiser padronizar testes no time

---

### Comando Principal: `/py-test`

Analisa cobertura e **cria testes Python em paralelo** automaticamente:

```bash
# Analisar projeto Python inteiro (padrÃ£o)
/py-test

# Analisar diretÃ³rio especÃ­fico
/py-test src/meu_modulo

# Definir threshold customizado (padrÃ£o: 80%)
/py-test --threshold 85
```

### O que acontece automaticamente:

1. **Detecta** framework de testes Python (pytest/unittest/nose)
2. **Identifica** gerenciador de pacotes Python (poetry/pipenv/uv/pip)
3. **Analisa** cobertura atual do projeto Python
4. **Identifica** mÃ³dulos Python com cobertura < 80%
5. **LÃª** fixtures e padrÃµes Python existentes (conftest.py)
6. **Cria testes em PARALELO** - mÃºltiplos arquivos simultaneamente (âš¡ atÃ© 80% mais rÃ¡pido)
7. **Executa** testes criados e valida cobertura
8. **Reporta** resultados detalhados

### âš¡ Performance com ParalelizaÃ§Ã£o

O plugin cria **mÃºltiplos arquivos de teste simultaneamente**:
- **5 mÃ³dulos sem testes** â†’ Cria 5 arquivos em paralelo
- **10 mÃ³dulos sem testes** â†’ Cria 10 arquivos em paralelo
- **ReduÃ§Ã£o de tempo**: AtÃ© 80% mais rÃ¡pido que criaÃ§Ã£o sequencial

---

## ğŸ¯ Casos de Uso

### Caso 1: Projeto Simples com Pytest

**CenÃ¡rio**: Projeto Python com pytest, cobertura atual de 65%

```bash
/test-coverage
```

**Resultado**:
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… ANÃLISE DE TESTES CONCLUÃDA
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š COBERTURA GERAL:
Antes:  65.0%
Depois: 85.0%
Delta:  +20.0%

ğŸ“ ARQUIVOS DE TESTE CRIADOS:
â”œâ”€ tests/unit/test_calculator.py (8 testes)
â”œâ”€ tests/unit/test_validator.py (12 testes)
â””â”€ tests/unit/test_parser.py (6 testes)

Total: 26 novos testes

ğŸ“ˆ MÃ“DULOS COM COBERTURA 80%+:
âœ… src/calculator.py - 90.0% (antes: 60.0%)
âœ… src/validator.py - 85.0% (antes: 55.0%)
âœ… src/parser.py - 82.0% (antes: 70.0%)
```

### Caso 2: Projeto LangChain com Poetry

**CenÃ¡rio**: Projeto usando LangChain, LangGraph, Poetry, cobertura 50%

```bash
/test-coverage
```

**O plugin detecta automaticamente**:
- Framework: pytest com poetry
- Bibliotecas: langchain, langgraph
- PadrÃ£o: Nodes, Chains, Agents com LLM

**Cria testes especÃ­ficos**:
```python
@patch("workflow.nodes.node_processar.ChatOpenAI")
@patch("workflow.nodes.node_processar.ChatPromptTemplate.from_template")
def test_node_processar_com_sucesso(
    self,
    mock_prompt,
    mock_chat,
    sample_state,  # Fixture do conftest.py
):
    """Teste: Node processa conversa com LLM mockado"""
    # Arrange
    mock_llm = Mock()
    mock_chain = Mock()
    mock_chain.invoke.return_value = {"resultado": "processado"}
    mock_prompt.return_value.__or__ = Mock(return_value=mock_chain)
    mock_chat.return_value = mock_llm

    # Act
    result = node_processar(sample_state)

    # Assert
    assert result is not None
    assert "resultado" in result
    mock_chat.assert_called_once()
```

### Caso 3: Projeto FastAPI

**CenÃ¡rio**: API REST com FastAPI, endpoints sem testes

```bash
/test-coverage
```

**Cria testes de API**:
```python
from fastapi.testclient import TestClient

class TestUserAPI:
    """Testes para endpoints de usuÃ¡rios"""

    @pytest.fixture
    def client(self):
        return TestClient(app)

    def test_get_users_success(self, client):
        """Teste: GET /users retorna lista de usuÃ¡rios"""
        # Act
        response = client.get("/api/users")

        # Assert
        assert response.status_code == 200
        assert isinstance(response.json(), list)

    @patch("api.routes.users.get_current_user")
    def test_create_user_authenticated(self, mock_auth, client):
        """Teste: POST /users requer autenticaÃ§Ã£o"""
        # Arrange
        mock_auth.return_value = {"user_id": "123"}

        # Act
        response = client.post(
            "/api/users",
            json={"name": "Test", "email": "test@example.com"},
            headers={"Authorization": "Bearer token"}
        )

        # Assert
        assert response.status_code == 201
```

### Caso 4: Projeto Django

**CenÃ¡rio**: AplicaÃ§Ã£o Django com models e views sem testes

```bash
/test-coverage
```

**Cria testes Django**:
```python
import pytest
from django.test import Client

class TestBlogViews:
    """Testes para views do blog"""

    @pytest.mark.django_db
    def test_post_list_view(self, client):
        """Teste: Listagem de posts funciona corretamente"""
        # Arrange
        Post.objects.create(title="Test", content="Content")

        # Act
        response = client.get("/blog/")

        # Assert
        assert response.status_code == 200
        assert "Test" in response.content.decode()

    @pytest.mark.django_db
    def test_create_post_model(self):
        """Teste: Model Post Ã© criado corretamente"""
        # Arrange & Act
        post = Post.objects.create(
            title="Test Post",
            content="Test Content",
            author_id=1
        )

        # Assert
        assert post.pk is not None
        assert post.title == "Test Post"
```

### Caso 5: AWS Lambda

**CenÃ¡rio**: Lambda handlers sem testes

```bash
/test-coverage
```

**Cria testes de Lambda**:
```python
@patch("main.boto3.client")
@patch("main.os.getenv")
def test_lambda_handler_processa_s3_event(self, mock_env, mock_boto):
    """Teste: Lambda processa evento S3 corretamente"""
    # Arrange
    mock_env.return_value = "test-bucket"
    mock_s3 = Mock()
    mock_s3.get_object.return_value = {
        "Body": Mock(read=lambda: b'{"data": "test"}')
    }
    mock_boto.return_value = mock_s3

    event = {
        "Records": [{
            "s3": {
                "bucket": {"name": "test-bucket"},
                "object": {"key": "test-key"}
            }
        }]
    }
    context = Mock()

    # Act
    response = lambda_handler(event, context)

    # Assert
    assert response["statusCode"] == 200
    mock_s3.get_object.assert_called_once()
```

---

## ğŸ”§ Frameworks e Bibliotecas Suportados

### Testing Frameworks
- âœ… **pytest** (recomendado)
- âœ… **unittest**
- âœ… **nose**

### Package Managers
- âœ… **poetry**
- âœ… **pipenv**
- âœ… **uv**
- âœ… **pip**

### Web Frameworks
- âœ… **FastAPI**
- âœ… **Django**
- âœ… **Flask**

### LLM & AI Frameworks
- âœ… **LangChain**
- âœ… **LangGraph**
- âœ… **LangSmith**

### Cloud & Serverless
- âœ… **AWS Lambda**
- âœ… **boto3 (AWS SDK)**

### Databases
- âœ… **SQLAlchemy**
- âœ… **Django ORM**
- âœ… **Pynamodb**

### HTTP Clients
- âœ… **requests**
- âœ… **httpx**
- âœ… **aiohttp**

### Mock Libraries
- âœ… **unittest.mock**
- âœ… **pytest-mock**
- âœ… **responses**
- âœ… **httpx-mock**

### Async
- âœ… **pytest-asyncio**
- âœ… **asyncio**
- âœ… **AsyncMock**

---

## ğŸ¯ Skills Especializadas

### LangChain Test Specialist

**Nova em v1.3.0**: Skill especializada para criar testes unitÃ¡rios e de integraÃ§Ã£o para aplicaÃ§Ãµes LangChain e LangGraph.

#### Recursos da Skill

Esta skill detecta automaticamente cÃ³digo LangChain/LangGraph e aplica **7 padrÃµes de teste especializados**:

1. **Basic LangGraph Test**: Testes state-based com `MemorySaver` checkpointer
2. **Individual Node Testing**: Testar nodes isoladamente via `graph.nodes["node_name"]`
3. **Partial Execution**: Uso de `update_state()` e `interrupt_after` para testes parciais
4. **Mocking LLM**: `GenericFakeChatModel` para testes unitÃ¡rios sem API calls
5. **Trajectory Match**: ValidaÃ§Ã£o de sequÃªncia de aÃ§Ãµes com `agentevals`
6. **LLM-as-Judge**: AvaliaÃ§Ã£o de qualidade usando LLM como juiz
7. **VCR Recording**: Gravar/replay HTTP calls com `pytest-recording`

#### Quando a Skill Ã© Ativada

A skill Ã© invocada automaticamente pelo Claude quando detecta:
- Imports de `langchain`, `langgraph`, ou `langchain_core`
- Uso de `StateGraph`, `MessageGraph`, chains LCEL
- LLM calls (`ChatOpenAI`, `ChatAnthropic`, etc.)
- Agents, tools, ou trajectories
- Pedidos como: "testar chain", "testar grafo", "mock LLM", "trajectory validation"

#### Exemplos de Testes Criados

**Teste de Grafo LangGraph**:
```python
def test_basic_graph_execution():
    """Teste: Grafo executa corretamente com estado inicial"""
    # Arrange
    checkpointer = MemorySaver()
    graph = create_graph()
    compiled_graph = graph.compile(checkpointer=checkpointer)

    # Act
    result = compiled_graph.invoke(
        {"my_key": "initial"},
        config={"configurable": {"thread_id": "1"}}
    )

    # Assert
    assert result["my_key"] == "expected_value"
```

**Teste com Mocked LLM**:
```python
from langchain_core.language_models.fake_chat_models import GenericFakeChatModel

def test_node_with_mocked_llm():
    """Teste: Node com LLM mockado retorna respostas esperadas"""
    # Arrange
    mock_llm = GenericFakeChatModel(messages=iter([
        AIMessage(content="Primeira resposta"),
        AIMessage(content="Segunda resposta")
    ]))

    # Act
    result = node_with_llm({"messages": [...]})

    # Assert
    assert result["response"] == "Primeira resposta"
```

**Trajectory Validation com AgentEvals**:
```python
from agentevals.trajectory.match import create_trajectory_match_evaluator

def test_trajectory_strict_match():
    """Teste: Trajectory corresponde exatamente Ã  esperada"""
    # Arrange
    evaluator = create_trajectory_match_evaluator(
        trajectory_match_mode="strict"
    )

    # Act
    evaluation = evaluator(
        outputs=actual_trajectory,
        reference_outputs=reference_trajectory
    )

    # Assert
    assert evaluation["score"] is True
```

**VCR Recording para Integration Tests**:
```python
@pytest.mark.vcr()
def test_agent_with_real_llm_vcr():
    """Teste: Grava chamadas LLM reais na primeira execuÃ§Ã£o, replay depois"""
    # PRIMEIRA EXECUÃ‡ÃƒO: Faz chamadas reais e grava
    # PRÃ“XIMAS EXECUÃ‡Ã•ES: Replay sem API calls (100% determinÃ­stico)

    agent = create_agent()
    result = agent.invoke({"input": "What's the capital of France?"})

    assert "Paris" in result["output"]
```

#### Dependencies Adicionais

Para usar os padrÃµes LangChain/LangGraph, certifique-se de ter:

```bash
# Principais
pip install langchain langchain-core langgraph

# Testing
pip install pytest pytest-asyncio agentevals vcrpy pytest-recording
```

Ou com poetry:
```toml
[tool.poetry.group.test.dependencies]
agentevals = "^0.1.0"      # Trajectory validation
vcrpy = "^6.0.0"            # HTTP recording
pytest-recording = "^0.13.0"  # pytest-vcr integration
```

#### Modos de Trajectory Matching

- **strict**: Ordem e conteÃºdo idÃªnticos
- **unordered**: Mesmo conteÃºdo, ordem irrelevante
- **subset**: Trajectory real contÃ©m pelo menos as aÃ§Ãµes esperadas
- **superset**: Trajectory real Ã© subconjunto das aÃ§Ãµes esperadas

#### LLM-as-Judge Models

Suportados:
- `openai:gpt-4o-mini`, `openai:o3-mini`
- `anthropic:claude-3-5-sonnet`, `anthropic:claude-3-5-haiku`

---

## ğŸ“ PadrÃµes de Teste Criados

### AAA Pattern (Arrange-Act-Assert)

Todos os testes seguem o padrÃ£o AAA:

```python
def test_exemplo(self):
    """Teste: DescriÃ§Ã£o clara do cenÃ¡rio"""
    # Arrange - Preparar dados e mocks
    data = {"key": "value"}
    mock_api.return_value = {"result": "ok"}

    # Act - Executar a funÃ§Ã£o/mÃ©todo
    result = function_under_test(data)

    # Assert - Validar resultado
    assert result is not None
    assert result["status"] == "ok"
    mock_api.assert_called_once()
```

### Happy Path + Erros + Edge Cases

Cada funÃ§Ã£o recebe pelo menos 3 tipos de testes:

```python
# 1. Happy path - CenÃ¡rio de sucesso
def test_process_data_success(self):
    """Teste: Processa dados vÃ¡lidos com sucesso"""
    ...

# 2. Error handling - Tratamento de erros
def test_process_data_invalid_input(self):
    """Teste: Lida corretamente com entrada invÃ¡lida"""
    with pytest.raises(ValueError):
        ...

# 3. Edge cases - Casos extremos
@pytest.mark.parametrize("input,expected", [
    (None, False),
    ("", False),
    ([], False),
    ({}, False),
])
def test_process_data_edge_cases(self, input, expected):
    """Teste: Lida com casos extremos"""
    ...
```

### Mocks de DependÃªncias Externas

Todas as dependÃªncias externas sÃ£o mockadas:

```python
@patch("module.requests.get")  # HTTP
@patch("module.boto3.client")  # AWS
@patch("module.ChatOpenAI")    # LLM
@patch("module.db.session")    # Database
def test_with_all_mocks(self, mock_db, mock_llm, mock_aws, mock_http):
    """Teste: Todas as dependÃªncias externas mockadas"""
    ...
```

---

## ğŸ“ Recursos AvanÃ§ados

### DetecÃ§Ã£o AutomÃ¡tica de Fixtures

O plugin lÃª automaticamente `conftest.py` e reutiliza fixtures:

```python
# conftest.py
@pytest.fixture
def sample_user():
    return {"id": 1, "name": "Test User"}

# Teste criado automaticamente usa a fixture
def test_get_user(self, sample_user):
    """Teste: Buscar usuÃ¡rio usa fixture existente"""
    result = get_user(sample_user["id"])
    assert result["name"] == sample_user["name"]
```

### ParametrizaÃ§Ã£o Inteligente

Cria testes parametrizados quando identifica mÃºltiplos casos:

```python
@pytest.mark.parametrize("input_value,expected_output", [
    ("valid@email.com", True),
    ("invalid-email", False),
    ("", False),
    (None, False),
    ("test@", False),
    ("@test.com", False),
])
def test_validate_email_parametrized(self, input_value, expected_output):
    """Teste: ValidaÃ§Ã£o de email com mÃºltiplos casos"""
    result = validate_email(input_value)
    assert result == expected_output
```

### Suporte a CÃ³digo AssÃ­ncrono

Detecta e cria testes assÃ­ncronos corretamente:

```python
@pytest.mark.asyncio
@patch("module.async_http_client")
async def test_fetch_data_async(self, mock_client):
    """Teste: FunÃ§Ã£o assÃ­ncrona funciona corretamente"""
    # Arrange
    mock_client.get.return_value = AsyncMock(
        return_value={"data": "test"}
    )

    # Act
    result = await fetch_data_async()

    # Assert
    assert result == {"data": "test"}
```

### Mocks EspecÃ­ficos de LangChain

Cria mocks corretos para LangChain patterns:

```python
@patch("module.ChatPromptTemplate.from_template")
@patch("module.ChatOpenAI")
def test_langchain_chain(self, mock_chat, mock_prompt):
    """Teste: Chain LangChain funciona corretamente"""
    # Arrange - Mock de chain com pipe operator
    mock_llm = Mock()
    mock_chain = Mock()
    mock_chain.invoke.return_value = {"output": "result"}
    mock_prompt.return_value.__or__ = Mock(return_value=mock_chain)
    mock_chat.return_value = mock_llm

    # Act
    result = process_with_chain(data)

    # Assert
    assert result["output"] == "result"
    mock_chain.invoke.assert_called_once()
```

---

## ğŸ“Š ConfiguraÃ§Ã£o de Cobertura

### Threshold PadrÃ£o

- **Cobertura mÃ­nima global**: 80%
- **Cobertura ideal**: 85-90%
- **Arquivos crÃ­ticos**: 90%+

### CustomizaÃ§Ã£o via Arquivos de ConfiguraÃ§Ã£o

O plugin respeita configuraÃ§Ãµes existentes:

**pytest.ini**:
```ini
[pytest]
addopts = --cov=src --cov-report=term-missing --cov-fail-under=80
```

**pyproject.toml**:
```toml
[tool.coverage.report]
fail_under = 80
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "raise NotImplementedError",
]
```

**setup.cfg**:
```cfg
[coverage:report]
fail_under = 80
```

**.coveragerc**:
```ini
[report]
fail_under = 80
omit =
    */tests/*
    */migrations/*
```

---

## âš¡ Modo EmpÃ­rico

Este plugin opera em **modo empÃ­rico** - executa automaticamente sem perguntas.

### O que NÃƒO faz:
- âŒ NÃ£o pergunta qual framework usar
- âŒ NÃ£o pergunta quais mÃ³dulos testar
- âŒ NÃ£o pergunta se deve criar testes
- âŒ NÃ£o pergunta se deve executar testes

### O que FAZ:
- âœ… Detecta automaticamente todo o ambiente
- âœ… Analisa cobertura imediatamente
- âœ… Cria testes diretamente
- âœ… Executa testes automaticamente
- âœ… Reporta resultados ao final

---

## ğŸ› Troubleshooting

### Problema: Import errors em testes criados

**Causa**: Estrutura de imports ou `sys.path` incorreta

**SoluÃ§Ã£o automÃ¡tica**: O plugin ajusta imports e adiciona `__init__.py` quando necessÃ¡rio

### Problema: Mocks nÃ£o funcionam

**Causa**: Path de mocking incorreto

**SoluÃ§Ã£o automÃ¡tica**: O plugin corrige paths de mocking e adiciona `spec` quando necessÃ¡rio

### Problema: Testes assÃ­ncronos nÃ£o executam

**Causa**: Falta `@pytest.mark.asyncio`

**SoluÃ§Ã£o automÃ¡tica**: O plugin adiciona markers e instala `pytest-asyncio` se necessÃ¡rio

### Problema: Fixtures nÃ£o encontradas

**Causa**: `conftest.py` ausente ou mal localizado

**SoluÃ§Ã£o automÃ¡tica**: O plugin move fixtures para local correto

---

## ğŸ“š Componentes do Plugin

### Command: `/test-coverage`
Comando principal que invoca o agente especializado

**LocalizaÃ§Ã£o**: `commands/test-coverage.md`

### Agent: `test-assistant`
Agente especializado em anÃ¡lise e criaÃ§Ã£o de testes

**LocalizaÃ§Ã£o**: `agents/test-assistant.md`

**Especialidades**:
- DetecÃ§Ã£o de ambiente e frameworks
- AnÃ¡lise de cobertura
- CriaÃ§Ã£o de testes seguindo padrÃµes
- Mocks inteligentes
- ValidaÃ§Ã£o de qualidade

---

## ğŸš€ Roadmap

Funcionalidades planejadas para versÃµes futuras:

- [ ] Suporte a mais frameworks de teste (Robot Framework, Behave)
- [ ] Testes de integraÃ§Ã£o automÃ¡ticos
- [ ] Testes E2E com Playwright/Selenium
- [ ] Mutation testing
- [ ] Performance testing
- [ ] GeraÃ§Ã£o de relatÃ³rios HTML customizados
- [ ] IntegraÃ§Ã£o com CI/CD (GitHub Actions, GitLab CI)
- [ ] Suporte a TypeScript/JavaScript
- [ ] Dashboard de cobertura

---

## ğŸ“„ LicenÃ§a

MIT License - Carlos Araujo

---

## ğŸ‘¤ Autor

**Carlos Araujo**
- Email: cadu.gevaerd@gmail.com
- GitHub: [@cadugevaerd](https://github.com/cadugevaerd)

---

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Para contribuir:

1. Fork o repositÃ³rio
2. Crie uma branch para sua feature (`git checkout -b feature/nova-feature`)
3. Commit suas mudanÃ§as (`git commit -m 'feat: adiciona nova feature'`)
4. Push para a branch (`git push origin feature/nova-feature`)
5. Abra um Pull Request

---

## ğŸ“ Suporte

- **Issues**: [GitHub Issues](https://github.com/cadugevaerd/claudecode_plugins/issues)
- **DocumentaÃ§Ã£o**: [claudecode_plugins](https://github.com/cadugevaerd/claudecode_plugins)

---

**Desenvolvido com â¤ï¸ para Claude Code Community**