# üêç Python Test Generator

Plugin de gera√ß√£o autom√°tica de testes unit√°rios Python com an√°lise de cobertura e **cria√ß√£o paralela** de arquivos para m√°xima performance.

## ‚ö†Ô∏è BREAKING CHANGES - v2.0.0

**MAJOR CHANGES in v2.0.0** (Released: 2025-11-01):

### üö® Coverage Threshold Enforcement

**Before (v1.x)**:
- `/py-test` always created tests regardless of current coverage
- No verification of existing coverage level

**After (v2.0+)**:
- `/py-test` **RESPECTS 80% coverage threshold**
- If coverage ‚â•80%: **STOPS and asks user** if they want to continue
- If coverage <80%: **Proceeds automatically** without questions

**Migration Guide**:

If you relied on automatic test generation regardless of coverage:
1. ‚úÖ Understand that v2.0+ prevents unnecessary test creation
2. ‚úÖ When coverage ‚â•80%, explicitly confirm test creation: respond "y" when prompted
3. ‚úÖ To bypass threshold check, you'll need to confirm explicitly

**Why this breaking change?**:
- ‚úÖ Prevents unnecessary test creation when coverage is already sufficient
- ‚úÖ Focuses test generation on code that actually needs it
- ‚úÖ Reduces noise and keeps test suite maintainable
- ‚úÖ Aligns with best practice: 80% coverage is the industry standard target

### üÜï New Command: `/update-claude-md`

**What it does**:
- Updates project's CLAUDE.md with python-test-generator configuration
- Follows best practices (‚â§40 lines, progressive disclosure)
- Documents agent and critical testing rules
- Auto-discovery friendly (no manual skill copying)

**When to use**:
```bash
# Setup python-test-generator in project
/update-claude-md
```

---

## üìã Descri√ß√£o

O **Python Test Generator** √© um plugin especializado que analisa a cobertura de testes do seu projeto Python e **cria automaticamente m√∫ltiplos testes em paralelo**, gerando testes unit√°rios completos, bem estruturados e com alta qualidade, seguindo os padr√µes e frameworks j√° utilizados no projeto.

**v2.0+**: Agora com **coverage threshold enforcement** - respeita 80% de cobertura para evitar cria√ß√£o desnecess√°ria de testes.

### üéØ Principais Recursos

- ‚ö° **Cria√ß√£o Paralela**: Gera m√∫ltiplos arquivos de teste simultaneamente (at√© 80% mais r√°pido)
- üßπ **üÜï v2.0: Detec√ß√£o de Testes Obsoletos**: Identifica e remove testes desnecess√°rios automaticamente
- üß™ **üÜï v2.0: Remo√ß√£o Condicional de Testes Falhando**: Remove testes falhando APENAS se cobertura permanecer ‚â•80%
- ‚úÖ **Detec√ß√£o Autom√°tica Python**: Identifica frameworks, estrutura e padr√µes do projeto Python
- ‚úÖ **An√°lise de Cobertura**: Executa e analisa cobertura atual automaticamente
- ‚úÖ **Cria√ß√£o Inteligente**: Gera testes Python seguindo AAA pattern e padr√µes do projeto
- ‚úÖ **Suporte Multi-Framework Python**: pytest, unittest, nose, FastAPI, Django, LangChain, etc.
- ‚úÖ **Mocks Autom√°ticos Python**: Cria mocks corretos de APIs, DB, LLM e outras depend√™ncias
- ‚úÖ **Modo Emp√≠rico**: Executa sem perguntas, totalmente automatizado
- ‚úÖ **Reutiliza√ß√£o**: Aproveita fixtures e factories Python existentes
- ‚úÖ **Valida√ß√£o**: Executa testes criados e valida cobertura alcan√ßada

### ‚ùå What the Agent Does NOT Do

**IMPORTANT**: This plugin does NOT create git commits.

**What the agent does**:
- ‚úÖ Generates test files and saves to disk
- ‚úÖ Runs tests to verify they work
- ‚úÖ Reports results and coverage

**What the agent does NOT do**:
- ‚ùå Does NOT create git commits (you commit when ready)
- ‚ùå Does NOT push to remote repositories
- ‚ùå Does NOT modify .gitignore or git configuration

**Workflow**:
1. Agent generates and saves test files
2. Agent runs tests to validate
3. Agent reports results
4. **You review tests**
5. **You commit when satisfied**: `git add tests/ && git commit -m "test: ..."`

---

## üöÄ Instala√ß√£o

Este plugin faz parte do reposit√≥rio **claudecode_plugins**. Para us√°-lo:

```bash
# 1. Clone ou atualize o reposit√≥rio
git pull origin main

# 2. Recarregue plugins no Claude Code
/plugin refresh

# 3. Verifique se o plugin est√° dispon√≠vel
/plugin list
```

---

## üìñ Uso

### üÜï `/update-claude-md` (v2.0+)

**Configura CLAUDE.md do projeto** com plugin python-test-generator.

**O que faz**:
- ‚úÖ Cria ou atualiza `CLAUDE.md` na raiz do projeto
- ‚úÖ Adiciona se√ß√£o python-test-generator (‚â§40 linhas)
- ‚úÖ Documenta agent test-assistant e regras cr√≠ticas
- ‚úÖ Link para README.md completo (progressive disclosure)
- ‚úÖ Preserva conte√∫do existente (n√£o sobrescreve)

**Uso**:
```bash
# Setup do plugin no projeto
/update-claude-md
```

**Quando usar**:
- ‚úÖ Ao come√ßar a usar python-test-generator em um projeto
- ‚úÖ Quando CLAUDE.md foi corrompido ou deletado
- ‚úÖ Para atualizar configura√ß√£o ap√≥s upgrade do plugin

---

### `/setup-project-tests`

**Configura CLAUDE.md do projeto** com padr√µes de testes Python.

**O que faz**:
- ‚úÖ Cria ou atualiza `CLAUDE.md` na raiz do projeto
- ‚úÖ Adiciona padr√µes de testes Python (AAA pattern, mocks, fixtures)
- ‚úÖ Configura frameworks detectados (pytest, unittest, nose)
- ‚úÖ Documenta padr√µes de mock (LangChain, FastAPI, Django, AWS)
- ‚úÖ Orienta sobre fixtures reutiliz√°veis (conftest.py)
- ‚úÖ Preserva conte√∫do existente (n√£o sobrescreve)
- ‚úÖ Detecta stack Python automaticamente

**Uso**:
```bash
# Setup b√°sico (detecta stack automaticamente)
/setup-project-tests

# Ou com descri√ß√£o da stack
/setup-project-tests "API FastAPI com LangChain + PostgreSQL"
```

**Resultado**:
Claude ficar√° automaticamente orientado a:
- Gerar testes Python seguindo padr√µes do projeto
- Reutilizar fixtures existentes (conftest.py)
- Criar mocks adequados (LangChain chains, FastAPI, AWS boto3)
- Aplicar AAA pattern consistentemente
- Garantir testes paralelos seguros (pytest-xdist)

**Quando usar**:
- ‚úÖ No in√≠cio de novos projetos Python
- ‚úÖ Ao adicionar este plugin em projetos existentes
- ‚úÖ Quando quiser padronizar testes no time

---

### Comando Principal: `/py-test`

Analisa cobertura e **cria testes Python em paralelo** automaticamente:

```bash
# Analisar projeto Python inteiro (padr√£o)
/py-test

# Analisar diret√≥rio espec√≠fico
/py-test src/meu_modulo

# Definir threshold customizado (padr√£o: 80%)
/py-test --threshold 85
```

### O que acontece automaticamente:

1. **Detecta** framework de testes Python (pytest/unittest/nose)
2. **Identifica** gerenciador de pacotes Python (poetry/pipenv/uv/pip)
3. **Analisa** cobertura atual do projeto Python
4. **üÜï v2.0: Verifica** se cobertura j√° est√° ‚â•80% (pergunta se continua)
5. **üÜï v2.0: Detecta testes falhando** e analisa impacto na cobertura
6. **üÜï v2.0: Remove testes falhando** (APENAS se cobertura ‚â•80% ap√≥s remo√ß√£o)
7. **üÜï v2.0: Detecta testes obsoletos** e pergunta se remove
8. **Identifica** m√≥dulos Python com cobertura < 80%
9. **L√™** fixtures e padr√µes Python existentes (conftest.py)
10. **Cria testes em PARALELO** - m√∫ltiplos arquivos simultaneamente (‚ö° at√© 80% mais r√°pido)
11. **Executa** testes criados e valida cobertura
12. **Reporta** resultados detalhados

### ‚ö° Performance com Paraleliza√ß√£o

O plugin cria **m√∫ltiplos arquivos de teste simultaneamente**:
- **5 m√≥dulos sem testes** ‚Üí Cria 5 arquivos em paralelo
- **10 m√≥dulos sem testes** ‚Üí Cria 10 arquivos em paralelo
- **Redu√ß√£o de tempo**: At√© 80% mais r√°pido que cria√ß√£o sequencial

### üß™ Remo√ß√£o Condicional de Testes Falhando (v2.0+)

**NOVO em v2.0**: O plugin agora detecta **testes falhando** e remove-os automaticamente **APENAS** se cobertura permanecer ‚â•80% ap√≥s remo√ß√£o.

#### Como Funciona

1. **Executa pytest** e identifica testes com falhas
2. **Calcula cobertura atual** do projeto
3. **Estima cobertura ap√≥s remo√ß√£o** dos testes falhando
4. **Decis√£o condicional**:
   - ‚úÖ Se cobertura ‚â•80%: **Oferece remo√ß√£o**
   - ‚ùå Se cobertura <80%: **Avisa para corrigir manualmente**

#### Cen√°rio 1: Cobertura ‚â•80% ap√≥s remo√ß√£o (REMOVE)

```
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
‚ö†Ô∏è  FAILING TESTS DETECTED (2 tests)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Coverage Analysis:
- Current coverage: 85%
- Estimated coverage after removal: 82%

üìç tests/unit/test_calculator.py::test_divide_by_zero
   Error: ZeroDivisionError

üìç tests/unit/test_validator.py::test_email_validation
   Error: AssertionError: expected True, got False

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚úÖ Coverage will remain ‚â•80% (82%) after removal.

These tests are failing and can be safely removed
without compromising coverage.

Remove failing tests? (y/n)
```

#### Cen√°rio 2: Cobertura <80% ap√≥s remo√ß√£o (N√ÉO REMOVE)

```
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
‚ö†Ô∏è  FAILING TESTS DETECTED (5 tests)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Coverage Analysis:
- Current coverage: 83%
- Estimated coverage after removal: 76%

‚ùå Cannot remove failing tests automatically.

Reason: Coverage would drop below 80% threshold (76% < 80%).

These tests are failing but cover critical code paths.
You should fix them instead of removing them:

üìç tests/unit/test_core.py::test_main_flow
üìç tests/unit/test_api.py::test_endpoint_validation
...

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚ö†Ô∏è  Action Required: Fix failing tests manually.
```

#### Por que isso √© √∫til?

- ‚úÖ **Autom√°tico**: Detecta testes falhando sem interven√ß√£o manual
- ‚úÖ **Seguro**: S√≥ remove se cobertura permanecer suficiente
- ‚úÖ **Transparente**: Mostra impacto na cobertura antes de remover
- ‚úÖ **Inteligente**: Evita remo√ß√£o de testes cr√≠ticos

---

### üßπ Detec√ß√£o de Testes Obsoletos (v2.0+)

O plugin tamb√©m detecta automaticamente **testes desnecess√°rios ou obsoletos** e oferece remov√™-los:

**Crit√©rios de Detec√ß√£o**:
1. **Fun√ß√£o n√£o existe mais**: Teste para fun√ß√£o que foi removida/renomeada
2. **Teste duplicado**: Outro teste j√° cobre o mesmo cen√°rio
3. **Sem asser√ß√µes reais**: Teste vazio ou s√≥ com `assert True`
4. **Mock inv√°lido**: Mocka fun√ß√£o/classe que n√£o existe mais
5. **C√≥digo refatorado**: Teste de implementa√ß√£o antiga que mudou

**Exemplo de Output**:
```
üßπ OBSOLETE TESTS DETECTED (3 tests)

üìç tests/unit/test_calculator.py
   Function: test_add_old
   Reason: Function 'add_old' no longer exists in source code

üìç tests/unit/test_validator.py
   Function: test_placeholder
   Reason: No real assertions - test body is empty

üìç tests/unit/test_parser.py
   Function: test_with_old_parser
   Reason: Mocks 'module.OldParser' which no longer exists

Remove obsolete tests? (y/n)
```

**Por que isso √© √∫til?**
- ‚úÖ Mant√©m suite de testes limpa e focada
- ‚úÖ Evita falsos positivos
- ‚úÖ Reduz tempo de execu√ß√£o dos testes
- ‚úÖ Facilita manuten√ß√£o do c√≥digo de testes

---

## üéØ Casos de Uso

### Caso 1: Projeto Simples com Pytest

**Cen√°rio**: Projeto Python com pytest, cobertura atual de 65%

```bash
/test-coverage
```

**Resultado**:
```
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
‚úÖ AN√ÅLISE DE TESTES CONCLU√çDA
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

üìä COBERTURA GERAL:
Antes:  65.0%
Depois: 85.0%
Delta:  +20.0%

üìÅ ARQUIVOS DE TESTE CRIADOS:
‚îú‚îÄ tests/unit/test_calculator.py (8 testes)
‚îú‚îÄ tests/unit/test_validator.py (12 testes)
‚îî‚îÄ tests/unit/test_parser.py (6 testes)

Total: 26 novos testes

üìà M√ìDULOS COM COBERTURA 80%+:
‚úÖ src/calculator.py - 90.0% (antes: 60.0%)
‚úÖ src/validator.py - 85.0% (antes: 55.0%)
‚úÖ src/parser.py - 82.0% (antes: 70.0%)
```

### Caso 2: Projeto LangChain com Poetry

**Cen√°rio**: Projeto usando LangChain, LangGraph, Poetry, cobertura 50%

```bash
/test-coverage
```

**O plugin detecta automaticamente**:
- Framework: pytest com poetry
- Bibliotecas: langchain, langgraph
- Padr√£o: Nodes, Chains, Agents com LLM

**Cria testes espec√≠ficos**:
```python
@patch("workflow.nodes.node_processar.ChatOpenAI")
@patch("workflow.nodes.node_processar.ChatPromptTemplate.from_template")
def test_node_processar_com_sucesso(
    self,
    mock_prompt,
    mock_chat,
    sample_state,  # Fixture do conftest.py
):
    """Teste: Node processa conversa com LLM mockado"""
    # Arrange
    mock_llm = Mock()
    mock_chain = Mock()
    mock_chain.invoke.return_value = {"resultado": "processado"}
    mock_prompt.return_value.__or__ = Mock(return_value=mock_chain)
    mock_chat.return_value = mock_llm

    # Act
    result = node_processar(sample_state)

    # Assert
    assert result is not None
    assert "resultado" in result
    mock_chat.assert_called_once()
```

### Caso 3: Projeto FastAPI

**Cen√°rio**: API REST com FastAPI, endpoints sem testes

```bash
/test-coverage
```

**Cria testes de API**:
```python
from fastapi.testclient import TestClient

class TestUserAPI:
    """Testes para endpoints de usu√°rios"""

    @pytest.fixture
    def client(self):
        return TestClient(app)

    def test_get_users_success(self, client):
        """Teste: GET /users retorna lista de usu√°rios"""
        # Act
        response = client.get("/api/users")

        # Assert
        assert response.status_code == 200
        assert isinstance(response.json(), list)

    @patch("api.routes.users.get_current_user")
    def test_create_user_authenticated(self, mock_auth, client):
        """Teste: POST /users requer autentica√ß√£o"""
        # Arrange
        mock_auth.return_value = {"user_id": "123"}

        # Act
        response = client.post(
            "/api/users",
            json={"name": "Test", "email": "test@example.com"},
            headers={"Authorization": "Bearer token"}
        )

        # Assert
        assert response.status_code == 201
```

### Caso 4: Projeto Django

**Cen√°rio**: Aplica√ß√£o Django com models e views sem testes

```bash
/test-coverage
```

**Cria testes Django**:
```python
import pytest
from django.test import Client

class TestBlogViews:
    """Testes para views do blog"""

    @pytest.mark.django_db
    def test_post_list_view(self, client):
        """Teste: Listagem de posts funciona corretamente"""
        # Arrange
        Post.objects.create(title="Test", content="Content")

        # Act
        response = client.get("/blog/")

        # Assert
        assert response.status_code == 200
        assert "Test" in response.content.decode()

    @pytest.mark.django_db
    def test_create_post_model(self):
        """Teste: Model Post √© criado corretamente"""
        # Arrange & Act
        post = Post.objects.create(
            title="Test Post",
            content="Test Content",
            author_id=1
        )

        # Assert
        assert post.pk is not None
        assert post.title == "Test Post"
```

### Caso 5: AWS Lambda

**Cen√°rio**: Lambda handlers sem testes

```bash
/test-coverage
```

**Cria testes de Lambda**:
```python
@patch("main.boto3.client")
@patch("main.os.getenv")
def test_lambda_handler_processa_s3_event(self, mock_env, mock_boto):
    """Teste: Lambda processa evento S3 corretamente"""
    # Arrange
    mock_env.return_value = "test-bucket"
    mock_s3 = Mock()
    mock_s3.get_object.return_value = {
        "Body": Mock(read=lambda: b'{"data": "test"}')
    }
    mock_boto.return_value = mock_s3

    event = {
        "Records": [{
            "s3": {
                "bucket": {"name": "test-bucket"},
                "object": {"key": "test-key"}
            }
        }]
    }
    context = Mock()

    # Act
    response = lambda_handler(event, context)

    # Assert
    assert response["statusCode"] == 200
    mock_s3.get_object.assert_called_once()
```

---

## üîß Frameworks e Bibliotecas Suportados

### Testing Frameworks
- ‚úÖ **pytest** (recomendado)
- ‚úÖ **unittest**
- ‚úÖ **nose**

### Package Managers
- ‚úÖ **poetry**
- ‚úÖ **pipenv**
- ‚úÖ **uv**
- ‚úÖ **pip**

### Web Frameworks
- ‚úÖ **FastAPI**
- ‚úÖ **Django**
- ‚úÖ **Flask**

### LLM & AI Frameworks
- ‚úÖ **LangChain**
- ‚úÖ **LangGraph**
- ‚úÖ **LangSmith**

### Cloud & Serverless
- ‚úÖ **AWS Lambda**
- ‚úÖ **boto3 (AWS SDK)**

### Databases
- ‚úÖ **SQLAlchemy**
- ‚úÖ **Django ORM**
- ‚úÖ **Pynamodb**

### HTTP Clients
- ‚úÖ **requests**
- ‚úÖ **httpx**
- ‚úÖ **aiohttp**

### Mock Libraries
- ‚úÖ **unittest.mock**
- ‚úÖ **pytest-mock**
- ‚úÖ **responses**
- ‚úÖ **httpx-mock**

### Async
- ‚úÖ **pytest-asyncio**
- ‚úÖ **asyncio**
- ‚úÖ **AsyncMock**

---

## üéØ Skills Especializadas

### LangChain Test Specialist

**Nova em v1.3.0**: Skill especializada para criar testes unit√°rios e de integra√ß√£o para aplica√ß√µes LangChain e LangGraph.

#### Recursos da Skill

Esta skill detecta automaticamente c√≥digo LangChain/LangGraph e aplica **7 padr√µes de teste especializados**:

1. **Basic LangGraph Test**: Testes state-based com `MemorySaver` checkpointer
2. **Individual Node Testing**: Testar nodes isoladamente via `graph.nodes["node_name"]`
3. **Partial Execution**: Uso de `update_state()` e `interrupt_after` para testes parciais
4. **Mocking LLM**: `GenericFakeChatModel` para testes unit√°rios sem API calls
5. **Trajectory Match**: Valida√ß√£o de sequ√™ncia de a√ß√µes com `agentevals`
6. **LLM-as-Judge**: Avalia√ß√£o de qualidade usando LLM como juiz
7. **VCR Recording**: Gravar/replay HTTP calls com `pytest-recording`

#### Quando a Skill √© Ativada

A skill √© invocada automaticamente pelo Claude quando detecta:
- Imports de `langchain`, `langgraph`, ou `langchain_core`
- Uso de `StateGraph`, `MessageGraph`, chains LCEL
- LLM calls (`ChatOpenAI`, `ChatAnthropic`, etc.)
- Agents, tools, ou trajectories
- Pedidos como: "testar chain", "testar grafo", "mock LLM", "trajectory validation"

#### Exemplos de Testes Criados

**Teste de Grafo LangGraph**:
```python
def test_basic_graph_execution():
    """Teste: Grafo executa corretamente com estado inicial"""
    # Arrange
    checkpointer = MemorySaver()
    graph = create_graph()
    compiled_graph = graph.compile(checkpointer=checkpointer)

    # Act
    result = compiled_graph.invoke(
        {"my_key": "initial"},
        config={"configurable": {"thread_id": "1"}}
    )

    # Assert
    assert result["my_key"] == "expected_value"
```

**Teste com Mocked LLM**:
```python
from langchain_core.language_models.fake_chat_models import GenericFakeChatModel

def test_node_with_mocked_llm():
    """Teste: Node com LLM mockado retorna respostas esperadas"""
    # Arrange
    mock_llm = GenericFakeChatModel(messages=iter([
        AIMessage(content="Primeira resposta"),
        AIMessage(content="Segunda resposta")
    ]))

    # Act
    result = node_with_llm({"messages": [...]})

    # Assert
    assert result["response"] == "Primeira resposta"
```

**Trajectory Validation com AgentEvals**:
```python
from agentevals.trajectory.match import create_trajectory_match_evaluator

def test_trajectory_strict_match():
    """Teste: Trajectory corresponde exatamente √† esperada"""
    # Arrange
    evaluator = create_trajectory_match_evaluator(
        trajectory_match_mode="strict"
    )

    # Act
    evaluation = evaluator(
        outputs=actual_trajectory,
        reference_outputs=reference_trajectory
    )

    # Assert
    assert evaluation["score"] is True
```

**VCR Recording para Integration Tests**:
```python
@pytest.mark.vcr()
def test_agent_with_real_llm_vcr():
    """Teste: Grava chamadas LLM reais na primeira execu√ß√£o, replay depois"""
    # PRIMEIRA EXECU√á√ÉO: Faz chamadas reais e grava
    # PR√ìXIMAS EXECU√á√ïES: Replay sem API calls (100% determin√≠stico)

    agent = create_agent()
    result = agent.invoke({"input": "What's the capital of France?"})

    assert "Paris" in result["output"]
```

#### Dependencies Adicionais

Para usar os padr√µes LangChain/LangGraph, certifique-se de ter:

```bash
# Principais
pip install langchain langchain-core langgraph

# Testing
pip install pytest pytest-asyncio agentevals vcrpy pytest-recording
```

Ou com poetry:
```toml
[tool.poetry.group.test.dependencies]
agentevals = "^0.1.0"      # Trajectory validation
vcrpy = "^6.0.0"            # HTTP recording
pytest-recording = "^0.13.0"  # pytest-vcr integration
```

#### Modos de Trajectory Matching

- **strict**: Ordem e conte√∫do id√™nticos
- **unordered**: Mesmo conte√∫do, ordem irrelevante
- **subset**: Trajectory real cont√©m pelo menos as a√ß√µes esperadas
- **superset**: Trajectory real √© subconjunto das a√ß√µes esperadas

#### LLM-as-Judge Models

Suportados:
- `openai:gpt-4o-mini`, `openai:o3-mini`
- `anthropic:claude-3-5-sonnet`, `anthropic:claude-3-5-haiku`

---

## üìù Padr√µes de Teste Criados

### AAA Pattern (Arrange-Act-Assert)

Todos os testes seguem o padr√£o AAA:

```python
def test_exemplo(self):
    """Teste: Descri√ß√£o clara do cen√°rio"""
    # Arrange - Preparar dados e mocks
    data = {"key": "value"}
    mock_api.return_value = {"result": "ok"}

    # Act - Executar a fun√ß√£o/m√©todo
    result = function_under_test(data)

    # Assert - Validar resultado
    assert result is not None
    assert result["status"] == "ok"
    mock_api.assert_called_once()
```

### Happy Path + Erros + Edge Cases

Cada fun√ß√£o recebe pelo menos 3 tipos de testes:

```python
# 1. Happy path - Cen√°rio de sucesso
def test_process_data_success(self):
    """Teste: Processa dados v√°lidos com sucesso"""
    ...

# 2. Error handling - Tratamento de erros
def test_process_data_invalid_input(self):
    """Teste: Lida corretamente com entrada inv√°lida"""
    with pytest.raises(ValueError):
        ...

# 3. Edge cases - Casos extremos
@pytest.mark.parametrize("input,expected", [
    (None, False),
    ("", False),
    ([], False),
    ({}, False),
])
def test_process_data_edge_cases(self, input, expected):
    """Teste: Lida com casos extremos"""
    ...
```

### Mocks de Depend√™ncias Externas

Todas as depend√™ncias externas s√£o mockadas:

```python
@patch("module.requests.get")  # HTTP
@patch("module.boto3.client")  # AWS
@patch("module.ChatOpenAI")    # LLM
@patch("module.db.session")    # Database
def test_with_all_mocks(self, mock_db, mock_llm, mock_aws, mock_http):
    """Teste: Todas as depend√™ncias externas mockadas"""
    ...
```

---

## üéì Recursos Avan√ßados

### Detec√ß√£o Autom√°tica de Fixtures

O plugin l√™ automaticamente `conftest.py` e reutiliza fixtures:

```python
# conftest.py
@pytest.fixture
def sample_user():
    return {"id": 1, "name": "Test User"}

# Teste criado automaticamente usa a fixture
def test_get_user(self, sample_user):
    """Teste: Buscar usu√°rio usa fixture existente"""
    result = get_user(sample_user["id"])
    assert result["name"] == sample_user["name"]
```

### Parametriza√ß√£o Inteligente

Cria testes parametrizados quando identifica m√∫ltiplos casos:

```python
@pytest.mark.parametrize("input_value,expected_output", [
    ("valid@email.com", True),
    ("invalid-email", False),
    ("", False),
    (None, False),
    ("test@", False),
    ("@test.com", False),
])
def test_validate_email_parametrized(self, input_value, expected_output):
    """Teste: Valida√ß√£o de email com m√∫ltiplos casos"""
    result = validate_email(input_value)
    assert result == expected_output
```

### Suporte a C√≥digo Ass√≠ncrono

Detecta e cria testes ass√≠ncronos corretamente:

```python
@pytest.mark.asyncio
@patch("module.async_http_client")
async def test_fetch_data_async(self, mock_client):
    """Teste: Fun√ß√£o ass√≠ncrona funciona corretamente"""
    # Arrange
    mock_client.get.return_value = AsyncMock(
        return_value={"data": "test"}
    )

    # Act
    result = await fetch_data_async()

    # Assert
    assert result == {"data": "test"}
```

### Mocks Espec√≠ficos de LangChain

Cria mocks corretos para LangChain patterns:

```python
@patch("module.ChatPromptTemplate.from_template")
@patch("module.ChatOpenAI")
def test_langchain_chain(self, mock_chat, mock_prompt):
    """Teste: Chain LangChain funciona corretamente"""
    # Arrange - Mock de chain com pipe operator
    mock_llm = Mock()
    mock_chain = Mock()
    mock_chain.invoke.return_value = {"output": "result"}
    mock_prompt.return_value.__or__ = Mock(return_value=mock_chain)
    mock_chat.return_value = mock_llm

    # Act
    result = process_with_chain(data)

    # Assert
    assert result["output"] == "result"
    mock_chain.invoke.assert_called_once()
```

---

## üìä Configura√ß√£o de Cobertura

### Threshold Padr√£o

- **Cobertura m√≠nima global**: 80%
- **Cobertura ideal**: 85-90%
- **Arquivos cr√≠ticos**: 90%+

### Customiza√ß√£o via Arquivos de Configura√ß√£o

O plugin respeita configura√ß√µes existentes:

**pytest.ini**:
```ini
[pytest]
addopts = --cov=src --cov-report=term-missing --cov-fail-under=80
```

**pyproject.toml**:
```toml
[tool.coverage.report]
fail_under = 80
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "raise NotImplementedError",
]
```

**setup.cfg**:
```cfg
[coverage:report]
fail_under = 80
```

**.coveragerc**:
```ini
[report]
fail_under = 80
omit =
    */tests/*
    */migrations/*
```

---

## ‚ö° Modo Emp√≠rico

Este plugin opera em **modo emp√≠rico** - executa automaticamente sem perguntas.

### O que N√ÉO faz:
- ‚ùå N√£o pergunta qual framework usar
- ‚ùå N√£o pergunta quais m√≥dulos testar
- ‚ùå N√£o pergunta se deve criar testes
- ‚ùå N√£o pergunta se deve executar testes

### O que FAZ:
- ‚úÖ Detecta automaticamente todo o ambiente
- ‚úÖ Analisa cobertura imediatamente
- ‚úÖ Cria testes diretamente
- ‚úÖ Executa testes automaticamente
- ‚úÖ Reporta resultados ao final

---

## üêõ Troubleshooting

### Problema: Import errors em testes criados

**Causa**: Estrutura de imports ou `sys.path` incorreta

**Solu√ß√£o autom√°tica**: O plugin ajusta imports e adiciona `__init__.py` quando necess√°rio

### Problema: Mocks n√£o funcionam

**Causa**: Path de mocking incorreto

**Solu√ß√£o autom√°tica**: O plugin corrige paths de mocking e adiciona `spec` quando necess√°rio

### Problema: Testes ass√≠ncronos n√£o executam

**Causa**: Falta `@pytest.mark.asyncio`

**Solu√ß√£o autom√°tica**: O plugin adiciona markers e instala `pytest-asyncio` se necess√°rio

### Problema: Fixtures n√£o encontradas

**Causa**: `conftest.py` ausente ou mal localizado

**Solu√ß√£o autom√°tica**: O plugin move fixtures para local correto

---

## üìö Componentes do Plugin

### Command: `/test-coverage`
Comando principal que invoca o agente especializado

**Localiza√ß√£o**: `commands/test-coverage.md`

### Agent: `test-assistant`
Agente especializado em an√°lise e cria√ß√£o de testes

**Localiza√ß√£o**: `agents/test-assistant.md`

**Especialidades**:
- Detec√ß√£o de ambiente e frameworks
- An√°lise de cobertura
- Cria√ß√£o de testes seguindo padr√µes
- Mocks inteligentes
- Valida√ß√£o de qualidade

---

## üöÄ Roadmap

Funcionalidades planejadas para vers√µes futuras:

- [ ] Suporte a mais frameworks de teste (Robot Framework, Behave)
- [ ] Testes de integra√ß√£o autom√°ticos
- [ ] Testes E2E com Playwright/Selenium
- [ ] Mutation testing
- [ ] Performance testing
- [ ] Gera√ß√£o de relat√≥rios HTML customizados
- [ ] Integra√ß√£o com CI/CD (GitHub Actions, GitLab CI)
- [ ] Suporte a TypeScript/JavaScript
- [ ] Dashboard de cobertura

---

## üìÑ Licen√ßa

MIT License - Carlos Araujo

---

## üë§ Autor

**Carlos Araujo**
- Email: cadu.gevaerd@gmail.com
- GitHub: [@cadugevaerd](https://github.com/cadugevaerd)

---

## ü§ù Contribuindo

Contribui√ß√µes s√£o bem-vindas! Para contribuir:

1. Fork o reposit√≥rio
2. Crie uma branch para sua feature (`git checkout -b feature/nova-feature`)
3. Commit suas mudan√ßas (`git commit -m 'feat: adiciona nova feature'`)
4. Push para a branch (`git push origin feature/nova-feature`)
5. Abra um Pull Request

---

## üìû Suporte

- **Issues**: [GitHub Issues](https://github.com/cadugevaerd/claudecode_plugins/issues)
- **Documenta√ß√£o**: [claudecode_plugins](https://github.com/cadugevaerd/claudecode_plugins)

---

**Desenvolvido com ‚ù§Ô∏è para Claude Code Community**