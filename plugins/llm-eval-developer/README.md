# LLM Eval Developer

> Plugin especializado em criar benchmarks comparativos de LLMs e evaluations com LangChain/LangGraph e LangSmith para tracking autom√°tico.

## üìã Vis√£o Geral

O **LLM Eval Developer** oferece ferramentas completas para:

- **Benchmarking**: Criar suites comparativas para testar m√∫ltiplos LLMs
- **Evaluations**: Desenvolver evaluators customizados (OpenEvals, LangSmith, custom)
- **Testing**: Padr√µes e melhores pr√°ticas para testar LLMs
- **Metrics**: M√©tricas de evaluation (BLEU, ROUGE, F1, LLM-as-judge, etc)
- **Patterns**: Padr√µes de c√≥digo comuns reutiliz√°veis

## üöÄ Comandos Dispon√≠veis

### `/benchmark-llms`

Cria suite de benchmark comparativo para avaliar m√∫ltiplos LLMs usando LangChain/LangGraph e LangSmith.

```bash
/benchmark-llms
```

**O que faz**:

- Configura dataset de test cases
- Define m√©tricas de compara√ß√£o
- Cria harness de benchmark com LangSmith
- Executa benchmark contra m√∫ltiplos modelos
- Gera relat√≥rio comparativo

### `/create-eval-suite`

Gera estrutura completa de evaluation suite com dataset, evaluators e testes.

```bash
/create-eval-suite
```

**O que faz**:

- Estrutura de diret√≥rios para evaluations
- Dataset de exemplos (train/test)
- Evaluators customizados
- Test suite integrado
- Configura√ß√£o CI/CD

### `/create-evaluator`

Gera c√≥digo de evaluator customizado para LLMs (OpenEvals, LangSmith, custom).

```bash
/create-evaluator
```

**O que faz**:

- Seleciona tipo de evaluator (OpenEvals, LangSmith, custom)
- Gera c√≥digo boilerplate
- Implementa m√©trica espec√≠fica
- Adiciona docstring e exemplos
- Integra com LangSmith

### `/eval-metrics`

Lista e documenta m√©tricas de evaluation dispon√≠veis com exemplos.

```bash
/eval-metrics
```

**M√©tricas Cobertas**:

- BLEU (machine translation)
- ROUGE (text summarization)
- F1 Score (classification)
- Exact Match (QA)
- LLM-as-Judge (semantic)
- Custom metrics

### `/eval-patterns`

Mostra padr√µes de c√≥digo comuns para evaluation (dataset, testing, CI/CD).

```bash
/eval-patterns
```

**Padr√µes**:

- Dataset creation and loading
- Test case organization
- Evaluation harness
- CI/CD integration
- Reporting and visualization

### `/setup-project-eval`

Configura CLAUDE.md do projeto com padr√µes de LLM evaluation, frameworks, m√©tricas e estrutura.

```bash
/setup-project-eval
```

## ü§ñ Agentes Dispon√≠veis

### `benchmark-specialist`

Agente especializado em criar benchmarks comparativos de LLMs usando LangChain/LangGraph e LangSmith para tracking autom√°tico de m√©tricas.

**Uso**:

```
Task tool com subagent_type=benchmark-specialist
```

### `eval-developer`

Agente especializado em desenvolver evaluations de LLMs - gera c√≥digo, padr√µes e estruturas.

**Uso**:

```
Task tool com subagent_type=eval-developer
```

## üí° Skills Dispon√≠veis

### `benchmark-runner`

Executa benchmarks comparativos de LLMs usando LangChain/LangGraph e LangSmith.

**Uso**: Auto-invocado como skill via Skill tool

### `evaluation-developer`

Desenvolve c√≥digo de evaluators para LLMs (OpenEvals, LangSmith, BLEU, ROUGE, LLM-as-judge).

**Uso**: Auto-invocado como skill via Skill tool

### `langchain-test-specialist`

Cria unit e integration tests para LangChain e LangGraph com advanced mocking patterns.

**Uso**: Auto-invocado como skill via Skill tool

### `smoke-test`

Smoke testing expertise - valida√ß√£o de funcionalidade cr√≠tica com pytest markers e CI integration.

**Uso**: Auto-invocado como skill via Skill tool

## üìö Casos de Uso

### 1. Comparar M√∫ltiplos LLMs

```bash
/benchmark-llms
# Compara Claude, GPT-4, Gemini em tarefas espec√≠ficas
```

### 2. Criar Evaluation Suite Completa

```bash
/create-eval-suite
# Estrutura completa com dataset, evaluators e testes
```

### 3. Implementar M√©trica Customizada

```bash
/create-evaluator
# Cria evaluator para m√©trica espec√≠fica do seu dom√≠nio
```

### 4. Entender M√©tricas Dispon√≠veis

```bash
/eval-metrics
# Lista todas as m√©tricas com exemplos de uso
```

### 5. Consultar Padr√µes de C√≥digo

```bash
/eval-patterns
# Mostra patterns comuns reutiliz√°veis
```

### 6. Configurar Projeto para Evaluation

```bash
/setup-project-eval
# Configura CLAUDE.md com toda estrutura necess√°ria
```

## üîß Integra√ß√£o com LangSmith

Todos os componentes integram automaticamente com LangSmith para:

- ‚úÖ Tracing de execu√ß√µes
- ‚úÖ Logging autom√°tico de m√©tricas
- ‚úÖ Compara√ß√£o de runs
- ‚úÖ Visualiza√ß√£o de resultados
- ‚úÖ An√°lise de regress√£o

## üìä M√©tricas Suportadas

| M√©trica | Tipo | Caso de Uso |
|---------|------|-----------|
| BLEU | L√©xical | Machine Translation |
| ROUGE | L√©xical | Summarization |
| F1 Score | Classification | Multi-task |
| Exact Match | QA | Question Answering |
| LLM-as-Judge | Sem√¢ntico | Avalia√ß√£o geral |
| Custom | Domain-specific | Dom√≠nios espec√≠ficos |

## üõ†Ô∏è Tecnologias

- **LangChain**: Framework para LLM apps
- **LangGraph**: State management e workflows
- **LangSmith**: Observability e evaluation
- **Python**: Linguagem principal
- **Pytest**: Testing framework

## üìñ Pr√≥ximos Passos

1. **Comece com**: `/setup-project-eval` para configurar seu projeto
1. **Entenda m√©tricas**: `/eval-metrics` para ver op√ß√µes dispon√≠veis
1. **Crie evaluation suite**: `/create-eval-suite` para estrutura completa
1. **Execute benchmark**: `/benchmark-llms` para comparar modelos
1. **Implemente evaluators**: `/create-evaluator` para m√©tricas customizadas

## üìù Changelog

Veja [CHANGELOG.md](./CHANGELOG.md) para hist√≥rico de vers√µes.

## üìÑ Licen√ßa

MIT

______________________________________________________________________

**Desenvolvido para Claude Code** üöÄ
