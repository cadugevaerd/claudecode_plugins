# LLM Eval Developer

Plugin especializado para **desenvolver evaluations de LLMs e agentes**. Gera cÃ³digo de evaluators, scaffolding de evaluation suites, e ensina padrÃµes de testing.

**FOCO**: Este plugin ajuda vocÃª a **CRIAR cÃ³digo de evaluation**, nÃ£o a executar evaluations.

## ğŸ¯ O Que Este Plugin Faz

âœ… **Gera cÃ³digo de evaluators customizados** (OpenEvals, LangSmith, custom)
âœ… **Cria scaffolding completo de evaluation suites**
âœ… **Implementa mÃ©tricas** (BLEU, ROUGE, LLM-as-judge, similarity)
âœ… **Ensina padrÃµes de testing** (pytest, mocks, CI/CD)
âœ… **Documenta best practices** de evaluation

âŒ **NÃƒO** executa evaluations (apenas gera cÃ³digo para executar)
âŒ **NÃƒO** analisa resultados (apenas ensina como analisar)

## ğŸ“¦ InstalaÃ§Ã£o

````bash
/plugin marketplace add cadugevaerd/claudecode_plugins
/plugin install llm-eval-developer

```text

## ğŸš€ Funcionalidades

### 1. Criar Evaluators Customizados

Gera cÃ³digo completo de evaluators para diferentes tipos de mÃ©tricas.

**Tipos suportados**:
- **LLM-as-Judge**: Para critÃ©rios subjetivos (relevance, hallucination, coherence)
- **Similarity-based**: BLEU, ROUGE, cosine similarity
- **Rule-based**: Regex, exact match, custom logic
- **Composite**: MÃºltiplas mÃ©tricas combinadas

**Frameworks suportados**:
- OpenEvals (langchain-ai/openevals)
- LangSmith Evaluation
- Custom implementations

### 2. Scaffolding de Evaluation Suites

Cria estrutura completa de projeto de evaluation, incluindo:
- Datasets anotados (golden datasets)
- Evaluators implementados
- Testes unitÃ¡rios e integration
- Scripts de execuÃ§Ã£o
- ConfiguraÃ§Ã£o de CI/CD

### 3. DocumentaÃ§Ã£o de MÃ©tricas

Lista e documenta mÃ©tricas de evaluation disponÃ­veis, com:
- ExplicaÃ§Ã£o de como funcionam
- Quando usar cada mÃ©trica
- Exemplos de cÃ³digo completos
- Trade-offs e limitaÃ§Ãµes

### 4. Benchmarking Comparativo de LLMs (NOVO! v1.2.0)

Cria suites completas de **comparative evaluation** usando **LangChain/LangGraph** e **LangSmith**:

**Usando LangChain/LCEL**:
- Chains LCEL para comparaÃ§Ã£o de modelos
- Batch processing paralelo (`runnable.batch`)
- LangSmith `evaluate()` API para tracking automÃ¡tico
- Evaluators nativos (qa, context_qa, criteria)

**Usando LangGraph**:
- Workflows paralelos para benchmark
- State management entre mÃºltiplos LLMs
- ExecuÃ§Ã£o concorrente otimizada

**MÃ©tricas trackadas**:
- âœ… **Qualidade**: accuracy, relevance, hallucination (via LangSmith evaluators)
- âœ… **Performance**: latency P50/P95/P99, TTFT (via LangChain callbacks)
- âœ… **Custo**: token usage e costs (via LangSmith automatic tracking)

**Vantagens de usar LangChain/LangSmith**:
- Tracking automÃ¡tico de tokens, custos e traces
- Evaluators prontos (sem cÃ³digo manual)
- Comparison view no LangSmith UI
- Dataset management centralizado

### 5. PadrÃµes de Desenvolvimento

Mostra padrÃµes comuns de cÃ³digo para:
- Dataset creation (manual, synthetic, sampling)
- Testing evaluators (pytest, mocks)
- CI/CD integration (GitHub Actions)
- Regression detection
- A/B testing

## ğŸ“‹ Comandos DisponÃ­veis

### `/setup-project-eval`

**Configura CLAUDE.md do projeto** com padrÃµes de LLM evaluation.

**O que faz**:
- âœ… Cria ou atualiza `CLAUDE.md` na raiz do projeto
- âœ… Adiciona padrÃµes de evaluators (LLM-as-judge, similarity, rule-based)
- âœ… Configura frameworks detectados (OpenEvals, LangSmith, custom)
- âœ… Documenta estrutura de evaluation suites
- âœ… Orienta sobre dataset management (golden datasets, synthetic)
- âœ… Preserva conteÃºdo existente (nÃ£o sobrescreve)
- âœ… Detecta stack de evaluation automaticamente

**Uso**:

```bash

# Setup bÃ¡sico (detecta stack automaticamente)
/setup-project-eval

# Ou com descriÃ§Ã£o do tipo de evaluation
/setup-project-eval "RAG system evaluation com LangSmith + OpenAI"

```text

**Resultado**:
Claude ficarÃ¡ automaticamente orientado a:
- Desenvolver evaluators customizados
- Estruturar evaluation suites adequadamente
- Implementar mÃ©tricas corretas (LLM-as-judge, similarity, rule-based)
- Gerenciar datasets de evaluation (golden datasets, synthetic)
- Testar evaluators com pytest
- Configurar CI/CD para regression testing

**Quando usar**:
- âœ… No inÃ­cio de projetos de evaluation de LLMs
- âœ… Ao adicionar este plugin em projetos existentes
- âœ… Quando quiser padronizar evaluations no time


### `/create-evaluator`

Gera cÃ³digo de evaluator customizado.

**Uso**:

```text

/create-evaluator

Tipo: llm-as-judge
Framework: langsmith
Nome: hallucination_detector
CritÃ©rio: Detectar alucinaÃ§Ãµes comparando output com contexto

```text

**Output**: CÃ³digo Python completo do evaluator com:
- Imports necessÃ¡rios
- ImplementaÃ§Ã£o funcional
- Docstrings explicativas
- Testes unitÃ¡rios
- Exemplos de uso

**Exemplos**:

<details>
<summary>Exemplo 1: Hallucination Detector (LangSmith)</summary>

```python
from langsmith.evaluation import evaluator
from openai import OpenAI

@evaluator
def hallucination_detector(outputs: dict, inputs: dict) -> dict:
    """Detecta alucinaÃ§Ãµes comparando output com contexto."""
    answer = outputs.get("answer", "")
    context = inputs.get("context", "")

    prompt = f"""
Verifique se a RESPOSTA contÃ©m APENAS informaÃ§Ãµes do CONTEXTO.

CONTEXTO: {context}
RESPOSTA: {answer}

Retorne JSON: {{"is_factual": true/false, "score": 0.0-1.0, "reason": "..."}}
"""

    client = OpenAI()
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"},
        temperature=0.0
    )

    result = eval(response.choices[0].message.content)
    return {"score": result["score"], "comment": result["reason"]}


# Teste unitÃ¡rio
def test_hallucination_detector():
    result = hallucination_detector(
        outputs={"answer": "Paris is the capital."},
        inputs={"context": "Paris is France's capital."}
    )
    assert result["score"] >= 0.8

```text

</details>

<details>
<summary>Exemplo 2: BLEU Score Evaluator (Custom)</summary>

```python
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

def bleu_evaluator(outputs: dict, reference_outputs: dict) -> dict:
    """Calcula BLEU score para translation/generation."""
    predicted = outputs.get("translation", "").split()
    reference = reference_outputs.get("translation", "").split()

    smoothing = SmoothingFunction().method1
    score = sentence_bleu([reference], predicted, smoothing_function=smoothing)

    return {"bleu_score": score}

```text

</details>

<details>
<summary>Exemplo 3: Regex Validator (OpenEvals)</summary>

```python
import re
from openevals.types import EvaluatorResult, SimpleEvaluator

def create_email_validator() -> SimpleEvaluator:
    """Valida se output contÃ©m email vÃ¡lido."""
    email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'

    def email_evaluator(outputs: dict) -> EvaluatorResult:
        output_text = outputs.get("output", "")
        match = re.search(email_pattern, output_text)

        return {
            "score": 1.0 if match else 0.0,
            "comment": f"Email: {match.group()}" if match else "No email found"
        }

    return email_evaluator

```text

</details>


### `/create-eval-suite`

Gera scaffolding completo de evaluation suite.

**Uso**:

```text

/create-eval-suite

Nome: rag-chatbot-eval
Tipo: chatbot com RAG
MÃ©tricas: accuracy, relevance, hallucination, response_time
Framework: langsmith

```text

**Output**: Estrutura completa de diretÃ³rios e arquivos:

```text

evaluations/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ eval_config.py           # ConfiguraÃ§Ã£o e thresholds
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ golden_dataset.json      # Dataset anotado
â”‚   â””â”€â”€ dataset_generator.py     # Gerador sintÃ©tico
â”œâ”€â”€ evaluators/
â”‚   â”œâ”€â”€ accuracy_evaluator.py
â”‚   â”œâ”€â”€ relevance_evaluator.py
â”‚   â”œâ”€â”€ hallucination_evaluator.py
â”‚   â””â”€â”€ response_time_evaluator.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_evaluators.py       # Testes unitÃ¡rios
â”‚   â””â”€â”€ test_app_evaluation.py   # Integration tests
â”œâ”€â”€ results/
â”‚   â””â”€â”€ .gitkeep
â”œâ”€â”€ run_evaluation.py             # Script principal
â””â”€â”€ README.md                     # DocumentaÃ§Ã£o

```text

**Cada arquivo Ã© gerado com cÃ³digo funcional completo!**


### `/benchmark-llms` (NOVO! v1.2.0)

**Cria suite de benchmark comparativo** para mÃºltiplos LLMs usando LangChain/LangGraph.

**O que faz**:
- âœ… Gera cÃ³digo completo usando **LCEL chains** ou **LangGraph workflows**
- âœ… Integra com **LangSmith evaluate()** API para tracking automÃ¡tico
- âœ… Usa **LangSmith evaluators** nativos (qa, context_qa, criteria, custom)
- âœ… Implementa **callbacks** para mÃ©tricas de latÃªncia (P95, P99, TTFT)
- âœ… Trackeia **custos automaticamente** via LangSmith
- âœ… Gera relatÃ³rios comparativos (JSON, Markdown, HTML, CSV, LangSmith UI)

**Uso**:

```bash
/benchmark-llms

```text

O comando pergunta interativamente:
1. **Modelos**: gpt-4o, claude-3.5-sonnet, gemini-1.5-pro, etc.
2. **Dataset**: LangSmith dataset, MMLU, HumanEval, TruthfulQA, custom
3. **MÃ©tricas**: accuracy, relevance, latency, cost, robustness, safety
4. **Formato output**: json, markdown, html, csv, langsmith
5. **ExecuÃ§Ã£o**: paralela (LCEL batch) ou sequencial

**Output**: Estrutura completa de benchmark:

```text

benchmarks/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ benchmark_config.py          # Config de modelos
â”‚   â””â”€â”€ langsmith_config.py          # Config LangSmith
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ dataset_loader.py            # Loader de datasets
â”‚   â””â”€â”€ dataset_uploader.py          # Upload para LangSmith
â”œâ”€â”€ benchmarking/
â”‚   â”œâ”€â”€ langchain_benchmark.py       # Benchmark LCEL
â”‚   â”œâ”€â”€ langgraph_benchmark.py       # Benchmark LangGraph (parallel)
â”‚   â”œâ”€â”€ callbacks/
â”‚   â”‚   â””â”€â”€ latency_callback.py      # Callback P95/P99/TTFT
â”‚   â”œâ”€â”€ evaluators/
â”‚   â”‚   â””â”€â”€ langsmith_evaluators.py  # Evaluators LangSmith
â”‚   â””â”€â”€ reporters/
â”‚       â””â”€â”€ markdown_reporter.py     # RelatÃ³rios comparativos
â”œâ”€â”€ run_benchmark.py                 # Script principal
â””â”€â”€ README.md

```text

**Cada arquivo Ã© gerado com cÃ³digo funcional usando LangChain/LangSmith!**

**Exemplo de output**:

```markdown

# ğŸ“Š LLM Benchmark Report

## Winners
ğŸ¯ **Best Quality**: gpt-4o (QA: 0.872)
âš¡ **Fastest (P95)**: claude-3.5-sonnet (891ms)
ğŸ’° **Best Value**: gemini-1.5-pro (196 qa/$)

## Metrics Comparison
| Model | Accuracy | P95 Latency | Total Cost | Cost-Efficiency |
|-------|----------|-------------|------------|-----------------|
| gpt-4o | 87.2% | 1456ms | $1.15 | 75.8 qa/$ |
| claude-3.5-sonnet | 85.8% | 891ms | $0.87 | 98.6 qa/$ |
| gemini-1.5-pro | 84.3% | 1034ms | $0.43 | 196 qa/$ |

```text

**Vantagens de LangChain/LangSmith**:
- Tracking automÃ¡tico de tokens, custos, traces
- Evaluators prontos (qa, context_qa, criteria)
- Comparison view no LangSmith UI
- Dataset management centralizado
- NÃ£o precisa implementar tracking manual!


### `/eval-metrics`

Lista e documenta mÃ©tricas de evaluation disponÃ­veis.

**Uso**:

```text

/eval-metrics

# Lista todas as mÃ©tricas

# Ou filtrar por categoria:
/eval-metrics
Categoria: llm-judge

```text

**Output**: DocumentaÃ§Ã£o completa de mÃ©tricas com:

**Traditional Metrics**:
- **BLEU**: Translation, text generation
- **ROUGE**: Summarization (ROUGE-1, ROUGE-2, ROUGE-L)
- **Exact Match**: Q&A, classification
- **Regex Match**: Format validation

**Embedding-based**:
- **Cosine Similarity**: Semantic similarity

**LLM-as-Judge**:
- **Relevance**: Q&A relevance
- **Hallucination Detection**: Factual accuracy
- **Coherence**: Text coherence and flow
- **Fluency**: Language fluency

**Task-specific**:
- **Response Time**: Performance testing
- **Citation Accuracy**: RAG systems

Cada mÃ©trica inclui:
- âœ… Como funciona
- âœ… Quando usar
- âœ… CÃ³digo de implementaÃ§Ã£o
- âœ… LimitaÃ§Ãµes e trade-offs


### `/eval-patterns`

Mostra padrÃµes de cÃ³digo comuns para evaluation.

**Uso**:

```text

/eval-patterns

# Lista todos os padrÃµes

# Ou filtrar:
/eval-patterns
Tipo: testing

```text

**Output**: PadrÃµes de cÃ³digo para:

**Dataset Creation**:
- Golden dataset manual
- Synthetic data generation
- Production data sampling

**Testing Patterns**:
- Unit testing evaluators (pytest)
- Mocking LLM calls (pytest-mock)
- Integration testing (LangSmith pytest plugin)
- Parametrized tests

**CI/CD Patterns**:
- GitHub Actions evaluation
- Regression detection
- Threshold checking
- PR comments with results

**Advanced Patterns**:
- A/B testing evaluators
- Pairwise evaluation
- Composite evaluators


## ğŸ¤– Agente Especializado

### `eval-developer`

Agente focado em **desenvolvimento de evaluations**.

**Invoke via Task tool**:

```text

Task: Crie um evaluator para detectar toxicidade em chatbot responses usando LLM-as-judge. Framework: LangSmith.

```text

**O agente irÃ¡**:
1. Gerar cÃ³digo completo do evaluator
2. Incluir testes unitÃ¡rios
3. Explicar trade-offs e quando usar
4. Mostrar como integrar em pipeline

**Exemplos de tarefas**:

<details>
<summary>Tarefa 1: Criar Evaluator de RelevÃ¢ncia</summary>

```text

Task: Implemente um evaluator de relevÃ¢ncia para Q&A usando LLM-as-judge com OpenEvals.

```text

**Agente gera**:
- CÃ³digo do evaluator com prompt otimizado
- Testes unitÃ¡rios (casos positivos e negativos)
- Exemplo de integraÃ§Ã£o
- ExplicaÃ§Ã£o de quando usar vs nÃ£o usar

</details>

<details>
<summary>Tarefa 2: Implementar ROUGE Score</summary>

```text

Task: Como implemento ROUGE score para avaliar meu summarizer? Sem usar frameworks externos.

```text

**Agente gera**:
- ImplementaÃ§Ã£o custom de ROUGE
- ExplicaÃ§Ã£o de ROUGE-1, ROUGE-2, ROUGE-L
- Testes com diferentes casos
- Trade-offs e limitaÃ§Ãµes

</details>

<details>
<summary>Tarefa 3: Suite Completa de Evaluation</summary>

```text

Task: Crie evaluation suite completa para RAG system com mÃ©tricas de hallucination, relevance e citation accuracy.

```text

**Agente gera**:
- Estrutura completa de diretÃ³rios
- 3 evaluators implementados
- Dataset de exemplo
- Testes unitÃ¡rios e integration
- Script de execuÃ§Ã£o
- README com documentaÃ§Ã£o

</details>


## ğŸ§  Skill de Auto-Discovery

### `evaluation-developer`

Skill automaticamente invocada por Claude quando detectar necessidade de evaluation.

**Trigger terms**:
- "criar evaluator", "desenvolver evaluation"
- "como avaliar meu LLM", "mÃ©tricas para"
- "hallucination detection", "BLEU", "ROUGE"
- "evaluation suite", "testar chatbot"

**O que a skill faz**:
- Identifica tipo de evaluation necessÃ¡rio
- Gera cÃ³digo de evaluator apropriado
- Inclui testes e documentaÃ§Ã£o
- Explica trade-offs

**Exemplo de ativaÃ§Ã£o**:

```text

VocÃª: "Preciso detectar alucinaÃ§Ãµes no meu RAG system"

Claude (usando skill automaticamente):
Vou criar um hallucination detector usando LLM-as-judge...

[Gera cÃ³digo completo do evaluator]

```text


## ğŸ’¡ Exemplos de Uso do Plugin

### Exemplo 1: Criar Evaluator de Hallucination

```text

/create-evaluator

Tipo: llm-as-judge
Framework: langsmith
Nome: hallucination_detector
CritÃ©rio: Detectar alucinaÃ§Ãµes comparando output com contexto fornecido

```text

**Plugin gera cÃ³digo completo pronto para usar!**


### Exemplo 2: Suite para Chatbot

```text

/create-eval-suite

Nome: chatbot-evaluation
Tipo: customer support chatbot
MÃ©tricas: relevance, tone, response_time, accuracy
Framework: langsmith

```text

**Plugin cria**:
- 4 evaluators implementados
- Dataset com exemplos de conversas
- Testes unitÃ¡rios
- Script de execuÃ§Ã£o
- GitHub Actions workflow


### Exemplo 3: Consultar MÃ©tricas

```text

/eval-metrics

Categoria: llm-judge

```text

**Plugin mostra**:
- Lista de mÃ©tricas LLM-as-judge
- Como implementar cada uma
- Quando usar
- CÃ³digo de exemplo


### Exemplo 4: PadrÃµes de Testing

```text

/eval-patterns

Tipo: testing

```text

**Plugin mostra**:
- Como testar evaluators com pytest
- Patterns de mock (evitar custos de API)
- Integration testing
- Parametrized tests


## ğŸ“š Casos de Uso

### Use Case 1: RAG System Evaluation

**Necessidade**: Avaliar se RAG retorna respostas factuais sem alucinaÃ§Ãµes.

**SoluÃ§Ã£o com plugin**:

```text

1. /create-evaluator â†’ hallucination_detector (LLM-as-judge)
2. /create-evaluator â†’ relevance_evaluator (LLM-as-judge)
3. /create-evaluator â†’ citation_accuracy (rule-based)
4. /create-eval-suite â†’ rag-eval-suite

```text

**Resultado**: Suite completa de evaluation para RAG com 3 mÃ©tricas.


### Use Case 2: Summarization System

**Necessidade**: Avaliar qualidade de summaries gerados.

**SoluÃ§Ã£o com plugin**:

```text

1. /create-evaluator â†’ rouge_evaluator (similarity)
2. /create-evaluator â†’ coherence_evaluator (LLM-as-judge)
3. /create-evaluator â†’ conciseness_evaluator (rule-based)

```text

**Resultado**: 3 evaluators complementares para avaliar summaries.


### Use Case 3: Chatbot Testing

**Necessidade**: Testar chatbot em production antes de deploy.

**SoluÃ§Ã£o com plugin**:

```text

1. /create-eval-suite â†’ chatbot-eval
2. /eval-patterns â†’ ci-cd

```text

**Resultado**: Suite com CI/CD integration, executa testes automaticamente em PRs.


## ğŸ“ Melhores PrÃ¡ticas

### 1. Sempre Teste Evaluators

**Use testes unitÃ¡rios** antes de usar evaluators em production:

```python
def test_evaluator_positive_case():
    result = evaluator(outputs={"good": "answer"})
    assert result["score"] >= 0.8

def test_evaluator_negative_case():
    result = evaluator(outputs={"bad": "answer"})
    assert result["score"] <= 0.3

```text

### 2. Combine MÃºltiplas MÃ©tricas

**NÃ£o confie em uma Ãºnica mÃ©trica**. Combine:
- LLM-as-judge (qualidade subjetiva)
- Similarity metrics (comparaÃ§Ã£o com ground truth)
- Rule-based (validaÃ§Ãµes exatas)

### 3. Mock LLM Calls em Testes

**Evite custos** de API em testes usando mocks:

```python
def test_with_mock(mocker):
    mocker.patch("openai.OpenAI.chat.completions.create", return_value=mock_response)
    result = evaluator(outputs={"test": "data"})
    assert result["score"] == expected

```text

### 4. Version Your Datasets

**Track mudanÃ§as** em datasets para detectar drift:

```text

datasets/
â”œâ”€â”€ v1.0/
â”‚   â””â”€â”€ golden_dataset.json
â”œâ”€â”€ v1.1/
â”‚   â””â”€â”€ golden_dataset.json
â””â”€â”€ current/
    â””â”€â”€ golden_dataset.json

```text

### 5. Automate in CI/CD

**Execute evaluations automaticamente** em PRs:

```yaml

# .github/workflows/evaluation.yml
- name: Run Evaluations
  run: pytest tests/evaluations/ --langsmith

```text


## ğŸ”§ IntegraÃ§Ã£o com Frameworks

### OpenEvals

```python
from openevals.llm import create_llm_as_judge

evaluator = create_llm_as_judge(
    prompt=YOUR_PROMPT,
    model="openai:gpt-4o-mini",
)

```text

### LangSmith

```python
from langsmith import evaluate
from langsmith.evaluation import evaluator

@evaluator
def custom_evaluator(outputs: dict) -> dict:
    return {"score": score}

results = evaluate(
    your_app,
    data="dataset-name",
    evaluators=[custom_evaluator]
)

```text

### Pytest Integration

```python
@pytest.mark.langsmith
def test_evaluation():
    results = evaluate(app, data=dataset)
    assert results["metric"]["mean"] >= threshold

```text


## ğŸ“– ReferÃªncias

### DocumentaÃ§Ã£o
- [OpenEvals GitHub](https://github.com/langchain-ai/openevals)
- [LangSmith Evaluation](https://docs.smith.langchain.com/evaluation)
- [LLM Evaluation Metrics](https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation)
- [LLM-as-a-Judge Guide](https://www.evidentlyai.com/llm-guide/llm-as-a-judge)

### Papers
- BLEU: [Papineni et al., 2002](https://aclanthology.org/P02-1040/)
- ROUGE: [Lin, 2004](https://aclanthology.org/W04-1013/)
- G-Eval: [Liu et al., 2023](https://arxiv.org/abs/2303.16634)


## ğŸ¤ Contribuindo

Este plugin faz parte do [claudecode_plugins marketplace](https://github.com/cadugevaerd/claudecode_plugins).

## ğŸ‘¤ Autor

**Carlos Araujo**
- Email: cadu.gevaerd@gmail.com
- GitHub: [@cadugevaerd](https://github.com/cadugevaerd)

## ğŸ“„ LicenÃ§a

MIT


**Desenvolvido para ajudar vocÃª a CRIAR evaluations de qualidade para seus LLMs! ğŸš€**
````
