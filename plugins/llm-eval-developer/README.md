# LLM Eval Developer

Plugin especializado para **desenvolver evaluations de LLMs e agentes**. Gera cÃ³digo de evaluators, scaffolding de evaluation suites, e ensina padrÃµes de testing.

**FOCO**: Este plugin ajuda vocÃª a **CRIAR cÃ³digo de evaluation**, nÃ£o a executar evaluations.

## ğŸ¯ O Que Este Plugin Faz

âœ… **Gera cÃ³digo de evaluators customizados** (OpenEvals, LangSmith, custom)
âœ… **Cria scaffolding completo de evaluation suites**
âœ… **Implementa mÃ©tricas** (BLEU, ROUGE, LLM-as-judge, similarity)
âœ… **Ensina padrÃµes de testing** (pytest, mocks, CI/CD)
âœ… **Documenta best practices** de evaluation

âŒ **NÃƒO** executa evaluations (apenas gera cÃ³digo para executar)
âŒ **NÃƒO** analisa resultados (apenas ensina como analisar)

## ğŸ“¦ InstalaÃ§Ã£o

```bash
/plugin marketplace add cadugevaerd/claudecode_plugins
/plugin install llm-eval-developer
```

## ğŸš€ Funcionalidades

### 1. Criar Evaluators Customizados

Gera cÃ³digo completo de evaluators para diferentes tipos de mÃ©tricas.

**Tipos suportados**:
- **LLM-as-Judge**: Para critÃ©rios subjetivos (relevance, hallucination, coherence)
- **Similarity-based**: BLEU, ROUGE, cosine similarity
- **Rule-based**: Regex, exact match, custom logic
- **Composite**: MÃºltiplas mÃ©tricas combinadas

**Frameworks suportados**:
- OpenEvals (langchain-ai/openevals)
- LangSmith Evaluation
- Custom implementations

### 2. Scaffolding de Evaluation Suites

Cria estrutura completa de projeto de evaluation, incluindo:
- Datasets anotados (golden datasets)
- Evaluators implementados
- Testes unitÃ¡rios e integration
- Scripts de execuÃ§Ã£o
- ConfiguraÃ§Ã£o de CI/CD

### 3. DocumentaÃ§Ã£o de MÃ©tricas

Lista e documenta mÃ©tricas de evaluation disponÃ­veis, com:
- ExplicaÃ§Ã£o de como funcionam
- Quando usar cada mÃ©trica
- Exemplos de cÃ³digo completos
- Trade-offs e limitaÃ§Ãµes

### 4. PadrÃµes de Desenvolvimento

Mostra padrÃµes comuns de cÃ³digo para:
- Dataset creation (manual, synthetic, sampling)
- Testing evaluators (pytest, mocks)
- CI/CD integration (GitHub Actions)
- Regression detection
- A/B testing

## ğŸ“‹ Comandos DisponÃ­veis

### `/setup-project-eval`

**Configura CLAUDE.md do projeto** com padrÃµes de LLM evaluation.

**O que faz**:
- âœ… Cria ou atualiza `CLAUDE.md` na raiz do projeto
- âœ… Adiciona padrÃµes de evaluators (LLM-as-judge, similarity, rule-based)
- âœ… Configura frameworks detectados (OpenEvals, LangSmith, custom)
- âœ… Documenta estrutura de evaluation suites
- âœ… Orienta sobre dataset management (golden datasets, synthetic)
- âœ… Preserva conteÃºdo existente (nÃ£o sobrescreve)
- âœ… Detecta stack de evaluation automaticamente

**Uso**:
```bash
# Setup bÃ¡sico (detecta stack automaticamente)
/setup-project-eval

# Ou com descriÃ§Ã£o do tipo de evaluation
/setup-project-eval "RAG system evaluation com LangSmith + OpenAI"
```

**Resultado**:
Claude ficarÃ¡ automaticamente orientado a:
- Desenvolver evaluators customizados
- Estruturar evaluation suites adequadamente
- Implementar mÃ©tricas corretas (LLM-as-judge, similarity, rule-based)
- Gerenciar datasets de evaluation (golden datasets, synthetic)
- Testar evaluators com pytest
- Configurar CI/CD para regression testing

**Quando usar**:
- âœ… No inÃ­cio de projetos de evaluation de LLMs
- âœ… Ao adicionar este plugin em projetos existentes
- âœ… Quando quiser padronizar evaluations no time

---

### `/create-evaluator`

Gera cÃ³digo de evaluator customizado.

**Uso**:
```
/create-evaluator

Tipo: llm-as-judge
Framework: langsmith
Nome: hallucination_detector
CritÃ©rio: Detectar alucinaÃ§Ãµes comparando output com contexto
```

**Output**: CÃ³digo Python completo do evaluator com:
- Imports necessÃ¡rios
- ImplementaÃ§Ã£o funcional
- Docstrings explicativas
- Testes unitÃ¡rios
- Exemplos de uso

**Exemplos**:

<details>
<summary>Exemplo 1: Hallucination Detector (LangSmith)</summary>

```python
from langsmith.evaluation import evaluator
from openai import OpenAI

@evaluator
def hallucination_detector(outputs: dict, inputs: dict) -> dict:
    """Detecta alucinaÃ§Ãµes comparando output com contexto."""
    answer = outputs.get("answer", "")
    context = inputs.get("context", "")

    prompt = f"""
Verifique se a RESPOSTA contÃ©m APENAS informaÃ§Ãµes do CONTEXTO.

CONTEXTO: {context}
RESPOSTA: {answer}

Retorne JSON: {{"is_factual": true/false, "score": 0.0-1.0, "reason": "..."}}
"""

    client = OpenAI()
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"},
        temperature=0.0
    )

    result = eval(response.choices[0].message.content)
    return {"score": result["score"], "comment": result["reason"]}


# Teste unitÃ¡rio
def test_hallucination_detector():
    result = hallucination_detector(
        outputs={"answer": "Paris is the capital."},
        inputs={"context": "Paris is France's capital."}
    )
    assert result["score"] >= 0.8
```

</details>

<details>
<summary>Exemplo 2: BLEU Score Evaluator (Custom)</summary>

```python
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

def bleu_evaluator(outputs: dict, reference_outputs: dict) -> dict:
    """Calcula BLEU score para translation/generation."""
    predicted = outputs.get("translation", "").split()
    reference = reference_outputs.get("translation", "").split()

    smoothing = SmoothingFunction().method1
    score = sentence_bleu([reference], predicted, smoothing_function=smoothing)

    return {"bleu_score": score}
```

</details>

<details>
<summary>Exemplo 3: Regex Validator (OpenEvals)</summary>

```python
import re
from openevals.types import EvaluatorResult, SimpleEvaluator

def create_email_validator() -> SimpleEvaluator:
    """Valida se output contÃ©m email vÃ¡lido."""
    email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'

    def email_evaluator(outputs: dict) -> EvaluatorResult:
        output_text = outputs.get("output", "")
        match = re.search(email_pattern, output_text)

        return {
            "score": 1.0 if match else 0.0,
            "comment": f"Email: {match.group()}" if match else "No email found"
        }

    return email_evaluator
```

</details>

---

### `/create-eval-suite`

Gera scaffolding completo de evaluation suite.

**Uso**:
```
/create-eval-suite

Nome: rag-chatbot-eval
Tipo: chatbot com RAG
MÃ©tricas: accuracy, relevance, hallucination, response_time
Framework: langsmith
```

**Output**: Estrutura completa de diretÃ³rios e arquivos:

```
evaluations/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ eval_config.py           # ConfiguraÃ§Ã£o e thresholds
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ golden_dataset.json      # Dataset anotado
â”‚   â””â”€â”€ dataset_generator.py     # Gerador sintÃ©tico
â”œâ”€â”€ evaluators/
â”‚   â”œâ”€â”€ accuracy_evaluator.py
â”‚   â”œâ”€â”€ relevance_evaluator.py
â”‚   â”œâ”€â”€ hallucination_evaluator.py
â”‚   â””â”€â”€ response_time_evaluator.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_evaluators.py       # Testes unitÃ¡rios
â”‚   â””â”€â”€ test_app_evaluation.py   # Integration tests
â”œâ”€â”€ results/
â”‚   â””â”€â”€ .gitkeep
â”œâ”€â”€ run_evaluation.py             # Script principal
â””â”€â”€ README.md                     # DocumentaÃ§Ã£o
```

**Cada arquivo Ã© gerado com cÃ³digo funcional completo!**

---

### `/eval-metrics`

Lista e documenta mÃ©tricas de evaluation disponÃ­veis.

**Uso**:
```
/eval-metrics

# Lista todas as mÃ©tricas

# Ou filtrar por categoria:
/eval-metrics
Categoria: llm-judge
```

**Output**: DocumentaÃ§Ã£o completa de mÃ©tricas com:

**Traditional Metrics**:
- **BLEU**: Translation, text generation
- **ROUGE**: Summarization (ROUGE-1, ROUGE-2, ROUGE-L)
- **Exact Match**: Q&A, classification
- **Regex Match**: Format validation

**Embedding-based**:
- **Cosine Similarity**: Semantic similarity

**LLM-as-Judge**:
- **Relevance**: Q&A relevance
- **Hallucination Detection**: Factual accuracy
- **Coherence**: Text coherence and flow
- **Fluency**: Language fluency

**Task-specific**:
- **Response Time**: Performance testing
- **Citation Accuracy**: RAG systems

Cada mÃ©trica inclui:
- âœ… Como funciona
- âœ… Quando usar
- âœ… CÃ³digo de implementaÃ§Ã£o
- âœ… LimitaÃ§Ãµes e trade-offs

---

### `/eval-patterns`

Mostra padrÃµes de cÃ³digo comuns para evaluation.

**Uso**:
```
/eval-patterns

# Lista todos os padrÃµes

# Ou filtrar:
/eval-patterns
Tipo: testing
```

**Output**: PadrÃµes de cÃ³digo para:

**Dataset Creation**:
- Golden dataset manual
- Synthetic data generation
- Production data sampling

**Testing Patterns**:
- Unit testing evaluators (pytest)
- Mocking LLM calls (pytest-mock)
- Integration testing (LangSmith pytest plugin)
- Parametrized tests

**CI/CD Patterns**:
- GitHub Actions evaluation
- Regression detection
- Threshold checking
- PR comments with results

**Advanced Patterns**:
- A/B testing evaluators
- Pairwise evaluation
- Composite evaluators

---

## ğŸ¤– Agente Especializado

### `eval-developer`

Agente focado em **desenvolvimento de evaluations**.

**Invoke via Task tool**:
```
Task: Crie um evaluator para detectar toxicidade em chatbot responses usando LLM-as-judge. Framework: LangSmith.
```

**O agente irÃ¡**:
1. Gerar cÃ³digo completo do evaluator
2. Incluir testes unitÃ¡rios
3. Explicar trade-offs e quando usar
4. Mostrar como integrar em pipeline

**Exemplos de tarefas**:

<details>
<summary>Tarefa 1: Criar Evaluator de RelevÃ¢ncia</summary>

```
Task: Implemente um evaluator de relevÃ¢ncia para Q&A usando LLM-as-judge com OpenEvals.
```

**Agente gera**:
- CÃ³digo do evaluator com prompt otimizado
- Testes unitÃ¡rios (casos positivos e negativos)
- Exemplo de integraÃ§Ã£o
- ExplicaÃ§Ã£o de quando usar vs nÃ£o usar

</details>

<details>
<summary>Tarefa 2: Implementar ROUGE Score</summary>

```
Task: Como implemento ROUGE score para avaliar meu summarizer? Sem usar frameworks externos.
```

**Agente gera**:
- ImplementaÃ§Ã£o custom de ROUGE
- ExplicaÃ§Ã£o de ROUGE-1, ROUGE-2, ROUGE-L
- Testes com diferentes casos
- Trade-offs e limitaÃ§Ãµes

</details>

<details>
<summary>Tarefa 3: Suite Completa de Evaluation</summary>

```
Task: Crie evaluation suite completa para RAG system com mÃ©tricas de hallucination, relevance e citation accuracy.
```

**Agente gera**:
- Estrutura completa de diretÃ³rios
- 3 evaluators implementados
- Dataset de exemplo
- Testes unitÃ¡rios e integration
- Script de execuÃ§Ã£o
- README com documentaÃ§Ã£o

</details>

---

## ğŸ§  Skill de Auto-Discovery

### `evaluation-developer`

Skill automaticamente invocada por Claude quando detectar necessidade de evaluation.

**Trigger terms**:
- "criar evaluator", "desenvolver evaluation"
- "como avaliar meu LLM", "mÃ©tricas para"
- "hallucination detection", "BLEU", "ROUGE"
- "evaluation suite", "testar chatbot"

**O que a skill faz**:
- Identifica tipo de evaluation necessÃ¡rio
- Gera cÃ³digo de evaluator apropriado
- Inclui testes e documentaÃ§Ã£o
- Explica trade-offs

**Exemplo de ativaÃ§Ã£o**:

```
VocÃª: "Preciso detectar alucinaÃ§Ãµes no meu RAG system"

Claude (usando skill automaticamente):
Vou criar um hallucination detector usando LLM-as-judge...

[Gera cÃ³digo completo do evaluator]
```

---

## ğŸ’¡ Exemplos de Uso do Plugin

### Exemplo 1: Criar Evaluator de Hallucination

```
/create-evaluator

Tipo: llm-as-judge
Framework: langsmith
Nome: hallucination_detector
CritÃ©rio: Detectar alucinaÃ§Ãµes comparando output com contexto fornecido
```

**Plugin gera cÃ³digo completo pronto para usar!**

---

### Exemplo 2: Suite para Chatbot

```
/create-eval-suite

Nome: chatbot-evaluation
Tipo: customer support chatbot
MÃ©tricas: relevance, tone, response_time, accuracy
Framework: langsmith
```

**Plugin cria**:
- 4 evaluators implementados
- Dataset com exemplos de conversas
- Testes unitÃ¡rios
- Script de execuÃ§Ã£o
- GitHub Actions workflow

---

### Exemplo 3: Consultar MÃ©tricas

```
/eval-metrics

Categoria: llm-judge
```

**Plugin mostra**:
- Lista de mÃ©tricas LLM-as-judge
- Como implementar cada uma
- Quando usar
- CÃ³digo de exemplo

---

### Exemplo 4: PadrÃµes de Testing

```
/eval-patterns

Tipo: testing
```

**Plugin mostra**:
- Como testar evaluators com pytest
- Patterns de mock (evitar custos de API)
- Integration testing
- Parametrized tests

---

## ğŸ“š Casos de Uso

### Use Case 1: RAG System Evaluation

**Necessidade**: Avaliar se RAG retorna respostas factuais sem alucinaÃ§Ãµes.

**SoluÃ§Ã£o com plugin**:
```
1. /create-evaluator â†’ hallucination_detector (LLM-as-judge)
2. /create-evaluator â†’ relevance_evaluator (LLM-as-judge)
3. /create-evaluator â†’ citation_accuracy (rule-based)
4. /create-eval-suite â†’ rag-eval-suite
```

**Resultado**: Suite completa de evaluation para RAG com 3 mÃ©tricas.

---

### Use Case 2: Summarization System

**Necessidade**: Avaliar qualidade de summaries gerados.

**SoluÃ§Ã£o com plugin**:
```
1. /create-evaluator â†’ rouge_evaluator (similarity)
2. /create-evaluator â†’ coherence_evaluator (LLM-as-judge)
3. /create-evaluator â†’ conciseness_evaluator (rule-based)
```

**Resultado**: 3 evaluators complementares para avaliar summaries.

---

### Use Case 3: Chatbot Testing

**Necessidade**: Testar chatbot em production antes de deploy.

**SoluÃ§Ã£o com plugin**:
```
1. /create-eval-suite â†’ chatbot-eval
2. /eval-patterns â†’ ci-cd
```

**Resultado**: Suite com CI/CD integration, executa testes automaticamente em PRs.

---

## ğŸ“ Melhores PrÃ¡ticas

### 1. Sempre Teste Evaluators

**Use testes unitÃ¡rios** antes de usar evaluators em production:

```python
def test_evaluator_positive_case():
    result = evaluator(outputs={"good": "answer"})
    assert result["score"] >= 0.8

def test_evaluator_negative_case():
    result = evaluator(outputs={"bad": "answer"})
    assert result["score"] <= 0.3
```

### 2. Combine MÃºltiplas MÃ©tricas

**NÃ£o confie em uma Ãºnica mÃ©trica**. Combine:
- LLM-as-judge (qualidade subjetiva)
- Similarity metrics (comparaÃ§Ã£o com ground truth)
- Rule-based (validaÃ§Ãµes exatas)

### 3. Mock LLM Calls em Testes

**Evite custos** de API em testes usando mocks:

```python
def test_with_mock(mocker):
    mocker.patch("openai.OpenAI.chat.completions.create", return_value=mock_response)
    result = evaluator(outputs={"test": "data"})
    assert result["score"] == expected
```

### 4. Version Your Datasets

**Track mudanÃ§as** em datasets para detectar drift:
```
datasets/
â”œâ”€â”€ v1.0/
â”‚   â””â”€â”€ golden_dataset.json
â”œâ”€â”€ v1.1/
â”‚   â””â”€â”€ golden_dataset.json
â””â”€â”€ current/
    â””â”€â”€ golden_dataset.json
```

### 5. Automate in CI/CD

**Execute evaluations automaticamente** em PRs:
```yaml
# .github/workflows/evaluation.yml
- name: Run Evaluations
  run: pytest tests/evaluations/ --langsmith
```

---

## ğŸ”§ IntegraÃ§Ã£o com Frameworks

### OpenEvals

```python
from openevals.llm import create_llm_as_judge

evaluator = create_llm_as_judge(
    prompt=YOUR_PROMPT,
    model="openai:gpt-4o-mini",
)
```

### LangSmith

```python
from langsmith import evaluate
from langsmith.evaluation import evaluator

@evaluator
def custom_evaluator(outputs: dict) -> dict:
    return {"score": score}

results = evaluate(
    your_app,
    data="dataset-name",
    evaluators=[custom_evaluator]
)
```

### Pytest Integration

```python
@pytest.mark.langsmith
def test_evaluation():
    results = evaluate(app, data=dataset)
    assert results["metric"]["mean"] >= threshold
```

---

## ğŸ“– ReferÃªncias

### DocumentaÃ§Ã£o
- [OpenEvals GitHub](https://github.com/langchain-ai/openevals)
- [LangSmith Evaluation](https://docs.smith.langchain.com/evaluation)
- [LLM Evaluation Metrics](https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation)
- [LLM-as-a-Judge Guide](https://www.evidentlyai.com/llm-guide/llm-as-a-judge)

### Papers
- BLEU: [Papineni et al., 2002](https://aclanthology.org/P02-1040/)
- ROUGE: [Lin, 2004](https://aclanthology.org/W04-1013/)
- G-Eval: [Liu et al., 2023](https://arxiv.org/abs/2303.16634)

---

## ğŸ¤ Contribuindo

Este plugin faz parte do [claudecode_plugins marketplace](https://github.com/cadugevaerd/claudecode_plugins).

## ğŸ‘¤ Autor

**Carlos Araujo**
- Email: cadu.gevaerd@gmail.com
- GitHub: [@cadugevaerd](https://github.com/cadugevaerd)

## ğŸ“„ LicenÃ§a

MIT

---

**Desenvolvido para ajudar vocÃª a CRIAR evaluations de qualidade para seus LLMs! ğŸš€**