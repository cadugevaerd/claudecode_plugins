---
description: Create automated evaluation script for LLM golden datasets with LangSmith integration
allowed-tools: Read, Write, Bash, Glob, Grep, Skill
model: claude-sonnet-4-5
argument-hint: '[EVALUATORS_DIR] [DATASETS_DIR]'
---

# Create Script Evals

Cria script automatizado que coleta informa√ß√µes de skills, envia datasets para LangSmith, executa quick-evals usando **LLM-as-Judge com crit√©rios integrados do LangSmith**, e extrai m√©tricas com pondera√ß√£o customizada.

## üéØ Objetivo

Este comando gera um script Python completo que:

- üîç Coleta informa√ß√µes de skills necess√°rias do projeto (incluindo `llm-as-a-judge`)
- üìä **Analisa datasets** e seleciona crit√©rios LLM-as-Judge apropriados (CORRECTNESS, RELEVANCE, CONCISENESS, COHERENCE, HELPFULNESS, HARMFULNESS, MALICIOUSNESS, CONTROVERSIALITY)
- üì§ Envia datasets da pasta `datasets/` para LangSmith (skip se j√° existir)
- ‚ö° Configura e executa quick-evals usando `create_llm_as_judge` do LangSmith
- üìä Extrai m√©tricas de execu√ß√£o com pondera√ß√£o configur√°vel
- üéØ Retorna score total baseado em pesos customizados

**Nota sobre o modelo:** Este comando usa **Sonnet 4.5** porque requer:

- Racioc√≠nio complexo para integrar m√∫ltiplas skills (evaluation-developer, benchmark-runner)
- An√°lise de estrutura de datasets e mapeamento para LangSmith API
- Gera√ß√£o de c√≥digo Python com error handling robusto e patterns avan√ßados
- Compreens√£o de m√©tricas de avalia√ß√£o e c√°lculo de scores ponderados
- Valida√ß√£o de configura√ß√µes e troubleshooting de integra√ß√£o com APIs externas

## üîß Instru√ß√µes

### Passo 1: Coletar Informa√ß√µes de Skills

1.1 **Executar Skills Relevantes**

Usar `Skill` tool para coletar conhecimento especializado:

```bash
Skill(skill="llm-eval-developer:llm-as-a-judge")
Skill(skill="llm-eval-developer:evals-automator")
Skill(skill="llm-eval-developer:datasets-evals")
Skill(skill="llm-eval-developer:quick-evals")
```

1.2 **Extrair Patterns de Evaluation**

Das skills, extrair:

- **LLM-as-Judge com `create_llm_as_judge`**: Usar fun√ß√£o helper do LangSmith
- **Crit√©rios integrados do LangSmith**: CORRECTNESS, RELEVANCE, CONCISENESS, COHERENCE, HELPFULNESS, HARMFULNESS, MALICIOUSNESS, CONTROVERSIALITY
- **Mapeamento de chaves**: input_keys, reference_output_keys, prediction_key
- Quando usar cada crit√©rio de avalia√ß√£o
- Integra√ß√£o com LangSmith API via `langsmith.evaluate()`
- Patterns de dataset upload
- M√©tricas dispon√≠veis (accuracy, relevance, latency, cost, errors)
- Como analisar estrutura de datasets para selecionar crit√©rios apropriados

### Passo 2: Analisar Estrutura Atual

2.1 **Verificar Diret√≥rios**

- Usar `Glob` para verificar se `evaluators/scripts/` existe
- Criar diret√≥rio se n√£o existir usando `Bash(mkdir -p evaluators/scripts/)`
- Verificar se `datasets/` existe e cont√©m arquivos

2.2 **Escanear e Analisar Datasets Existentes**

- Usar `Glob` para listar arquivos em `datasets/` (JSON, JSONL, CSV)
- **Para cada dataset**, usar `Read` para analisar estrutura completa:
  - Ler primeiro exemplo do dataset
  - Detectar campos de input e output
  - Identificar tipo de tarefa (Q&A, summarization, classification, generation)
  - Verificar se h√° reference outputs (ground truth)
  - Analisar formato dos outputs esperados (texto livre, JSON estruturado, categorias)
  - Determinar natureza da avalia√ß√£o necess√°ria (objetiva vs subjetiva)
- Determinar schema necess√°rio para LangSmith
- **Para cada dataset, decidir tipo de evaluator apropriado**:
  - **Similarity-based** (BLEU, ROUGE, embedding): Se h√° reference outputs exatos
  - **Rule-based** (regex, exact match): Se outputs t√™m formato fixo/valid√°vel
  - **LLM-as-Judge**: Se crit√©rios s√£o subjetivos, complexos ou sem ground truth
  - **Composite**: Se precisa avaliar m√∫ltiplos aspectos

2.3 **Documentar Decis√µes de Evaluators e Crit√©rios LLM-as-Judge**

- Criar dict mapeando cada dataset para seus evaluators e crit√©rios LLM-as-Judge recomendados
- **Para LLM-as-Judge**: Selecionar crit√©rio apropriado baseado na natureza do dataset
- Exemplo:
  ```python
  dataset_evaluators = {
      "qa-dataset": {
          "type": "llm_as_judge",
          "criteria": "CORRECTNESS",  # Precis√£o factual para Q&A
          "input_keys": ["question"],
          "reference_output_keys": ["expected_answer"],
          "prediction_key": "answer"
      },
      "summary-dataset": {
          "type": "llm_as_judge",
          "criteria": "CONCISENESS",  # Brevidade para summarization
          "input_keys": ["text"],
          "reference_output_keys": ["summary"],
          "prediction_key": "output"
      },
      "chatbot-dataset": {
          "type": "llm_as_judge",
          "criteria": "HELPFULNESS",  # Utilidade para assistentes
          "input_keys": ["user_message"],
          "reference_output_keys": None,  # Sem ground truth
          "prediction_key": "response"
      },
      "safety-test": {
          "type": "llm_as_judge",
          "criteria": "HARMFULNESS",  # Teste de seguran√ßa
          "input_keys": ["prompt"],
          "reference_output_keys": None,
          "prediction_key": "completion"
      }
  }
  ```
- **Guia de Sele√ß√£o de Crit√©rios**:
  - `CORRECTNESS`: Q&A, RAG, extra√ß√£o de fatos (requer ground truth)
  - `RELEVANCE`: Verificar alinhamento com pergunta/contexto
  - `CONCISENESS`: Summarization, chatbots (respostas breves)
  - `COHERENCE`: Gera√ß√£o de texto longo, artigos
  - `HELPFULNESS`: Assistentes, chatbots (avalia√ß√£o geral)
  - `HARMFULNESS`: Safety, guardrails (detectar conte√∫do prejudicial)
  - `MALICIOUSNESS`: Detectar inten√ß√£o maliciosa ou enganosa
  - `CONTROVERSIALITY`: Modera√ß√£o de conte√∫do
- Essa informa√ß√£o ser√° usada para gerar o script `quick_evals.py` customizado com `create_llm_as_judge`

### Passo 3: Criar Script de Upload de Datasets

3.1 **Gerar `upload_datasets.py`**

Criar script que:

```python
"""
Upload Datasets para LangSmith
Skip se dataset j√° existir no LangSmith.
"""

import sys
import os
import json
from pathlib import Path
from dotenv import load_dotenv
from langsmith import Client

# Add project root to Python path for imports
project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

# Carregar vari√°veis de ambiente do arquivo .env
load_dotenv()

def upload_dataset_to_langsmith(dataset_path: Path, client: Client):
    """
    Faz upload de dataset local para LangSmith.

    Args:
        dataset_path: Path para arquivo do dataset (JSON/JSONL)
        client: LangSmith Client

    Returns:
        str: Dataset name no LangSmith
    """
    dataset_name = dataset_path.stem

    # Check if exists
    try:
        existing = client.read_dataset(dataset_name=dataset_name)
        print(f"‚úÖ Dataset '{dataset_name}' j√° existe. Skipping upload.")
        return dataset_name
    except:
        pass

    # Load local dataset
    with open(dataset_path, 'r') as f:
        if dataset_path.suffix == '.jsonl':
            examples = [json.loads(line) for line in f]
        else:
            examples = json.load(f)

    # Create dataset
    dataset = client.create_dataset(dataset_name=dataset_name)

    # Upload examples
    for example in examples:
        client.create_example(
            dataset_id=dataset.id,
            inputs=example.get("input", {}),
            outputs=example.get("output", {})
        )

    print(f"üì§ Uploaded {len(examples)} examples to '{dataset_name}'")
    return dataset_name


def main():
    """Upload all datasets from datasets/ directory"""
    client = Client()
    datasets_dir = Path("datasets")

    if not datasets_dir.exists():
        print("‚ùå Directory 'datasets/' not found")
        return

    dataset_files = list(datasets_dir.glob("*.json")) + list(datasets_dir.glob("*.jsonl"))

    if not dataset_files:
        print("‚ö†Ô∏è  No datasets found in datasets/")
        return

    uploaded = []
    for dataset_file in dataset_files:
        name = upload_dataset_to_langsmith(dataset_file, client)
        uploaded.append(name)

    print(f"\n‚úÖ Upload complete. {len(uploaded)} datasets available in LangSmith:")
    for name in uploaded:
        print(f"  - {name}")

if __name__ == "__main__":
    main()
```

3.2 **Salvar Script**

Usar `Write` para criar `evaluators/scripts/upload_datasets.py`

### Passo 4: Criar Script de Quick Evals

4.1 **Preencher Configura√ß√£o de Evaluators**

Usar o mapeamento criado no Passo 2.3 (`dataset_evaluators`) para popular a constante `DATASET_EVALUATORS` no script.

Exemplo de preenchimento:
```python
DATASET_EVALUATORS = {
    "qa-golden-set": ["qa", "context_qa"],
    "summary-eval": ["llm_as_judge"],
    "code-generation": ["llm_as_judge", "embedding_distance"],
}
```

4.2 **Gerar `quick_evals.py`**

Criar script que executa evaluations sobre golden datasets usando a configura√ß√£o de evaluators:

```python
"""
Quick Evaluations sobre Golden Datasets no LangSmith
Executa evaluations e extrai m√©tricas com pondera√ß√£o customizada.
"""

import sys
import os
from typing import Dict, List
from pathlib import Path
from dotenv import load_dotenv
from langsmith import Client
from langsmith.evaluation import evaluate, LangChainStringEvaluator
from langchain_core.callbacks import BaseCallbackHandler
import numpy as np
import time
import json

# Add project root to Python path for imports
project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

# Carregar vari√°veis de ambiente do arquivo .env
load_dotenv()

# ==================== CONFIGURA√á√ÉO DE PESOS ====================

METRIC_WEIGHTS = {
    "accuracy": 0.45,      # 45% - Precis√£o das respostas
    "relevance": 0.20,     # 20% - Relev√¢ncia do conte√∫do
    "latency": 0.15,       # 15% - Tempo de resposta (P95)
    "cost": 0.15,          # 15% - Custo por execu√ß√£o
    "errors": 0.05         # 5%  - Taxa de erro
}

# Validar que soma = 100%
assert abs(sum(METRIC_WEIGHTS.values()) - 1.0) < 0.001, "Weights must sum to 1.0"


# ==================== DATASET EVALUATOR CONFIGURATION ====================

# Configura√ß√£o de evaluators por dataset
# Esta configura√ß√£o √© gerada automaticamente pelo comando /create-script-evals
# baseado na an√°lise dos datasets em datasets/
DATASET_EVALUATORS = {
    # Exemplo:
    # "qa-dataset": ["qa", "context_qa"],
    # "summary-dataset": ["llm_as_judge"],
    # "generation-dataset": ["llm_as_judge", "embedding_distance"]
}

# ==================== EVALUATORS ====================

# LLM-as-Judge usando create_llm_as_judge do LangSmith
from langsmith.evaluation import create_llm_as_judge


class LatencyCallback(BaseCallbackHandler):
    """Track latency metrics for P95 calculation"""
    def __init__(self):
        self.latencies = []
        self.start = None

    def on_llm_start(self, *args, **kwargs):
        self.start = time.perf_counter()

    def on_llm_end(self, *args, **kwargs):
        if self.start:
            latency_ms = (time.perf_counter() - self.start) * 1000
            self.latencies.append(latency_ms)
            self.start = None

    def get_p95(self):
        """Get P95 latency in milliseconds"""
        return np.percentile(self.latencies, 95) if self.latencies else 0


def normalize_score(value: float, min_val: float, max_val: float, invert: bool = False) -> float:
    """
    Normaliza score para 0-1.

    Args:
        value: Valor atual
        min_val: Valor m√≠nimo esperado
        max_val: Valor m√°ximo esperado
        invert: Se True, valores menores = score maior (para latency/cost)

    Returns:
        float: Score normalizado 0-1
    """
    if max_val == min_val:
        return 1.0

    normalized = (value - min_val) / (max_val - min_val)
    normalized = max(0.0, min(1.0, normalized))  # Clamp 0-1

    if invert:
        normalized = 1.0 - normalized

    return normalized


# ==================== MAIN EVALUATION ====================

def select_evaluators_for_dataset(dataset_name: str) -> List:
    """
    Seleciona evaluators apropriados baseado na configura√ß√£o DATASET_EVALUATORS.

    Esta configura√ß√£o √© gerada automaticamente pelo comando /create-script-evals
    ap√≥s analisar a estrutura de cada dataset e escolher crit√©rios LLM-as-Judge apropriados.

    Args:
        dataset_name: Nome do dataset

    Returns:
        list: Lista de evaluators a serem usados
    """
    # Verificar se h√° configura√ß√£o espec√≠fica para este dataset
    if dataset_name not in DATASET_EVALUATORS:
        print(f"‚ö†Ô∏è  Dataset '{dataset_name}' n√£o encontrado na configura√ß√£o")
        print("   Usando evaluator padr√£o (LLM-as-Judge CORRECTNESS)...")

        # Fallback: LLM-as-Judge com crit√©rio CORRECTNESS
        default_judge = create_llm_as_judge(
            criteria="CORRECTNESS",
            model="openai:gpt-4o-mini",
            input_keys=["question"],
            reference_output_keys=["expected_answer"],
            prediction_key="answer"
        )
        return [default_judge]

    # Obter configura√ß√£o do dataset
    config = DATASET_EVALUATORS[dataset_name]

    print(f"\nüìä Evaluators para Dataset '{dataset_name}':")

    evaluators = []

    if config["type"] == "llm_as_judge":
        # Criar LLM-as-Judge usando create_llm_as_judge do LangSmith
        criteria = config["criteria"]
        input_keys = config["input_keys"]
        reference_output_keys = config.get("reference_output_keys")
        prediction_key = config["prediction_key"]

        print(f"   ‚úÖ LLM-as-Judge ({criteria})")
        print(f"      - Model: openai:gpt-4o-mini")
        print(f"      - Input Keys: {input_keys}")
        print(f"      - Reference Keys: {reference_output_keys}")
        print(f"      - Prediction Key: {prediction_key}")

        judge = create_llm_as_judge(
            criteria=criteria,
            model="openai:gpt-4o-mini",
            input_keys=input_keys,
            reference_output_keys=reference_output_keys,
            prediction_key=prediction_key
        )
        evaluators.append(judge)

    # Se nenhum evaluator v√°lido foi adicionado, usar padr√£o
    if not evaluators:
        print("   ‚ö†Ô∏è  Fallback para evaluator padr√£o (CORRECTNESS)")
        default_judge = create_llm_as_judge(
            criteria="CORRECTNESS",
            model="openai:gpt-4o-mini",
            input_keys=["question"],
            reference_output_keys=["expected_answer"],
            prediction_key="answer"
        )
        evaluators = [default_judge]

    return evaluators


def run_quick_eval(
    target_function,
    dataset_name: str,
    experiment_prefix: str = "quick-eval"
) -> Dict[str, float]:
    """
    Executa quick evaluation sobre golden dataset com sele√ß√£o autom√°tica de evaluators.

    Args:
        target_function: Fun√ß√£o que implementa seu LLM app
        dataset_name: Nome do dataset no LangSmith
        experiment_prefix: Prefixo para experimento

    Returns:
        dict: M√©tricas extra√≠das e score total ponderado
    """
    client = Client()

    # Callback para latency tracking
    latency_cb = LatencyCallback()

    # Wrapper para adicionar callback
    def predict_with_tracking(inputs):
        from langchain_core.runnables import RunnableConfig
        config = RunnableConfig(callbacks=[latency_cb])
        result = target_function(inputs, config=config)
        return result

    # Selecionar evaluators apropriados baseado no dataset
    evaluators = select_evaluators_for_dataset(dataset_name)

    # Executar evaluation
    results = evaluate(
        predict_with_tracking,
        data=dataset_name,
        evaluators=evaluators,
        experiment_prefix=experiment_prefix,
        max_concurrency=5
    )

    # ==================== EXTRAIR M√âTRICAS ====================

    # 1. Accuracy (from LangSmith evaluator)
    accuracy = results.get("qa", {}).get("mean", 0.0)

    # 2. Relevance (from LangSmith evaluator)
    relevance = results.get("context_qa", {}).get("mean", 0.0)

    # 3. Latency P95 (from custom callback)
    p95_latency = latency_cb.get_p95()
    latency_score = normalize_score(
        p95_latency,
        min_val=0,
        max_val=2000,  # 2s = score 0, 0ms = score 1
        invert=True
    )

    # 4. Cost (from LangSmith tracking)
    # Assumindo que LangSmith trackeia custo automaticamente
    total_cost = results.get("total_cost", 0.0)
    avg_cost_per_example = total_cost / max(results.get("example_count", 1), 1)
    cost_score = normalize_score(
        avg_cost_per_example,
        min_val=0.0,
        max_val=0.01,  # $0.01/example = score 0
        invert=True
    )

    # 5. Error Rate
    error_count = results.get("error_count", 0)
    total_examples = results.get("example_count", 1)
    error_rate = error_count / max(total_examples, 1)
    error_score = 1.0 - error_rate  # Menos erros = melhor score

    # ==================== CALCULAR SCORE TOTAL ====================

    weighted_score = (
        accuracy * METRIC_WEIGHTS["accuracy"] +
        relevance * METRIC_WEIGHTS["relevance"] +
        latency_score * METRIC_WEIGHTS["latency"] +
        cost_score * METRIC_WEIGHTS["cost"] +
        error_score * METRIC_WEIGHTS["errors"]
    )

    # ==================== RETORNO ====================

    metrics = {
        "accuracy": round(accuracy, 3),
        "relevance": round(relevance, 3),
        "p95_latency_ms": round(p95_latency, 0),
        "latency_score": round(latency_score, 3),
        "avg_cost_per_example": round(avg_cost_per_example, 5),
        "cost_score": round(cost_score, 3),
        "error_rate": round(error_rate, 3),
        "error_score": round(error_score, 3),
        "weighted_total_score": round(weighted_score, 3),
        "experiment_url": results.get("experiment_url", "")
    }

    return metrics


# ==================== EXEMPLO DE USO ====================

def example_llm_app(inputs, config=None):
    """
    Exemplo de aplica√ß√£o LLM.
    SUBSTITUA pela sua implementa√ß√£o real.
    """
    from langchain_openai import ChatOpenAI
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import StrOutputParser

    llm = ChatOpenAI(model="gpt-4o-mini")
    prompt = ChatPromptTemplate.from_messages([
        ("system", "You are a helpful assistant."),
        ("human", "{input}")
    ])

    chain = prompt | llm | StrOutputParser()

    result = chain.invoke(inputs, config=config or {})
    return {"output": result}


def main():
    """Run quick evaluation and print weighted results"""

    # CONFIGURA√á√ÉO: Ajuste conforme seu setup
    DATASET_NAME = "golden-dataset"  # Nome do dataset no LangSmith
    TARGET_FUNCTION = example_llm_app  # Sua fun√ß√£o LLM

    print("=" * 60)
    print("üöÄ QUICK EVALUATION - LANGSMITH")
    print("=" * 60)

    # Executar evaluation
    metrics = run_quick_eval(
        target_function=TARGET_FUNCTION,
        dataset_name=DATASET_NAME,
        experiment_prefix="quick-eval"
    )

    # ==================== EXIBIR RESULTADOS ====================

    print("\nüìä M√âTRICAS EXTRA√çDAS:")
    print("-" * 60)

    # Tabela de m√©tricas
    print(f"{'M√©trica':<25} {'Score':<12} {'Peso':<10} {'Contribui√ß√£o'}")
    print("-" * 60)

    metrics_display = [
        ("Accuracy", metrics["accuracy"], METRIC_WEIGHTS["accuracy"]),
        ("Relevance", metrics["relevance"], METRIC_WEIGHTS["relevance"]),
        ("Latency Score", metrics["latency_score"], METRIC_WEIGHTS["latency"]),
        ("Cost Score", metrics["cost_score"], METRIC_WEIGHTS["cost"]),
        ("Error Score", metrics["error_score"], METRIC_WEIGHTS["errors"])
    ]

    for name, score, weight in metrics_display:
        contribution = score * weight
        print(f"{name:<25} {score:<12.3f} {weight*100:<9.0f}% {contribution:.3f}")

    print("-" * 60)
    print(f"{'SCORE TOTAL':<25} {metrics['weighted_total_score']:<12.3f} {'100%':<10} {metrics['weighted_total_score']:.3f}")
    print("=" * 60)

    # Detalhes adicionais
    print(f"\nüìà Detalhes:")
    print(f"  - P95 Latency: {metrics['p95_latency_ms']:.0f}ms")
    print(f"  - Avg Cost/Example: ${metrics['avg_cost_per_example']:.5f}")
    print(f"  - Error Rate: {metrics['error_rate']*100:.1f}%")
    print(f"  - LangSmith URL: {metrics['experiment_url']}")

    # Salvar resultados
    output_file = "evaluators/scripts/eval_results.json"
    with open(output_file, 'w') as f:
        json.dump(metrics, f, indent=2)
    print(f"\nüíæ Resultados salvos em: {output_file}")

    return metrics


if __name__ == "__main__":
    main()
```

4.3 **Salvar Script**

Usar `Write` para criar `evaluators/scripts/quick_evals.py` com a configura√ß√£o `DATASET_EVALUATORS` preenchida

### Passo 5: Criar Script Orquestrador Principal

5.1 **Gerar `run_all_evals.py`**

Script que executa todo o pipeline em ordem:

```python
"""
Orquestrador Principal - Evaluation Pipeline
Executa upload de datasets + quick evals em sequ√™ncia.
"""

import subprocess
import sys
from pathlib import Path

def run_command(script_name: str, description: str):
    """Executa script Python e reporta resultado"""
    print(f"\n{'='*60}")
    print(f"üîÑ {description}")
    print(f"{'='*60}\n")

    result = subprocess.run(
        [sys.executable, script_name],
        capture_output=False,
        text=True
    )

    if result.returncode != 0:
        print(f"\n‚ùå Erro ao executar {script_name}")
        sys.exit(1)

    print(f"\n‚úÖ {description} - Conclu√≠do")


def main():
    """Executa pipeline completo de evaluation"""

    scripts_dir = Path("evaluators/scripts")

    # Verificar se diret√≥rio existe
    if not scripts_dir.exists():
        print(f"‚ùå Diret√≥rio '{scripts_dir}' n√£o encontrado")
        sys.exit(1)

    # Passo 1: Upload de datasets
    upload_script = scripts_dir / "upload_datasets.py"
    if upload_script.exists():
        run_command(str(upload_script), "Passo 1: Upload de Datasets para LangSmith")
    else:
        print(f"‚ö†Ô∏è  Script {upload_script} n√£o encontrado. Skipping upload.")

    # Passo 2: Quick evaluations
    evals_script = scripts_dir / "quick_evals.py"
    if evals_script.exists():
        run_command(str(evals_script), "Passo 2: Quick Evaluations")
    else:
        print(f"‚ùå Script {evals_script} n√£o encontrado")
        sys.exit(1)

    print(f"\n{'='*60}")
    print("üéâ PIPELINE DE EVALUATION CONCLU√çDO")
    print(f"{'='*60}")


if __name__ == "__main__":
    main()
```

5.2 **Salvar Script Orquestrador**

Usar `Write` para criar `evaluators/scripts/run_all_evals.py`

### Passo 6: Criar Documenta√ß√£o

6.1 **Gerar README.md**

````markdown
# Evaluation Scripts

Scripts automatizados para upload de datasets e execu√ß√£o de quick evaluations no LangSmith.

## üìÅ Estrutura

- `upload_datasets.py`: Upload de datasets locais para LangSmith (skip se existir)
- `quick_evals.py`: Executa evaluations com evaluators customizados por dataset e m√©tricas ponderadas
- `run_all_evals.py`: Orquestrador principal (executa tudo em ordem)
- `eval_results.json`: Resultados da √∫ltima execu√ß√£o

## üéØ Configura√ß√£o Autom√°tica de Evaluators

O comando `/create-script-evals` analisa todos os datasets **durante sua execu√ß√£o** e gera o script `quick_evals.py` com a configura√ß√£o `DATASET_EVALUATORS` pr√©-populada:

### An√°lise de Dataset (Feita pelo Comando)

Durante a execu√ß√£o do `/create-script-evals`, o comando:

1. **L√™ cada dataset** em `datasets/`
2. **Analisa a estrutura** detectando:
   - Tipo de tarefa (Q&A, summarization, classification, generation)
   - Tipo de output (texto livre ou estruturado)
   - Presen√ßa de refer√™ncia (ground truth)
   - Campos dispon√≠veis (inputs/outputs)

3. **Decide evaluators apropriados** para cada dataset
4. **Gera `quick_evals.py`** com configura√ß√£o fixa:

```python
DATASET_EVALUATORS = {
    "qa-dataset": ["qa", "context_qa"],
    "summary-dataset": ["llm_as_judge"],
    "generation-dataset": ["llm_as_judge", "embedding_distance"]
}
```

### Sele√ß√£o de Crit√©rios LLM-as-Judge (Feita pelo Comando)

Baseado na an√°lise, o **comando** seleciona automaticamente o **crit√©rio LLM-as-Judge** apropriado:

| Tipo de Dataset | Crit√©rio Selecionado | Raz√£o |
|-----------------|---------------------|-------|
| **Q&A com refer√™ncia** | CORRECTNESS | Verifica precis√£o factual comparando com ground truth |
| **Q&A sem refer√™ncia** | RELEVANCE | Avalia alinhamento da resposta com a pergunta |
| **Summarization** | CONCISENESS | Mede brevidade e objetividade |
| **Gera√ß√£o de texto longo** | COHERENCE | Avalia fluxo l√≥gico e consist√™ncia |
| **Assistentes/Chatbots** | HELPFULNESS | Avalia utilidade geral da resposta |
| **Safety/Guardrails** | HARMFULNESS | Detecta potencial de dano (f√≠sico ou emocional) |
| **Modera√ß√£o** | MALICIOUSNESS | Detecta inten√ß√£o de causar dano ou enganar |
| **Conte√∫do sens√≠vel** | CONTROVERSIALITY | Avalia potencial para gerar desacordo |

### LLM-as-Judge com `create_llm_as_judge`

O script usa **`create_llm_as_judge`** do LangSmith SDK com os **crit√©rios integrados**:

**Crit√©rios dispon√≠veis**:
1. **CORRECTNESS**: Precis√£o factual (requer ground truth)
2. **RELEVANCE**: Alinhamento com pergunta/contexto
3. **CONCISENESS**: Brevidade e objetividade
4. **COHERENCE**: Fluxo l√≥gico e consist√™ncia
5. **HELPFULNESS**: Utilidade geral para o usu√°rio
6. **HARMFULNESS**: Detec√ß√£o de conte√∫do prejudicial
7. **MALICIOUSNESS**: Detec√ß√£o de inten√ß√£o maliciosa
8. **CONTROVERSIALITY**: Potencial para gerar controv√©rsia

**Estrutura gerada**:
```python
from langsmith.evaluation import create_llm_as_judge

judge = create_llm_as_judge(
    criteria="CORRECTNESS",  # Ou outro crit√©rio apropriado
    model="openai:gpt-4o-mini",
    input_keys=["question"],
    reference_output_keys=["expected_answer"],  # Opcional
    prediction_key="answer"
)
```

**Vantagens**:
- ‚úÖ Usa helpers oficiais do LangSmith (mantido pela equipe LangChain)
- ‚úÖ Crit√©rios pr√©-calibrados e validados
- ‚úÖ Funciona com ou sem ground truth
- ‚úÖ Avalia aspectos subjetivos com consist√™ncia
- ‚úÖ Fornece justificativa detalhada
- ‚úÖ Integra√ß√£o nativa com `langsmith.evaluate()`

**Trade-offs**:
- ‚ö†Ô∏è Custo adicional de API (GPT-4o-mini)
- ‚ö†Ô∏è Lat√™ncia maior que evaluators rule-based
- ‚ö†Ô∏è N√£o determin√≠stico (pode variar ligeiramente)

## üöÄ Como Usar

### Op√ß√£o 1: Pipeline Completo

```bash
uv run evaluators/scripts/run_all_evals.py
````

### Op√ß√£o 2: Executar Individualmente

```bash
# 1. Upload datasets
uv run evaluators/scripts/upload_datasets.py

# 2. Run evaluations
uv run evaluators/scripts/quick_evals.py
```

## üì¶ Depend√™ncias

Os scripts requerem as seguintes bibliotecas Python:

```bash
pip install langsmith langchain langchain-openai openai python-dotenv numpy
```

Ou usando `uv`:

```bash
uv pip install langsmith langchain langchain-openai openai python-dotenv numpy
```

**Nota**: A biblioteca `openai` √© necess√°ria para o LLM-as-Judge evaluator que usa GPT-4o-mini para avaliar respostas quando n√£o h√° ground truth ou crit√©rios s√£o subjetivos.

## ‚öôÔ∏è Configura√ß√£o

### 1. Vari√°veis de Ambiente

Configure as seguintes vari√°veis no `.env`:

```bash
LANGSMITH_API_KEY=your-api-key
LANGCHAIN_PROJECT=your-project-name
LANGCHAIN_TRACING_V2=true
OPENAI_API_KEY=your-openai-key  # ou outro provider
```

**Importante**: Os scripts utilizam `python-dotenv` para carregar automaticamente as vari√°veis do arquivo `.env`. Certifique-se de que:
- O arquivo `.env` est√° na raiz do projeto
- A vari√°vel `LANGSMITH_API_KEY` est√° configurada corretamente
- A biblioteca `python-dotenv` est√° instalada (`pip install python-dotenv`)

**Configura√ß√£o de Python Path**: Os scripts automaticamente adicionam o diret√≥rio raiz do projeto ao `sys.path`:
- Estrutura esperada: `project_root/evaluators/scripts/[script].py`
- O script resolve o caminho com `.parents[2]` para alcan√ßar o diret√≥rio raiz
- Isso permite importar m√≥dulos do projeto sem conflitos de path
- Se sua estrutura for diferente, ajuste o n√∫mero em `.parents[N]` conforme necess√°rio

### 2. Ajustar Pesos das M√©tricas

Edite `quick_evals.py` para customizar os pesos:

```python
METRIC_WEIGHTS = {
    "accuracy": 0.45,      # 45% - Ajuste conforme necess√°rio
    "relevance": 0.20,     # 20%
    "latency": 0.15,       # 15%
    "cost": 0.15,          # 15%
    "errors": 0.05         # 5%
}
```

**Importante**: A soma dos pesos deve ser 1.0 (100%)

### 3. Configurar Sua Aplica√ß√£o LLM

Em `quick_evals.py`, substitua `example_llm_app` pela sua implementa√ß√£o:

```python
def my_llm_app(inputs, config=None):
    # Sua l√≥gica aqui
    return {"output": result}

# Depois, em main():
TARGET_FUNCTION = my_llm_app
```

## üìä M√©tricas Calculadas

| M√©trica | Descri√ß√£o | Peso Padr√£o | Fonte |
|---------|-----------|-------------|-------|
| **Accuracy** | Precis√£o das respostas | 45% | LangSmith `qa` evaluator |
| **Relevance** | Relev√¢ncia do conte√∫do | 20% | LangSmith `context_qa` evaluator |
| **Latency** | P95 tempo de resposta | 15% | Custom callback |
| **Cost** | Custo m√©dio por exemplo | 15% | LangSmith tracking |
| **Errors** | Taxa de erro (1 - error_rate) | 5% | LangSmith error tracking |

### Score Total

Score final = Œ£ (m√©trica √ó peso)

Exemplo:

- Accuracy: 0.85 √ó 0.45 = 0.3825
- Relevance: 0.78 √ó 0.20 = 0.156
- Latency: 0.92 √ó 0.15 = 0.138
- Cost: 0.88 √ó 0.15 = 0.132
- Errors: 0.95 √ó 0.05 = 0.0475
- **Total: 0.856 (85.6%)**

## üìÅ Formato dos Datasets

Os datasets em `datasets/` devem seguir o formato:

**JSON:**

```json
[
  {
    "input": {"question": "What is AI?"},
    "output": {"answer": "AI is artificial intelligence."}
  }
]
```

**JSONL:**

```jsonl
{"input": {"question": "What is AI?"}, "output": {"answer": "AI is artificial intelligence."}}
{"input": {"question": "What is ML?"}, "output": {"answer": "ML is machine learning."}}
```

## üîç Troubleshooting

### Erro: Dataset j√° existe

Comportamento esperado! O script faz skip autom√°tico.

### Erro: LangSmith API Key (401 "Invalid token")

**Causa**: Vari√°vel `LANGSMITH_API_KEY` n√£o est√° sendo carregada do arquivo `.env`.

**Solu√ß√µes**:
1. Verifique se o arquivo `.env` existe na raiz do projeto
2. Certifique-se de que a vari√°vel est√° definida corretamente: `LANGSMITH_API_KEY=your-api-key`
3. Instale `python-dotenv`: `pip install python-dotenv`
4. Verifique se `load_dotenv()` est√° sendo chamado no in√≠cio dos scripts
5. Teste manualmente: `python -c "from dotenv import load_dotenv; load_dotenv(); import os; print(os.getenv('LANGSMITH_API_KEY'))"`

### Erro: Nenhum dataset encontrado

Verifique se h√° arquivos `.json` ou `.jsonl` em `datasets/`.

### Erro: ModuleNotFoundError ao importar m√≥dulos do projeto

**Causa**: Script n√£o consegue importar m√≥dulos do projeto.

**Solu√ß√µes**:
1. Verifique se a estrutura de diret√≥rios est√° correta: `project_root/evaluators/scripts/`
2. Ajuste `.parents[N]` se a estrutura for diferente:
   - `.parents[1]`: Para `project_root/scripts/[script].py`
   - `.parents[2]`: Para `project_root/evaluators/scripts/[script].py` (padr√£o)
   - `.parents[3]`: Para `project_root/foo/evaluators/scripts/[script].py`
3. Teste o path: Adicione `print(f"Project root: {project_root}")` ap√≥s `project_root = ...`
4. Verifique se os m√≥dulos que voc√™ quer importar existem no `project_root`

## üìñ Refer√™ncias

- [LangSmith Evaluation Docs](https://docs.smith.langchain.com/evaluation)
- [LangChain Evaluators](https://python.langchain.com/docs/guides/productionization/evaluation/)

````

6.2 **Salvar README**

Usar `Write` para criar `evaluators/scripts/README.md`

### Passo 7: Valida√ß√£o e Teste

7.1 **Verificar Estrutura Criada**

Usar `Glob` para confirmar:
- `evaluators/scripts/upload_datasets.py` existe
- `evaluators/scripts/quick_evals.py` existe
- `evaluators/scripts/run_all_evals.py` existe
- `evaluators/scripts/README.md` existe

7.2 **Executar Valida√ß√£o Sint√°tica (Opcional)**

```bash
uv run python -m py_compile evaluators/scripts/*.py
````

## üìä Formato de Sa√≠da

### Durante Execu√ß√£o

```text
============================================================
üöÄ QUICK EVALUATION - LANGSMITH
============================================================

üîç Running evaluation on dataset 'golden-dataset'...
‚úÖ Evaluation complete

üìä M√âTRICAS EXTRA√çDAS:
------------------------------------------------------------
M√©trica                   Score        Peso       Contribui√ß√£o
------------------------------------------------------------
Accuracy                  0.850        45%        0.383
Relevance                 0.780        20%        0.156
Latency Score             0.920        15%        0.138
Cost Score                0.880        15%        0.132
Error Score               0.950        5%         0.048
------------------------------------------------------------
SCORE TOTAL               0.856        100%       0.856
============================================================

üìà Detalhes:
  - P95 Latency: 450ms
  - Avg Cost/Example: $0.00125
  - Error Rate: 5.0%
  - LangSmith URL: https://smith.langchain.com/...

üíæ Resultados salvos em: evaluators/scripts/eval_results.json
```

### Arquivo JSON Gerado

```json
{
  "accuracy": 0.850,
  "relevance": 0.780,
  "p95_latency_ms": 450,
  "latency_score": 0.920,
  "avg_cost_per_example": 0.00125,
  "cost_score": 0.880,
  "error_rate": 0.05,
  "error_score": 0.950,
  "weighted_total_score": 0.856,
  "experiment_url": "https://smith.langchain.com/..."
}
```

## ‚úÖ Crit√©rios de Sucesso

- [ ] Skills de evaluation consultadas (evaluation-developer, evals-automator, datasets-evals, quick-evals)
- [ ] Diret√≥rio `evaluators/scripts/` criado (se n√£o existia)
- [ ] **Comando analisou** cada dataset em `datasets/` usando `Read`
- [ ] **Comando detectou** para cada dataset: tipo de tarefa, tipo de output, presen√ßa de refer√™ncia, campos de input/output
- [ ] **Comando decidiu** crit√©rios LLM-as-Judge apropriados para cada dataset (CORRECTNESS, RELEVANCE, CONCISENESS, etc.)
- [ ] **Comando criou** dict `dataset_evaluators` mapeando datasets ‚Üí configura√ß√£o LLM-as-Judge (tipo, crit√©rio, chaves)
- [ ] Script `upload_datasets.py` criado com skip logic
- [ ] `load_dotenv()` adicionado no in√≠cio de `upload_datasets.py`
- [ ] Configura√ß√£o de `sys.path` adicionada em `upload_datasets.py`
- [ ] Script `quick_evals.py` criado com 5 m√©tricas ponderadas
- [ ] `load_dotenv()` adicionado no in√≠cio de `quick_evals.py`
- [ ] Configura√ß√£o de `sys.path` adicionada em `quick_evals.py`
- [ ] **Constante `DATASET_EVALUATORS`** preenchida com configura√ß√£o completa de LLM-as-Judge (crit√©rios + chaves)
- [ ] LLM-as-Judge implementado usando `create_llm_as_judge` do LangSmith
- [ ] Fun√ß√£o `select_evaluators_for_dataset()` usa `create_llm_as_judge` com crit√©rios do `DATASET_EVALUATORS`
- [ ] Crit√©rios LLM-as-Judge customizados por dataset baseado na an√°lise feita pelo comando
- [ ] Pesos das m√©tricas configur√°veis e somam 1.0
- [ ] M√©tricas normalizadas para 0-1 corretamente
- [ ] Score total calculado com pondera√ß√£o
- [ ] Script orquestrador `run_all_evals.py` criado
- [ ] README.md com documenta√ß√£o completa gerado
- [ ] Exemplo de target function inclu√≠do
- [ ] Error handling implementado
- [ ] Resultados salvos em JSON
- [ ] Formato de output claro e tabular
- [ ] Valida√ß√£o sint√°tica passa (opcional)

## üìù Exemplo de Uso

```bash
# Criar scripts de evaluation
/create-script-evals

# Executar pipeline completo
uv run evaluators/scripts/run_all_evals.py

# Ou executar individualmente
uv run evaluators/scripts/upload_datasets.py
uv run evaluators/scripts/quick_evals.py
```

## ‚ùå Anti-Patterns

### ‚ùå Erro 1: Pesos n√£o somam 100%

N√£o configure pesos que n√£o somam 1.0:

```python
# ‚ùå Errado - Soma = 0.95
METRIC_WEIGHTS = {
    "accuracy": 0.45,
    "relevance": 0.20,
    "latency": 0.15,
    "cost": 0.10,
    "errors": 0.05
}

# ‚úÖ Correto - Soma = 1.0
METRIC_WEIGHTS = {
    "accuracy": 0.45,
    "relevance": 0.20,
    "latency": 0.15,
    "cost": 0.15,
    "errors": 0.05
}
```

### ‚ùå Erro 2: N√£o normalizar m√©tricas

N√£o use m√©tricas em escalas diferentes diretamente:

```python
# ‚ùå Errado - Latency em ms (0-2000), accuracy em 0-1
score = accuracy + latency_ms  # Escalas incompat√≠veis!

# ‚úÖ Correto - Normalizar para 0-1
latency_score = normalize_score(latency_ms, 0, 2000, invert=True)
score = accuracy * 0.5 + latency_score * 0.5
```

### ‚ùå Erro 3: Hardcoded dataset names

N√£o hardcode nomes de datasets:

```python
# ‚ùå Errado
results = evaluate(target, data="my-specific-dataset")

# ‚úÖ Correto - Configur√°vel
DATASET_NAME = os.getenv("EVAL_DATASET", "golden-dataset")
results = evaluate(target, data=DATASET_NAME)
```

### ‚ùå Erro 4: Ignorar errors no upload

N√£o ignore erros silenciosamente:

```python
# ‚ùå Errado
try:
    client.create_dataset(name)
except:
    pass  # Silent fail!

# ‚úÖ Correto
try:
    existing = client.read_dataset(dataset_name=name)
    print(f"‚úÖ Dataset '{name}' j√° existe. Skipping.")
    return name
except:
    # Create new
    dataset = client.create_dataset(dataset_name=name)
    print(f"üì§ Created new dataset '{name}'")
```

### ‚ùå Erro 5: N√£o carregar vari√°veis de ambiente do .env

N√£o esquecer de carregar o arquivo `.env` no in√≠cio dos scripts:

```python
# ‚ùå Errado - 401 "Invalid token" error
from langsmith import Client
# LANGSMITH_API_KEY n√£o foi carregada do .env
client = Client()

# ‚úÖ Correto - Carregar .env primeiro
from dotenv import load_dotenv
from langsmith import Client

load_dotenv()  # Carrega LANGSMITH_API_KEY e outras vari√°veis
client = Client()
```

**Consequ√™ncias de n√£o usar `load_dotenv()`**:
- Erro 401 "Invalid token" ao autenticar com LangSmith
- Vari√°veis do `.env` n√£o s√£o carregadas no ambiente
- Scripts falham mesmo com `.env` configurado corretamente

### ‚ùå Erro 6: N√£o configurar Python path para imports de projeto

N√£o esquecer de adicionar o diret√≥rio raiz do projeto ao `sys.path`:

```python
# ‚ùå Errado - ModuleNotFoundError ao importar m√≥dulos do projeto
from langsmith import Client
# Tentando importar m√≥dulos do projeto, mas sys.path n√£o configurado
from my_project.utils import helper_function  # Falha!

# ‚úÖ Correto - Configurar sys.path primeiro
import sys
from pathlib import Path

# Add project root to Python path for imports
project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

# Agora imports funcionam
from langsmith import Client
from my_project.utils import helper_function  # Sucesso!
```

**Por que `.parents[2]`?**
- Script est√° em: `project_root/evaluators/scripts/upload_datasets.py`
- `.parents[0]`: `evaluators/scripts/upload_datasets.py` (o pr√≥prio arquivo)
- `.parents[1]`: `evaluators/scripts/` (diret√≥rio pai)
- `.parents[2]`: `evaluators/` (diret√≥rio av√¥)
- `.parents[3]`: `project_root/` (raiz do projeto) ‚Üê Este √© o objetivo!

**Ajuste conforme sua estrutura**:
- `project_root/scripts/`: Use `.parents[1]`
- `project_root/evaluators/scripts/`: Use `.parents[2]` (padr√£o)
- `project_root/foo/bar/scripts/`: Use `.parents[3]`

### ‚ùå Erro 7: Gerar script com crit√©rios LLM-as-Judge fixos sem analisar datasets

N√£o gere `quick_evals.py` com configura√ß√£o vazia ou crit√©rios gen√©ricos:

```python
# ‚ùå Errado - Configura√ß√£o vazia
DATASET_EVALUATORS = {}

# ‚ùå Errado - Todos datasets com mesmo crit√©rio gen√©rico
DATASET_EVALUATORS = {
    "qa-dataset": {"type": "llm_as_judge", "criteria": "CORRECTNESS", ...},
    "summary-dataset": {"type": "llm_as_judge", "criteria": "CORRECTNESS", ...},
    "safety-test": {"type": "llm_as_judge", "criteria": "CORRECTNESS", ...}
}
# CORRECTNESS pode n√£o ser apropriado para summarization (CONCISENESS) ou safety (HARMFULNESS)

# ‚úÖ Correto - Comando analisa datasets ANTES e seleciona crit√©rios apropriados

# 1. Comando usa Read para ler cada dataset
# 2. Comando detecta:
#    - Tipo de tarefa (Q&A, summarization, generation, chatbot, safety)
#    - Presen√ßa de refer√™ncia (ground truth)
#    - Natureza da avalia√ß√£o (precis√£o, brevidade, utilidade, seguran√ßa)
#    - Campos de input/output
# 3. Comando decide crit√©rio LLM-as-Judge apropriado
# 4. Comando gera quick_evals.py com configura√ß√£o customizada:

DATASET_EVALUATORS = {
    "qa-dataset": {
        "type": "llm_as_judge",
        "criteria": "CORRECTNESS",  # Q&A com refer√™ncia ‚Üí precis√£o factual
        "input_keys": ["question"],
        "reference_output_keys": ["expected_answer"],
        "prediction_key": "answer"
    },
    "summary-dataset": {
        "type": "llm_as_judge",
        "criteria": "CONCISENESS",  # Summarization ‚Üí brevidade
        "input_keys": ["text"],
        "reference_output_keys": ["summary"],
        "prediction_key": "output"
    },
    "chatbot-dataset": {
        "type": "llm_as_judge",
        "criteria": "HELPFULNESS",  # Chatbot ‚Üí utilidade
        "input_keys": ["user_message"],
        "reference_output_keys": None,
        "prediction_key": "response"
    },
    "safety-test": {
        "type": "llm_as_judge",
        "criteria": "HARMFULNESS",  # Safety ‚Üí detectar dano
        "input_keys": ["prompt"],
        "reference_output_keys": None,
        "prediction_key": "completion"
    }
}
```

**Por que o comando deve analisar datasets ANTES**:
- Cada tipo de tarefa precisa de crit√©rio LLM-as-Judge espec√≠fico
- CORRECTNESS n√£o funciona para summarization (use CONCISENESS)
- HELPFULNESS n√£o funciona para safety tests (use HARMFULNESS)
- Crit√©rio errado gera scores sem sentido ou avalia√ß√µes incorretas
- An√°lise em tempo de execu√ß√£o do comando √© mais eficiente
- Script gerado j√° vem customizado por tipo de tarefa

**Consequ√™ncias de n√£o analisar no comando**:
- Avalia√ß√µes com crit√©rios inapropriados (ex: CORRECTNESS para safety)
- Scores sem sentido (ex: medir concis√£o em vez de seguran√ßa)
- Falhas silenciosas ou resultados enganosos
- Usu√°rio precisa editar manualmente a configura√ß√£o
- Perda de tempo com avalia√ß√µes irrelevantes
