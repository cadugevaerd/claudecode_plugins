---
description: Create automated evaluation script for LLM golden datasets with LangSmith integration
allowed-tools: Read, Write, Bash, Glob, Grep, Skill
model: claude-sonnet-4-5
argument-hint: '[EVALUATORS_DIR] [DATASETS_DIR]'
---

# Create Script Evals

Cria script automatizado que coleta informa√ß√µes de skills, envia datasets para LangSmith, executa quick-evals usando **openevals** (biblioteca pr√©-constru√≠da recomendada) ou custom evaluators, e extrai m√©tricas com pondera√ß√£o customizada.

## üéØ Objetivo

Este comando gera um script Python completo que:

- üîç Coleta informa√ß√µes de skills necess√°rias do projeto (incluindo `llm-as-a-judge`)
- üìä **Analisa datasets** e seleciona crit√©rios LLM-as-Judge apropriados (CORRECTNESS, RELEVANCE, CONCISENESS, COHERENCE, HELPFULNESS, HARMFULNESS, MALICIOUSNESS, CONTROVERSIALITY)
- üì§ Envia datasets da pasta `datasets/` para LangSmith (skip se j√° existir)
- ‚ö° Configura e executa quick-evals usando **openevals** (biblioteca pr√©-constru√≠da recomendada) ou custom evaluators
- üìä Extrai m√©tricas de execu√ß√£o com pondera√ß√£o configur√°vel
- üéØ Retorna score total baseado em pesos customizados

**IMPORTANTE**: O script gerado usa **openevals** (recomendado) ou custom evaluators com LangSmith SDK. **NUNCA** implementa LLM-as-Judge manualmente sem framework.

**Nota sobre o modelo:** Este comando usa **Sonnet 4.5** porque requer:

- Racioc√≠nio complexo para integrar m√∫ltiplas skills (evaluation-developer, benchmark-runner)
- An√°lise de estrutura de datasets e mapeamento para LangSmith API
- Gera√ß√£o de c√≥digo Python com error handling robusto e patterns avan√ßados
- Compreens√£o de m√©tricas de avalia√ß√£o e c√°lculo de scores ponderados
- Valida√ß√£o de configura√ß√µes e troubleshooting de integra√ß√£o com APIs externas

## üîß Instru√ß√µes

### Passo 1: Coletar Informa√ß√µes de Skills

1.1 **Executar Skills Relevantes**

Usar `Skill` tool para coletar conhecimento especializado:

```bash
Skill(skill="llm-eval-developer:llm-as-a-judge")
Skill(skill="llm-eval-developer:evals-automator")
Skill(skill="llm-eval-developer:datasets-evals")
Skill(skill="llm-eval-developer:quick-evals")
```

1.2 **Extrair Patterns de Evaluation**

Das skills, extrair:

- **LLM-as-Judge com `create_llm_as_judge`**: Usar **EXCLUSIVAMENTE** a fun√ß√£o helper oficial via `openevals` (biblioteca pr√©-constru√≠da recomendada)
- **NUNCA usar implementa√ß√£o manual de LLM-as-Judge**: Sempre usar `from openevals.llm import create_llm_as_judge` OU custom evaluators com LangSmith SDK
- **Crit√©rios integrados via openevals.prompts**: CORRECTNESS_PROMPT, RELEVANCE_PROMPT, CONCISENESS_PROMPT, COHERENCE_PROMPT, HELPFULNESS_PROMPT, HARMFULNESS_PROMPT, MALICIOUSNESS_PROMPT, CONTROVERSY_PROMPT
- **Mapeamento**: feedback_key, model, prompt (usando openevals prompts pr√©-constru√≠dos)
- Quando usar cada crit√©rio de avalia√ß√£o
- Integra√ß√£o com LangSmith API via `langsmith.evaluate()`
- Patterns de dataset upload
- M√©tricas dispon√≠veis (accuracy, relevance, latency, cost, errors)
- Como analisar estrutura de datasets para selecionar crit√©rios apropriados

### Passo 2: Analisar Estrutura Atual

2.1 **Verificar Diret√≥rios**

- Usar `Glob` para verificar se `evaluators/scripts/` existe
- Criar diret√≥rio se n√£o existir usando `Bash(mkdir -p evaluators/scripts/)`
- Verificar se `datasets/` existe e cont√©m arquivos

2.2 **Escanear e Analisar Datasets Existentes**

- Usar `Glob` para listar arquivos em `datasets/` (JSON, JSONL, CSV)
- **Para cada dataset**, usar `Read` para analisar estrutura completa:
  - Ler primeiro exemplo do dataset
  - Detectar campos de input e output
  - Identificar tipo de tarefa (Q&A, summarization, classification, generation)
  - Verificar se h√° reference outputs (ground truth)
  - Analisar formato dos outputs esperados (texto livre, JSON estruturado, categorias)
  - Determinar natureza da avalia√ß√£o necess√°ria (objetiva vs subjetiva)
- Determinar schema necess√°rio para LangSmith
- **Para cada dataset, decidir tipo de evaluator apropriado**:
  - **Similarity-based** (BLEU, ROUGE, embedding): Se h√° reference outputs exatos
  - **Rule-based** (regex, exact match): Se outputs t√™m formato fixo/valid√°vel
  - **LLM-as-Judge**: Se crit√©rios s√£o subjetivos, complexos ou sem ground truth
  - **Composite**: Se precisa avaliar m√∫ltiplos aspectos

2.3 **Documentar Decis√µes de Evaluators e Crit√©rios LLM-as-Judge**

- Criar dict mapeando cada dataset para seus evaluators e crit√©rios LLM-as-Judge recomendados
- **Para LLM-as-Judge**: Selecionar crit√©rio apropriado baseado na natureza do dataset
- Exemplo:
  ```python
  dataset_evaluators = {
      "qa-dataset": {
          "type": "llm_as_judge",
          "criteria": "CORRECTNESS",  # Precis√£o factual para Q&A
          "input_keys": ["question"],
          "reference_output_keys": ["expected_answer"],
          "prediction_key": "answer"
      },
      "summary-dataset": {
          "type": "llm_as_judge",
          "criteria": "CONCISENESS",  # Brevidade para summarization
          "input_keys": ["text"],
          "reference_output_keys": ["summary"],
          "prediction_key": "output"
      },
      "chatbot-dataset": {
          "type": "llm_as_judge",
          "criteria": "HELPFULNESS",  # Utilidade para assistentes
          "input_keys": ["user_message"],
          "reference_output_keys": None,  # Sem ground truth
          "prediction_key": "response"
      },
      "safety-test": {
          "type": "llm_as_judge",
          "criteria": "HARMFULNESS",  # Teste de seguran√ßa
          "input_keys": ["prompt"],
          "reference_output_keys": None,
          "prediction_key": "completion"
      }
  }
  ```
- **Guia de Sele√ß√£o de Crit√©rios**:
  - `CORRECTNESS`: Q&A, RAG, extra√ß√£o de fatos (requer ground truth)
  - `RELEVANCE`: Verificar alinhamento com pergunta/contexto
  - `CONCISENESS`: Summarization, chatbots (respostas breves)
  - `COHERENCE`: Gera√ß√£o de texto longo, artigos
  - `HELPFULNESS`: Assistentes, chatbots (avalia√ß√£o geral)
  - `HARMFULNESS`: Safety, guardrails (detectar conte√∫do prejudicial)
  - `MALICIOUSNESS`: Detectar inten√ß√£o maliciosa ou enganosa
  - `CONTROVERSIALITY`: Modera√ß√£o de conte√∫do
- Essa informa√ß√£o ser√° usada para gerar o script `quick_evals.py` customizado com `create_llm_as_judge`

### Passo 3: Criar Script de Upload de Datasets

3.1 **Gerar `upload_datasets.py`**

Criar script que:

```python
"""
Upload Datasets para LangSmith
Skip se dataset j√° existir no LangSmith.
"""

import sys
import os
import json
from pathlib import Path
from dotenv import load_dotenv
from langsmith import Client

# Add project root to Python path for imports
project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

# Carregar vari√°veis de ambiente do arquivo .env
load_dotenv()

def upload_dataset_to_langsmith(dataset_path: Path, client: Client):
    """
    Faz upload de dataset local para LangSmith.

    Args:
        dataset_path: Path para arquivo do dataset (JSON/JSONL)
        client: LangSmith Client

    Returns:
        str: Dataset name no LangSmith
    """
    dataset_name = dataset_path.stem

    # Check if exists
    try:
        existing = client.read_dataset(dataset_name=dataset_name)
        print(f"‚úÖ Dataset '{dataset_name}' j√° existe. Skipping upload.")
        return dataset_name
    except:
        pass

    # Load local dataset
    with open(dataset_path, 'r') as f:
        if dataset_path.suffix == '.jsonl':
            examples = [json.loads(line) for line in f]
        else:
            examples = json.load(f)

    # Create dataset
    dataset = client.create_dataset(dataset_name=dataset_name)

    # Upload examples
    for example in examples:
        client.create_example(
            dataset_id=dataset.id,
            inputs=example.get("input", {}),
            outputs=example.get("output", {})
        )

    print(f"üì§ Uploaded {len(examples)} examples to '{dataset_name}'")
    return dataset_name


def main():
    """Upload all datasets from datasets/ directory"""
    client = Client()
    datasets_dir = Path("datasets")

    if not datasets_dir.exists():
        print("‚ùå Directory 'datasets/' not found")
        return

    dataset_files = list(datasets_dir.glob("*.json")) + list(datasets_dir.glob("*.jsonl"))

    if not dataset_files:
        print("‚ö†Ô∏è  No datasets found in datasets/")
        return

    uploaded = []
    for dataset_file in dataset_files:
        name = upload_dataset_to_langsmith(dataset_file, client)
        uploaded.append(name)

    print(f"\n‚úÖ Upload complete. {len(uploaded)} datasets available in LangSmith:")
    for name in uploaded:
        print(f"  - {name}")

if __name__ == "__main__":
    main()
```

3.2 **Salvar Script**

Usar `Write` para criar `evaluators/scripts/upload_datasets.py`

### Passo 4: Criar Script de Quick Evals

4.1 **Preencher Configura√ß√£o de Evaluators**

Usar o mapeamento criado no Passo 2.3 (`dataset_evaluators`) para popular a constante `DATASET_EVALUATORS` no script.

Exemplo de preenchimento:

```python
DATASET_EVALUATORS = {
    "qa-golden-set": ["qa", "context_qa"],
    "summary-eval": ["llm_as_judge"],
    "code-generation": ["llm_as_judge", "embedding_distance"],
}
```

4.2 **Gerar `quick_evals.py`**

Criar script que executa evaluations sobre golden datasets usando a configura√ß√£o de evaluators:

```python
"""
Quick Evaluations sobre Golden Datasets no LangSmith
Executa evaluations e extrai m√©tricas com pondera√ß√£o customizada.
"""

import sys
import os
from typing import Dict, List
from pathlib import Path
from dotenv import load_dotenv
from langsmith import Client
from langsmith.evaluation import evaluate, LangChainStringEvaluator
from langchain_core.callbacks import BaseCallbackHandler
import numpy as np
import time
import json

# Add project root to Python path for imports
project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

# Carregar vari√°veis de ambiente do arquivo .env
load_dotenv()

# ==================== CONFIGURA√á√ÉO DE PESOS ====================

METRIC_WEIGHTS = {
    "accuracy": 0.60,      # 60% - Score do LLM-as-Judge (qualidade)
    "latency": 0.20,       # 20% - Tempo de resposta (P95)
    "cost": 0.15,          # 15% - Custo por execu√ß√£o
    "errors": 0.05         # 5%  - Taxa de erro
}

# Validar que soma = 100%
assert abs(sum(METRIC_WEIGHTS.values()) - 1.0) < 0.001, "Weights must sum to 1.0"


# ==================== DATASET EVALUATOR CONFIGURATION ====================

# Configura√ß√£o de evaluators por dataset
# Esta configura√ß√£o √© gerada automaticamente pelo comando /create-script-evals
# baseado na an√°lise dos datasets em datasets/
DATASET_EVALUATORS = {
    # Exemplo:
    # "qa-dataset": ["qa", "context_qa"],
    # "summary-dataset": ["llm_as_judge"],
    # "generation-dataset": ["llm_as_judge", "embedding_distance"]
}

# ==================== EVALUATORS ====================

# IMPORTANTE: Usar openevals (biblioteca pr√©-constru√≠da recomendada)
# Alternativa: Custom evaluators com LangSmith SDK
from openevals.llm import create_llm_as_judge
from openevals.prompts import (
    CORRECTNESS_PROMPT,
    RELEVANCE_PROMPT,
    CONCISENESS_PROMPT,
    COHERENCE_PROMPT,
    HELPFULNESS_PROMPT,
    HARMFULNESS_PROMPT,
    MALICIOUSNESS_PROMPT,
    CONTROVERSY_PROMPT
)

# Benef√≠cios do openevals:
# - Prompts pr√©-constru√≠dos e testados pela comunidade
# - Simples de usar (poucas linhas de c√≥digo)
# - Boas pr√°ticas built-in
# - Integra√ß√£o nativa com langsmith.evaluate()
# - Manuten√ß√£o e updates pela comunidade

# Nota: Todas as m√©tricas (lat√™ncia, custo, errors) ser√£o extra√≠das dos metadados do LangSmith
# ap√≥s a execu√ß√£o de evaluate(). N√£o √© necess√°rio tracking manual.


def normalize_score(value: float, min_val: float, max_val: float, invert: bool = False) -> float:
    """
    Normaliza score para 0-1.

    Args:
        value: Valor atual
        min_val: Valor m√≠nimo esperado
        max_val: Valor m√°ximo esperado
        invert: Se True, valores menores = score maior (para latency/cost)

    Returns:
        float: Score normalizado 0-1
    """
    if max_val == min_val:
        return 1.0

    normalized = (value - min_val) / (max_val - min_val)
    normalized = max(0.0, min(1.0, normalized))  # Clamp 0-1

    if invert:
        normalized = 1.0 - normalized

    return normalized


# ==================== MAIN EVALUATION ====================

def select_evaluators_for_dataset(dataset_name: str) -> List:
    """
    Seleciona evaluators apropriados baseado na configura√ß√£o DATASET_EVALUATORS.

    Esta configura√ß√£o √© gerada automaticamente pelo comando /create-script-evals
    ap√≥s analisar a estrutura de cada dataset e escolher crit√©rios LLM-as-Judge apropriados.

    IMPORTANTE: Usa openevals (biblioteca pr√©-constru√≠da recomendada).
    Alternativa: Custom evaluators com LangSmith SDK.

    Args:
        dataset_name: Nome do dataset

    Returns:
        list: Lista de evaluators criados com openevals
    """
    # Verificar se h√° configura√ß√£o espec√≠fica para este dataset
    if dataset_name not in DATASET_EVALUATORS:
        print(f"‚ö†Ô∏è  Dataset '{dataset_name}' n√£o encontrado na configura√ß√£o")
        print("   Usando evaluator padr√£o (LLM-as-Judge CORRECTNESS)...")

        # Fallback: LLM-as-Judge com crit√©rio CORRECTNESS via openevals
        default_judge = create_llm_as_judge(
            prompt=CORRECTNESS_PROMPT,
            feedback_key="correctness",
            model="openai:gpt-4o-mini"
        )
        return [default_judge]

    # Obter configura√ß√£o do dataset
    config = DATASET_EVALUATORS[dataset_name]

    print(f"\nüìä Evaluators para Dataset '{dataset_name}':")

    evaluators = []

    if config["type"] == "llm_as_judge":
        # Mapear crit√©rio para prompt openevals
        criteria = config["criteria"]
        prompt_mapping = {
            "CORRECTNESS": CORRECTNESS_PROMPT,
            "RELEVANCE": RELEVANCE_PROMPT,
            "CONCISENESS": CONCISENESS_PROMPT,
            "COHERENCE": COHERENCE_PROMPT,
            "HELPFULNESS": HELPFULNESS_PROMPT,
            "HARMFULNESS": HARMFULNESS_PROMPT,
            "MALICIOUSNESS": MALICIOUSNESS_PROMPT,
            "CONTROVERSIALITY": CONTROVERSY_PROMPT
        }

        prompt = prompt_mapping.get(criteria, CORRECTNESS_PROMPT)

        print(f"   ‚úÖ LLM-as-Judge ({criteria})")
        print(f"      - Model: openai:gpt-4o-mini")
        print(f"      - Prompt: openevals.prompts.{criteria}_PROMPT")

        # Criar judge usando openevals
        judge = create_llm_as_judge(
            prompt=prompt,
            feedback_key=criteria.lower(),
            model="openai:gpt-4o-mini"
        )
        evaluators.append(judge)

    # Se nenhum evaluator v√°lido foi adicionado, usar padr√£o
    if not evaluators:
        print("   ‚ö†Ô∏è  Fallback para evaluator padr√£o (CORRECTNESS)")
        default_judge = create_llm_as_judge(
            prompt=CORRECTNESS_PROMPT,
            feedback_key="correctness",
            model="openai:gpt-4o-mini"
        )
        evaluators = [default_judge]

    return evaluators


def run_quick_eval(
    target_function,
    dataset_name: str,
    experiment_prefix: str = "quick-eval"
) -> Dict[str, float]:
    """
    Executa quick evaluation sobre golden dataset usando openevals.

    IMPORTANTE: Usa openevals (biblioteca pr√©-constru√≠da recomendada).
    Alternativa: Custom evaluators com LangSmith SDK.

    Todas as m√©tricas s√£o extra√≠das dos metadados do LangSmith ap√≥s evaluate().

    Args:
        target_function: Fun√ß√£o que implementa seu LLM app
        dataset_name: Nome do dataset no LangSmith
        experiment_prefix: Prefixo para experimento

    Returns:
        dict: M√©tricas extra√≠das dos metadados do LangSmith e score total ponderado
    """
    client = Client()

    # Selecionar evaluators apropriados (openevals ou custom)
    evaluators = select_evaluators_for_dataset(dataset_name)

    # Executar evaluation
    results = evaluate(
        target_function,
        data=dataset_name,
        evaluators=evaluators,
        experiment_prefix=experiment_prefix,
        max_concurrency=5
    )

    # ==================== EXTRAIR M√âTRICAS DOS METADADOS DO LANGSMITH ====================

    # Buscar experiment metadata do LangSmith
    experiment_name = results.get("experiment_name")

    # Obter runs do experiment para calcular m√©tricas agregadas
    runs = list(client.list_runs(
        project_name=experiment_name,
        execution_order=1,
        is_root=True
    ))

    # 1. LLM-as-Judge Score (extra√≠do do evaluator)
    # O nome do evaluator depende do crit√©rio usado (ex: "correctness", "relevance", etc.)
    # Vamos buscar pela primeira key de evaluator dispon√≠vel
    llm_judge_score = 0.0
    for key in results.keys():
        if key not in ["experiment_name", "experiment_url", "results"]:
            # Pegar score m√©dio do evaluator
            llm_judge_score = results.get(key, {}).get("mean", 0.0)
            break

    # 2. Latency (extra√≠da dos metadados dos runs)
    latencies = []
    for run in runs:
        if run.latency:
            latencies.append(run.latency / 1000)  # Converter para ms

    p95_latency = np.percentile(latencies, 95) if latencies else 0
    avg_latency = np.mean(latencies) if latencies else 0

    latency_score = normalize_score(
        p95_latency,
        min_val=0,
        max_val=2000,  # 2s = score 0, 0ms = score 1
        invert=True
    )

    # 3. Cost (extra√≠da dos metadados dos runs)
    total_cost = 0.0
    for run in runs:
        if run.total_cost:
            total_cost += run.total_cost

    total_examples = len(runs)
    avg_cost_per_example = total_cost / max(total_examples, 1)

    cost_score = normalize_score(
        avg_cost_per_example,
        min_val=0.0,
        max_val=0.01,  # $0.01/example = score 0
        invert=True
    )

    # 4. Error Rate (extra√≠da dos metadados dos runs)
    error_count = sum(1 for run in runs if run.error is not None)
    error_rate = error_count / max(total_examples, 1)
    error_score = 1.0 - error_rate  # Menos erros = melhor score

    # ==================== CALCULAR SCORE TOTAL ====================

    weighted_score = (
        llm_judge_score * METRIC_WEIGHTS["accuracy"] +
        latency_score * METRIC_WEIGHTS["latency"] +
        cost_score * METRIC_WEIGHTS["cost"] +
        error_score * METRIC_WEIGHTS["errors"]
    )

    # ==================== RETORNO ====================

    metrics = {
        "llm_judge_score": round(llm_judge_score, 3),
        "p95_latency_ms": round(p95_latency, 0),
        "avg_latency_ms": round(avg_latency, 0),
        "latency_score": round(latency_score, 3),
        "total_cost": round(total_cost, 5),
        "avg_cost_per_example": round(avg_cost_per_example, 5),
        "cost_score": round(cost_score, 3),
        "error_count": error_count,
        "total_examples": total_examples,
        "error_rate": round(error_rate, 3),
        "error_score": round(error_score, 3),
        "weighted_total_score": round(weighted_score, 3),
        "experiment_name": experiment_name,
        "experiment_url": results.get("experiment_url", "")
    }

    return metrics


# ==================== EXEMPLO DE USO ====================

def example_llm_app(inputs, config=None):
    """
    Exemplo de aplica√ß√£o LLM.
    SUBSTITUA pela sua implementa√ß√£o real.
    """
    from langchain_openai import ChatOpenAI
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import StrOutputParser

    llm = ChatOpenAI(model="gpt-4o-mini")
    prompt = ChatPromptTemplate.from_messages([
        ("system", "You are a helpful assistant."),
        ("human", "{input}")
    ])

    chain = prompt | llm | StrOutputParser()

    result = chain.invoke(inputs, config=config or {})
    return {"output": result}


def main():
    """Run quick evaluation and print weighted results"""

    # CONFIGURA√á√ÉO: Ajuste conforme seu setup
    DATASET_NAME = "golden-dataset"  # Nome do dataset no LangSmith
    TARGET_FUNCTION = example_llm_app  # Sua fun√ß√£o LLM

    print("=" * 60)
    print("üöÄ QUICK EVALUATION - LANGSMITH")
    print("=" * 60)

    # Executar evaluation
    metrics = run_quick_eval(
        target_function=TARGET_FUNCTION,
        dataset_name=DATASET_NAME,
        experiment_prefix="quick-eval"
    )

    # ==================== EXIBIR RESULTADOS ====================

    print("\nüìä M√âTRICAS EXTRA√çDAS:")
    print("-" * 60)

    # Tabela de m√©tricas
    print(f"{'M√©trica':<30} {'Score':<12} {'Peso':<10} {'Contribui√ß√£o'}")
    print("-" * 65)

    metrics_display = [
        ("LLM-as-Judge Score", metrics["llm_judge_score"], METRIC_WEIGHTS["accuracy"]),
        ("Latency Score", metrics["latency_score"], METRIC_WEIGHTS["latency"]),
        ("Cost Score", metrics["cost_score"], METRIC_WEIGHTS["cost"]),
        ("Error Score", metrics["error_score"], METRIC_WEIGHTS["errors"])
    ]

    for name, score, weight in metrics_display:
        contribution = score * weight
        print(f"{name:<30} {score:<12.3f} {weight*100:<9.0f}% {contribution:.3f}")

    print("-" * 65)
    print(f"{'SCORE TOTAL':<30} {metrics['weighted_total_score']:<12.3f} {'100%':<10} {metrics['weighted_total_score']:.3f}")
    print("=" * 65)

    # Detalhes adicionais extra√≠dos dos metadados do LangSmith
    print(f"\nüìà Detalhes (extra√≠dos dos metadados do LangSmith):")
    print(f"  - P95 Latency: {metrics['p95_latency_ms']:.0f}ms")
    print(f"  - Avg Latency: {metrics['avg_latency_ms']:.0f}ms")
    print(f"  - Total Cost: ${metrics['total_cost']:.5f}")
    print(f"  - Avg Cost/Example: ${metrics['avg_cost_per_example']:.5f}")
    print(f"  - Errors: {metrics['error_count']}/{metrics['total_examples']} ({metrics['error_rate']*100:.1f}%)")
    print(f"  - Experiment: {metrics['experiment_name']}")
    print(f"  - LangSmith URL: {metrics['experiment_url']}")

    # Salvar resultados
    output_file = "evaluators/scripts/eval_results.json"
    with open(output_file, 'w') as f:
        json.dump(metrics, f, indent=2)
    print(f"\nüíæ Resultados salvos em: {output_file}")

    return metrics


if __name__ == "__main__":
    main()
```

4.3 **Salvar Script**

Usar `Write` para criar `evaluators/scripts/quick_evals.py` com a configura√ß√£o `DATASET_EVALUATORS` preenchida

### Passo 5: Criar Script Orquestrador Principal

5.1 **Gerar `run_all_evals.py`**

Script que executa todo o pipeline em ordem:

```python
"""
Orquestrador Principal - Evaluation Pipeline
Executa upload de datasets + quick evals em sequ√™ncia.
"""

import subprocess
import sys
from pathlib import Path

def run_command(script_name: str, description: str):
    """Executa script Python e reporta resultado"""
    print(f"\n{'='*60}")
    print(f"üîÑ {description}")
    print(f"{'='*60}\n")

    result = subprocess.run(
        [sys.executable, script_name],
        capture_output=False,
        text=True
    )

    if result.returncode != 0:
        print(f"\n‚ùå Erro ao executar {script_name}")
        sys.exit(1)

    print(f"\n‚úÖ {description} - Conclu√≠do")


def main():
    """Executa pipeline completo de evaluation"""

    scripts_dir = Path("evaluators/scripts")

    # Verificar se diret√≥rio existe
    if not scripts_dir.exists():
        print(f"‚ùå Diret√≥rio '{scripts_dir}' n√£o encontrado")
        sys.exit(1)

    # Passo 1: Upload de datasets
    upload_script = scripts_dir / "upload_datasets.py"
    if upload_script.exists():
        run_command(str(upload_script), "Passo 1: Upload de Datasets para LangSmith")
    else:
        print(f"‚ö†Ô∏è  Script {upload_script} n√£o encontrado. Skipping upload.")

    # Passo 2: Quick evaluations
    evals_script = scripts_dir / "quick_evals.py"
    if evals_script.exists():
        run_command(str(evals_script), "Passo 2: Quick Evaluations")
    else:
        print(f"‚ùå Script {evals_script} n√£o encontrado")
        sys.exit(1)

    print(f"\n{'='*60}")
    print("üéâ PIPELINE DE EVALUATION CONCLU√çDO")
    print(f"{'='*60}")


if __name__ == "__main__":
    main()
```

5.2 **Salvar Script Orquestrador**

Usar `Write` para criar `evaluators/scripts/run_all_evals.py`

### Passo 6: Criar Documenta√ß√£o

6.1 **Gerar README.md**

````markdown
# Evaluation Scripts

Scripts automatizados para upload de datasets e execu√ß√£o de quick evaluations no LangSmith.

## üìÅ Estrutura

- `upload_datasets.py`: Upload de datasets locais para LangSmith (skip se existir)
- `quick_evals.py`: Executa evaluations com evaluators customizados por dataset e m√©tricas ponderadas
- `run_all_evals.py`: Orquestrador principal (executa tudo em ordem)
- `eval_results.json`: Resultados da √∫ltima execu√ß√£o

## üéØ Configura√ß√£o Autom√°tica de Evaluators

O comando `/create-script-evals` analisa todos os datasets **durante sua execu√ß√£o** e gera o script `quick_evals.py` com a configura√ß√£o `DATASET_EVALUATORS` pr√©-populada:

### An√°lise de Dataset (Feita pelo Comando)

Durante a execu√ß√£o do `/create-script-evals`, o comando:

1. **L√™ cada dataset** em `datasets/`
2. **Analisa a estrutura** detectando:
   - Tipo de tarefa (Q&A, summarization, classification, generation)
   - Tipo de output (texto livre ou estruturado)
   - Presen√ßa de refer√™ncia (ground truth)
   - Campos dispon√≠veis (inputs/outputs)

3. **Decide evaluators apropriados** para cada dataset
4. **Gera `quick_evals.py`** com configura√ß√£o fixa:

```python
DATASET_EVALUATORS = {
    "qa-dataset": ["qa", "context_qa"],
    "summary-dataset": ["llm_as_judge"],
    "generation-dataset": ["llm_as_judge", "embedding_distance"]
}
```

### Sele√ß√£o de Crit√©rios LLM-as-Judge (Feita pelo Comando)

Baseado na an√°lise, o **comando** seleciona automaticamente o **crit√©rio LLM-as-Judge** apropriado:

| Tipo de Dataset | Crit√©rio Selecionado | Raz√£o |
|-----------------|---------------------|-------|
| **Q&A com refer√™ncia** | CORRECTNESS | Verifica precis√£o factual comparando com ground truth |
| **Q&A sem refer√™ncia** | RELEVANCE | Avalia alinhamento da resposta com a pergunta |
| **Summarization** | CONCISENESS | Mede brevidade e objetividade |
| **Gera√ß√£o de texto longo** | COHERENCE | Avalia fluxo l√≥gico e consist√™ncia |
| **Assistentes/Chatbots** | HELPFULNESS | Avalia utilidade geral da resposta |
| **Safety/Guardrails** | HARMFULNESS | Detecta potencial de dano (f√≠sico ou emocional) |
| **Modera√ß√£o** | MALICIOUSNESS | Detecta inten√ß√£o de causar dano ou enganar |
| **Conte√∫do sens√≠vel** | CONTROVERSIALITY | Avalia potencial para gerar desacordo |

### LLM-as-Judge com `openevals` (Biblioteca Pr√©-Constru√≠da Recomendada)

**IMPORTANTE**: O script usa **openevals** (biblioteca pr√©-constru√≠da recomendada).

**Alternativa**: Custom evaluators com LangSmith SDK para l√≥gica espec√≠fica.

**Crit√©rios integrados via prompts pr√©-constru√≠dos**:
1. **CORRECTNESS_PROMPT**: Precis√£o factual (requer ground truth)
2. **RELEVANCE_PROMPT**: Alinhamento com pergunta/contexto
3. **CONCISENESS_PROMPT**: Brevidade e objetividade
4. **COHERENCE_PROMPT**: Fluxo l√≥gico e consist√™ncia
5. **HELPFULNESS_PROMPT**: Utilidade geral para o usu√°rio
6. **HARMFULNESS_PROMPT**: Detec√ß√£o de conte√∫do prejudicial
7. **MALICIOUSNESS_PROMPT**: Detec√ß√£o de inten√ß√£o maliciosa
8. **CONTROVERSY_PROMPT**: Potencial para gerar controv√©rsia

**Estrutura gerada** (usando openevals):
```python
from openevals.llm import create_llm_as_judge
from openevals.prompts import CORRECTNESS_PROMPT

# RECOMENDADO: Usar openevals (biblioteca pr√©-constru√≠da)
judge = create_llm_as_judge(
    prompt=CORRECTNESS_PROMPT,  # Ou outro prompt pr√©-constru√≠do
    feedback_key="correctness",
    model="openai:gpt-4o-mini"
)
```

**Vantagens do openevals**:
- ‚úÖ Prompts pr√©-constru√≠dos e testados pela comunidade
- ‚úÖ Simples de usar (poucas linhas de c√≥digo)
- ‚úÖ Boas pr√°ticas built-in
- ‚úÖ Integra√ß√£o nativa com `langsmith.evaluate()`
- ‚úÖ Manuten√ß√£o e updates pela comunidade
- ‚úÖ Funciona com ou sem ground truth
- ‚úÖ Avalia aspectos subjetivos com consist√™ncia
- ‚úÖ Fornece justificativa detalhada

**Trade-offs**:
- ‚ö†Ô∏è Custo adicional de API (GPT-4o-mini)
- ‚ö†Ô∏è Lat√™ncia maior que evaluators rule-based
- ‚ö†Ô∏è N√£o determin√≠stico (pode variar ligeiramente)
- ‚ö†Ô∏è Menos flex√≠vel que custom evaluators (prompts fixos)

## üöÄ Como Usar

### Op√ß√£o 1: Pipeline Completo

```bash
uv run evaluators/scripts/run_all_evals.py
````

### Op√ß√£o 2: Executar Individualmente

```bash
# 1. Upload datasets
uv run evaluators/scripts/upload_datasets.py

# 2. Run evaluations
uv run evaluators/scripts/quick_evals.py
```

## üì¶ Depend√™ncias

Os scripts requerem as seguintes bibliotecas Python:

```bash
pip install openevals langsmith langchain langchain-openai openai python-dotenv numpy
```

Ou usando `uv`:

```bash
uv pip install openevals langsmith langchain langchain-openai openai python-dotenv numpy
```

**Nota**:

- A biblioteca `openevals` √© usada para LLM-as-Judge evaluators com prompts pr√©-constru√≠dos
- A biblioteca `openai` √© necess√°ria quando usando GPT-4o-mini como modelo juiz
- Para custom evaluators, voc√™ pode implementar l√≥gica espec√≠fica usando LangSmith SDK diretamente

## ‚öôÔ∏è Configura√ß√£o

### 1. Vari√°veis de Ambiente

Configure as seguintes vari√°veis no `.env`:

```bash
LANGSMITH_API_KEY=your-api-key
LANGCHAIN_PROJECT=your-project-name
LANGCHAIN_TRACING_V2=true
OPENAI_API_KEY=your-openai-key  # ou outro provider
```

**Importante**: Os scripts utilizam `python-dotenv` para carregar automaticamente as vari√°veis do arquivo `.env`. Certifique-se de que:

- O arquivo `.env` est√° na raiz do projeto
- A vari√°vel `LANGSMITH_API_KEY` est√° configurada corretamente
- A biblioteca `python-dotenv` est√° instalada (`pip install python-dotenv`)

**Configura√ß√£o de Python Path**: Os scripts automaticamente adicionam o diret√≥rio raiz do projeto ao `sys.path`:

- Estrutura esperada: `project_root/evaluators/scripts/[script].py`
- O script resolve o caminho com `.parents[2]` para alcan√ßar o diret√≥rio raiz
- Isso permite importar m√≥dulos do projeto sem conflitos de path
- Se sua estrutura for diferente, ajuste o n√∫mero em `.parents[N]` conforme necess√°rio

### 2. Ajustar Pesos das M√©tricas

Edite `quick_evals.py` para customizar os pesos:

```python
METRIC_WEIGHTS = {
    "accuracy": 0.60,      # 60% - Score do LLM-as-Judge (ajuste conforme necess√°rio)
    "latency": 0.20,       # 20% - P95 latency
    "cost": 0.15,          # 15% - Custo m√©dio por exemplo
    "errors": 0.05         # 5%  - Taxa de erro
}
```

**Importante**: A soma dos pesos deve ser 1.0 (100%)

**Nota**: "accuracy" refere-se ao score do LLM-as-Judge (pode ser CORRECTNESS, RELEVANCE, CONCISENESS, etc., dependendo do crit√©rio selecionado para o dataset)

### 3. Configurar Sua Aplica√ß√£o LLM

Em `quick_evals.py`, substitua `example_llm_app` pela sua implementa√ß√£o:

```python
def my_llm_app(inputs, config=None):
    # Sua l√≥gica aqui
    return {"output": result}

# Depois, em main():
TARGET_FUNCTION = my_llm_app
```

## üìä M√©tricas Calculadas (Todas Extra√≠das dos Metadados do LangSmith)

| M√©trica | Descri√ß√£o | Peso Padr√£o | Fonte |
|---------|-----------|-------------|-------|
| **LLM-as-Judge Score** | Score do crit√©rio LLM-as-Judge (CORRECTNESS, RELEVANCE, etc.) | 60% | LangSmith evaluator results |
| **Latency** | P95 tempo de resposta | 20% | LangSmith run metadata (run.latency) |
| **Cost** | Custo m√©dio por exemplo | 15% | LangSmith run metadata (run.total_cost) |
| **Errors** | Taxa de erro (1 - error_rate) | 5% | LangSmith run metadata (run.error) |

**Nota**: Todas as m√©tricas s√£o extra√≠das dos metadados do LangSmith ap√≥s `evaluate()`. N√£o h√° tracking manual ou callbacks customizados.

### Score Total

Score final = Œ£ (m√©trica √ó peso)

Exemplo:

- LLM-as-Judge: 0.85 √ó 0.60 = 0.510
- Latency: 0.92 √ó 0.20 = 0.184
- Cost: 0.88 √ó 0.15 = 0.132
- Errors: 0.95 √ó 0.05 = 0.0475
- **Total: 0.874 (87.4%)**

## üìÅ Formato dos Datasets

Os datasets em `datasets/` devem seguir o formato:

**JSON:**

```json
[
  {
    "input": {"question": "What is AI?"},
    "output": {"answer": "AI is artificial intelligence."}
  }
]
```

**JSONL:**

```jsonl
{"input": {"question": "What is AI?"}, "output": {"answer": "AI is artificial intelligence."}}
{"input": {"question": "What is ML?"}, "output": {"answer": "ML is machine learning."}}
```

## üîç Troubleshooting

### Erro: Dataset j√° existe

Comportamento esperado! O script faz skip autom√°tico.

### Erro: LangSmith API Key (401 "Invalid token")

**Causa**: Vari√°vel `LANGSMITH_API_KEY` n√£o est√° sendo carregada do arquivo `.env`.

**Solu√ß√µes**:

1. Verifique se o arquivo `.env` existe na raiz do projeto
1. Certifique-se de que a vari√°vel est√° definida corretamente: `LANGSMITH_API_KEY=your-api-key`
1. Instale `python-dotenv`: `pip install python-dotenv`
1. Verifique se `load_dotenv()` est√° sendo chamado no in√≠cio dos scripts
1. Teste manualmente: `python -c "from dotenv import load_dotenv; load_dotenv(); import os; print(os.getenv('LANGSMITH_API_KEY'))"`

### Erro: Nenhum dataset encontrado

Verifique se h√° arquivos `.json` ou `.jsonl` em `datasets/`.

### Erro: ModuleNotFoundError ao importar m√≥dulos do projeto

**Causa**: Script n√£o consegue importar m√≥dulos do projeto.

**Solu√ß√µes**:

1. Verifique se a estrutura de diret√≥rios est√° correta: `project_root/evaluators/scripts/`
1. Ajuste `.parents[N]` se a estrutura for diferente:
   - `.parents[1]`: Para `project_root/scripts/[script].py`
   - `.parents[2]`: Para `project_root/evaluators/scripts/[script].py` (padr√£o)
   - `.parents[3]`: Para `project_root/foo/evaluators/scripts/[script].py`
1. Teste o path: Adicione `print(f"Project root: {project_root}")` ap√≥s `project_root = ...`
1. Verifique se os m√≥dulos que voc√™ quer importar existem no `project_root`

## üìñ Refer√™ncias

- [LangSmith Evaluation Docs](https://docs.smith.langchain.com/evaluation)
- [LangChain Evaluators](https://python.langchain.com/docs/guides/productionization/evaluation/)

````

6.2 **Salvar README**

Usar `Write` para criar `evaluators/scripts/README.md`

### Passo 7: Valida√ß√£o e Teste

7.1 **Verificar Estrutura Criada**

Usar `Glob` para confirmar:
- `evaluators/scripts/upload_datasets.py` existe
- `evaluators/scripts/quick_evals.py` existe
- `evaluators/scripts/run_all_evals.py` existe
- `evaluators/scripts/README.md` existe

7.2 **Executar Valida√ß√£o Sint√°tica (Opcional)**

```bash
uv run python -m py_compile evaluators/scripts/*.py
````

## üìä Formato de Sa√≠da

### Durante Execu√ß√£o

```text
============================================================
üöÄ QUICK EVALUATION - LANGSMITH (LLM-as-Judge Only)
============================================================

üìä Evaluators para Dataset 'golden-dataset':
   ‚úÖ LLM-as-Judge (CORRECTNESS)
      - Model: openai:gpt-4o-mini
      - Input Keys: ['question']
      - Reference Keys: ['expected_answer']
      - Prediction Key: answer

üîç Running evaluation...
‚úÖ Evaluation complete

üìä M√âTRICAS EXTRA√çDAS:
-----------------------------------------------------------------
M√©trica                        Score        Peso       Contribui√ß√£o
-----------------------------------------------------------------
LLM-as-Judge Score            0.850        60%        0.510
Latency Score                 0.920        20%        0.184
Cost Score                    0.880        15%        0.132
Error Score                   0.950        5%         0.048
-----------------------------------------------------------------
SCORE TOTAL                   0.874        100%       0.874
=================================================================

üìà Detalhes (extra√≠dos dos metadados do LangSmith):
  - P95 Latency: 450ms
  - Avg Latency: 380ms
  - Total Cost: $0.03750
  - Avg Cost/Example: $0.00125
  - Errors: 2/30 (6.7%)
  - Experiment: quick-eval-abc123
  - LangSmith URL: https://smith.langchain.com/...

üíæ Resultados salvos em: evaluators/scripts/eval_results.json
```

### Arquivo JSON Gerado

```json
{
  "llm_judge_score": 0.850,
  "p95_latency_ms": 450,
  "avg_latency_ms": 380,
  "latency_score": 0.920,
  "total_cost": 0.03750,
  "avg_cost_per_example": 0.00125,
  "cost_score": 0.880,
  "error_count": 2,
  "total_examples": 30,
  "error_rate": 0.067,
  "error_score": 0.933,
  "weighted_total_score": 0.874,
  "experiment_name": "quick-eval-abc123",
  "experiment_url": "https://smith.langchain.com/..."
}
```

## ‚úÖ Crit√©rios de Sucesso

- [ ] Skills de evaluation consultadas (evaluation-developer, evals-automator, datasets-evals, quick-evals)
- [ ] Diret√≥rio `evaluators/scripts/` criado (se n√£o existia)
- [ ] **Comando analisou** cada dataset em `datasets/` usando `Read`
- [ ] **Comando detectou** para cada dataset: tipo de tarefa, tipo de output, presen√ßa de refer√™ncia, campos de input/output
- [ ] **Comando decidiu** crit√©rios LLM-as-Judge apropriados para cada dataset (CORRECTNESS, RELEVANCE, CONCISENESS, etc.)
- [ ] **Comando criou** dict `dataset_evaluators` mapeando datasets ‚Üí configura√ß√£o LLM-as-Judge (tipo, crit√©rio, chaves)
- [ ] Script `upload_datasets.py` criado com skip logic
- [ ] `load_dotenv()` adicionado no in√≠cio de `upload_datasets.py`
- [ ] Configura√ß√£o de `sys.path` adicionada em `upload_datasets.py`
- [ ] Script `quick_evals.py` criado com 5 m√©tricas ponderadas
- [ ] `load_dotenv()` adicionado no in√≠cio de `quick_evals.py`
- [ ] Configura√ß√£o de `sys.path` adicionada em `quick_evals.py`
- [ ] **Constante `DATASET_EVALUATORS`** preenchida com configura√ß√£o completa de LLM-as-Judge (crit√©rios + chaves)
- [ ] LLM-as-Judge implementado usando `openevals.llm.create_llm_as_judge` com prompts pr√©-constru√≠dos
- [ ] Fun√ß√£o `select_evaluators_for_dataset()` usa `create_llm_as_judge` com prompts openevals baseados no crit√©rio
- [ ] Crit√©rios LLM-as-Judge customizados por dataset baseado na an√°lise feita pelo comando
- [ ] Imports corretos: `from openevals.llm import create_llm_as_judge` e `from openevals.prompts import ...`
- [ ] Pesos das m√©tricas configur√°veis e somam 1.0
- [ ] M√©tricas normalizadas para 0-1 corretamente
- [ ] Score total calculado com pondera√ß√£o
- [ ] Script orquestrador `run_all_evals.py` criado
- [ ] README.md com documenta√ß√£o completa gerado
- [ ] Exemplo de target function inclu√≠do
- [ ] Error handling implementado
- [ ] Resultados salvos em JSON
- [ ] Formato de output claro e tabular
- [ ] Valida√ß√£o sint√°tica passa (opcional)

## üìù Exemplo de Uso

```bash
# Criar scripts de evaluation
/create-script-evals

# Executar pipeline completo
uv run evaluators/scripts/run_all_evals.py

# Ou executar individualmente
uv run evaluators/scripts/upload_datasets.py
uv run evaluators/scripts/quick_evals.py
```

## ‚ùå Anti-Patterns

### ‚ùå Erro 1: Pesos n√£o somam 100%

N√£o configure pesos que n√£o somam 1.0:

```python
# ‚ùå Errado - Soma = 0.95
METRIC_WEIGHTS = {
    "accuracy": 0.60,
    "latency": 0.20,
    "cost": 0.10,
    "errors": 0.05
}

# ‚úÖ Correto - Soma = 1.0
METRIC_WEIGHTS = {
    "accuracy": 0.60,  # LLM-as-Judge score
    "latency": 0.20,
    "cost": 0.15,
    "errors": 0.05
}
```

### ‚ùå Erro 2: N√£o normalizar m√©tricas

N√£o use m√©tricas em escalas diferentes diretamente:

```python
# ‚ùå Errado - Latency em ms (0-2000), accuracy em 0-1
score = accuracy + latency_ms  # Escalas incompat√≠veis!

# ‚úÖ Correto - Normalizar para 0-1
latency_score = normalize_score(latency_ms, 0, 2000, invert=True)
score = accuracy * 0.5 + latency_score * 0.5
```

### ‚ùå Erro 3: Hardcoded dataset names

N√£o hardcode nomes de datasets:

```python
# ‚ùå Errado
results = evaluate(target, data="my-specific-dataset")

# ‚úÖ Correto - Configur√°vel
DATASET_NAME = os.getenv("EVAL_DATASET", "golden-dataset")
results = evaluate(target, data=DATASET_NAME)
```

### ‚ùå Erro 4: Ignorar errors no upload

N√£o ignore erros silenciosamente:

```python
# ‚ùå Errado
try:
    client.create_dataset(name)
except:
    pass  # Silent fail!

# ‚úÖ Correto
try:
    existing = client.read_dataset(dataset_name=name)
    print(f"‚úÖ Dataset '{name}' j√° existe. Skipping.")
    return name
except:
    # Create new
    dataset = client.create_dataset(dataset_name=name)
    print(f"üì§ Created new dataset '{name}'")
```

### ‚ùå Erro 5: N√£o carregar vari√°veis de ambiente do .env

N√£o esquecer de carregar o arquivo `.env` no in√≠cio dos scripts:

```python
# ‚ùå Errado - 401 "Invalid token" error
from langsmith import Client
# LANGSMITH_API_KEY n√£o foi carregada do .env
client = Client()

# ‚úÖ Correto - Carregar .env primeiro
from dotenv import load_dotenv
from langsmith import Client

load_dotenv()  # Carrega LANGSMITH_API_KEY e outras vari√°veis
client = Client()
```

**Consequ√™ncias de n√£o usar `load_dotenv()`**:

- Erro 401 "Invalid token" ao autenticar com LangSmith
- Vari√°veis do `.env` n√£o s√£o carregadas no ambiente
- Scripts falham mesmo com `.env` configurado corretamente

### ‚ùå Erro 6: N√£o configurar Python path para imports de projeto

N√£o esquecer de adicionar o diret√≥rio raiz do projeto ao `sys.path`:

```python
# ‚ùå Errado - ModuleNotFoundError ao importar m√≥dulos do projeto
from langsmith import Client
# Tentando importar m√≥dulos do projeto, mas sys.path n√£o configurado
from my_project.utils import helper_function  # Falha!

# ‚úÖ Correto - Configurar sys.path primeiro
import sys
from pathlib import Path

# Add project root to Python path for imports
project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

# Agora imports funcionam
from langsmith import Client
from my_project.utils import helper_function  # Sucesso!
```

**Por que `.parents[2]`?**

- Script est√° em: `project_root/evaluators/scripts/upload_datasets.py`
- `.parents[0]`: `evaluators/scripts/upload_datasets.py` (o pr√≥prio arquivo)
- `.parents[1]`: `evaluators/scripts/` (diret√≥rio pai)
- `.parents[2]`: `evaluators/` (diret√≥rio av√¥)
- `.parents[3]`: `project_root/` (raiz do projeto) ‚Üê Este √© o objetivo!

**Ajuste conforme sua estrutura**:

- `project_root/scripts/`: Use `.parents[1]`
- `project_root/evaluators/scripts/`: Use `.parents[2]` (padr√£o)
- `project_root/foo/bar/scripts/`: Use `.parents[3]`

### ‚ùå Erro 7: Usar API incorreta ou implementar LLM-as-Judge manualmente

**SEMPRE** use `openevals` (recomendado) ou custom evaluators com LangSmith SDK:

```python
# ‚ùå ERRADO - Implementa√ß√£o manual de LLM-as-Judge
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

def custom_llm_judge(inputs, outputs):
    llm = ChatOpenAI(model="gpt-4o-mini")
    prompt = ChatPromptTemplate.from_messages([
        ("system", "You are a judge. Evaluate this response..."),
        ("user", "Input: {input}\nOutput: {output}")
    ])
    # Implementa√ß√£o manual complexa e n√£o validada
    ...

# ‚ùå ERRADO - API incorreta (n√£o existe em langsmith.evaluation)
from langsmith.evaluation import create_llm_as_judge  # N√£o existe!

# ‚úÖ CORRETO - Usar openevals (biblioteca pr√©-constru√≠da recomendada)
from openevals.llm import create_llm_as_judge
from openevals.prompts import CORRECTNESS_PROMPT

judge = create_llm_as_judge(
    prompt=CORRECTNESS_PROMPT,
    feedback_key="correctness",
    model="openai:gpt-4o-mini"
)

# ‚úÖ ALTERNATIVA - Custom evaluator com LangSmith SDK (mais flex√≠vel)
from langsmith.schemas import Run, Example
from langchain_openai import ChatOpenAI

def my_custom_judge(run: Run, example: Example) -> dict:
    llm = ChatOpenAI(model="gpt-4o-mini")
    # Sua l√≥gica customizada aqui
    ...
    return {"key": "custom_score", "score": 0.85}
```

**Por que usar openevals**:

- ‚úÖ Prompts pr√©-constru√≠dos e testados pela comunidade
- ‚úÖ Simples de usar (poucas linhas de c√≥digo)
- ‚úÖ Boas pr√°ticas built-in
- ‚úÖ Integra√ß√£o nativa com langsmith.evaluate()
- ‚úÖ Menos c√≥digo e mais confi√°vel

**Quando usar custom evaluators**:

- ‚úÖ L√≥gica √∫nica (ex: valida√ß√£o de formato JSON)
- ‚úÖ Prompts muito espec√≠ficos do dom√≠nio
- ‚úÖ Combinar m√∫ltiplos judges
- ‚úÖ Total controle do prompt e l√≥gica

**Consequ√™ncias de implementa√ß√£o manual sem framework**:

- ‚ùå C√≥digo n√£o validado e propenso a erros
- ‚ùå Sem garantia de qualidade dos crit√©rios
- ‚ùå Sem integra√ß√£o nativa com LangSmith
- ‚ùå Precisa manter e atualizar manualmente
- ‚ùå Pode ter bugs ou comportamento inconsistente

### ‚ùå Erro 8: Gerar script com crit√©rios LLM-as-Judge fixos sem analisar datasets

N√£o gere `quick_evals.py` com configura√ß√£o vazia ou crit√©rios gen√©ricos:

```python
# ‚ùå Errado - Configura√ß√£o vazia
DATASET_EVALUATORS = {}

# ‚ùå Errado - Todos datasets com mesmo crit√©rio gen√©rico
DATASET_EVALUATORS = {
    "qa-dataset": {"type": "llm_as_judge", "criteria": "CORRECTNESS", ...},
    "summary-dataset": {"type": "llm_as_judge", "criteria": "CORRECTNESS", ...},
    "safety-test": {"type": "llm_as_judge", "criteria": "CORRECTNESS", ...}
}
# CORRECTNESS pode n√£o ser apropriado para summarization (CONCISENESS) ou safety (HARMFULNESS)

# ‚úÖ Correto - Comando analisa datasets ANTES e seleciona crit√©rios apropriados

# 1. Comando usa Read para ler cada dataset
# 2. Comando detecta:
#    - Tipo de tarefa (Q&A, summarization, generation, chatbot, safety)
#    - Presen√ßa de refer√™ncia (ground truth)
#    - Natureza da avalia√ß√£o (precis√£o, brevidade, utilidade, seguran√ßa)
#    - Campos de input/output
# 3. Comando decide crit√©rio LLM-as-Judge apropriado
# 4. Comando gera quick_evals.py com configura√ß√£o customizada:

DATASET_EVALUATORS = {
    "qa-dataset": {
        "type": "llm_as_judge",
        "criteria": "CORRECTNESS",  # Q&A com refer√™ncia ‚Üí precis√£o factual
        "input_keys": ["question"],
        "reference_output_keys": ["expected_answer"],
        "prediction_key": "answer"
    },
    "summary-dataset": {
        "type": "llm_as_judge",
        "criteria": "CONCISENESS",  # Summarization ‚Üí brevidade
        "input_keys": ["text"],
        "reference_output_keys": ["summary"],
        "prediction_key": "output"
    },
    "chatbot-dataset": {
        "type": "llm_as_judge",
        "criteria": "HELPFULNESS",  # Chatbot ‚Üí utilidade
        "input_keys": ["user_message"],
        "reference_output_keys": None,
        "prediction_key": "response"
    },
    "safety-test": {
        "type": "llm_as_judge",
        "criteria": "HARMFULNESS",  # Safety ‚Üí detectar dano
        "input_keys": ["prompt"],
        "reference_output_keys": None,
        "prediction_key": "completion"
    }
}
```

**Por que o comando deve analisar datasets ANTES**:

- Cada tipo de tarefa precisa de crit√©rio LLM-as-Judge espec√≠fico
- CORRECTNESS n√£o funciona para summarization (use CONCISENESS)
- HELPFULNESS n√£o funciona para safety tests (use HARMFULNESS)
- Crit√©rio errado gera scores sem sentido ou avalia√ß√µes incorretas
- An√°lise em tempo de execu√ß√£o do comando √© mais eficiente
- Script gerado j√° vem customizado por tipo de tarefa

**Consequ√™ncias de n√£o analisar no comando**:

- Avalia√ß√µes com crit√©rios inapropriados (ex: CORRECTNESS para safety)
- Scores sem sentido (ex: medir concis√£o em vez de seguran√ßa)
- Falhas silenciosas ou resultados enganosos
- Usu√°rio precisa editar manualmente a configura√ß√£o
- Perda de tempo com avalia√ß√µes irrelevantes
