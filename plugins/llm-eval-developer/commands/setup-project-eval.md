---
description: Configura CLAUDE.md do projeto com padrÃµes de LLM evaluation, frameworks, mÃ©tricas e estrutura de evaluators
---

# Setup Project for LLM Evaluation

Este comando configura o arquivo `CLAUDE.md` do projeto atual com instruÃ§Ãµes sobre desenvolvimento de evaluations de LLMs, frameworks, mÃ©tricas e padrÃµes de testing.

## ğŸ¯ Objetivo

Adicionar ao `CLAUDE.md` do projeto instruÃ§Ãµes claras para que Claude:

- Desenvolva evaluators customizados (OpenEvals, LangSmith, custom)
- Estruture evaluation suites adequadamente
- Implemente mÃ©tricas corretas (LLM-as-judge, similarity, rule-based)
- Gerencie datasets de evaluation (golden datasets, synthetic)
- Teste evaluators corretamente
- Configure CI/CD para regression testing

## ğŸ“‹ Como usar

````bash
/setup-project-eval

```text

Ou com descriÃ§Ã£o do tipo de evaluation:

```bash
/setup-project-eval "RAG system evaluation com LangSmith + OpenAI"

```text

## ğŸ” Processo de ExecuÃ§Ã£o

### 1. Detectar ou Criar CLAUDE.md

**Se CLAUDE.md existe**:
- Ler arquivo atual
- Adicionar seÃ§Ã£o "LLM Evaluation Standards" ao final
- Preservar conteÃºdo existente

**Se CLAUDE.md NÃƒO existe**:
- Criar arquivo na raiz do projeto
- Adicionar template completo de padrÃµes de evaluation

### 2. Adicionar InstruÃ§Ãµes de LLM Evaluation

O comando deve adicionar a seguinte seÃ§Ã£o ao `CLAUDE.md`:

```markdown

# LLM Evaluation Standards

**IMPORTANTE**: Este projeto utiliza o plugin `llm-eval-developer` para desenvolvimento de evaluations de LLMs com padrÃµes consistentes e frameworks profissionais.

## ğŸ“‹ PadrÃµes de Evaluation

### âœ… Frameworks de Evaluation

**Framework Principal**: [OpenEvals/LangSmith/Custom - detectado automaticamente]

**Bibliotecas de Evaluation**:
- Evaluation framework: OpenEvals, LangSmith
- MÃ©tricas: BLEU, ROUGE, BERTScore
- LLM-as-judge: OpenAI, Anthropic, local models
- Dataset management: datasets, pandas
- Testing: pytest, pytest-mock

### ğŸ“ Estrutura de DiretÃ³rios

```text

projeto/
â”œâ”€â”€ src/                        # CÃ³digo do sistema (LLM app, RAG, etc.)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ chains/
â”‚   â”œâ”€â”€ agents/
â”‚   â””â”€â”€ tools/
â”œâ”€â”€ evaluations/                # Evaluations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ evaluators/            # Evaluators customizados
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ hallucination_detector.py
â”‚   â”‚   â”œâ”€â”€ relevance_evaluator.py
â”‚   â”‚   â””â”€â”€ custom_metric.py
â”‚   â”œâ”€â”€ datasets/              # Golden datasets
â”‚   â”‚   â”œâ”€â”€ qa_golden.json
â”‚   â”‚   â”œâ”€â”€ rag_examples.json
â”‚   â”‚   â””â”€â”€ edge_cases.json
â”‚   â”œâ”€â”€ suites/                # Evaluation suites
â”‚   â”‚   â”œâ”€â”€ rag_eval_suite.py
â”‚   â”‚   â””â”€â”€ agent_eval_suite.py
â”‚   â””â”€â”€ tests/                 # Testes dos evaluators
â”‚       â”œâ”€â”€ test_evaluators.py
â”‚       â””â”€â”€ conftest.py
â”œâ”€â”€ scripts/                   # Scripts de execuÃ§Ã£o
â”‚   â”œâ”€â”€ run_eval.py
â”‚   â””â”€â”€ generate_report.py
â””â”€â”€ .github/workflows/         # CI/CD
    â””â”€â”€ eval_regression.yml

```text

### ğŸ¯ Tipos de Evaluators

#### 1. LLM-as-Judge

**Quando usar**: CritÃ©rios subjetivos (relevance, hallucination, coherence, helpfulness)

```python
from langsmith.evaluation import evaluator
from openai import OpenAI

@evaluator
def hallucination_detector(run, example):
    """
    Detecta alucinaÃ§Ãµes comparando output com contexto fornecido.

    Args:
        run: ExecuÃ§Ã£o do sistema (contÃ©m output)
        example: Exemplo de teste (contÃ©m contexto esperado)

    Returns:
        dict: Score (0-1) e raciocÃ­nio
    """
    client = OpenAI()

    prompt = f"""
    Compare o output gerado com o contexto fornecido.
    Identifique se hÃ¡ informaÃ§Ãµes alucinadas (nÃ£o presentes no contexto).

    Contexto: {example.inputs['context']}
    Output: {run.outputs['answer']}

    Retorne JSON:
    {{
        "score": 0.0-1.0,  // 1.0 = sem alucinaÃ§Ã£o, 0.0 = muita alucinaÃ§Ã£o
        "reasoning": "explicaÃ§Ã£o"
    }}
    """

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"}
    )

    result = json.loads(response.choices[0].message.content)
    return {
        "key": "hallucination",
        "score": result["score"],
        "comment": result["reasoning"]
    }

```text

#### 2. Similarity-Based

**Quando usar**: ComparaÃ§Ã£o com output esperado (exact match, semantic similarity)

```python
from langsmith.evaluation import evaluator
from sentence_transformers import SentenceTransformer
import numpy as np

model = SentenceTransformer('all-MiniLM-L6-v2')

@evaluator
def semantic_similarity(run, example):
    """
    Calcula similaridade semÃ¢ntica entre output e resposta esperada.

    Returns:
        dict: Cosine similarity score (0-1)
    """
    expected = example.outputs['answer']
    actual = run.outputs['answer']

    # Calcular embeddings
    emb_expected = model.encode(expected)
    emb_actual = model.encode(actual)

    # Cosine similarity
    similarity = np.dot(emb_expected, emb_actual) / (
        np.linalg.norm(emb_expected) * np.linalg.norm(emb_actual)
    )

    return {
        "key": "semantic_similarity",
        "score": float(similarity)
    }

```text

#### 3. Rule-Based

**Quando usar**: ValidaÃ§Ãµes objetivas (formato, length, keywords, regex)

```python
from langsmith.evaluation import evaluator

@evaluator
def answer_format_validator(run, example):
    """
    Valida se a resposta segue formato esperado.

    ValidaÃ§Ãµes:
    - Tem pelo menos 50 caracteres
    - NÃ£o contÃ©m placeholders
    - Termina com pontuaÃ§Ã£o
    """
    answer = run.outputs['answer']

    validations = {
        "min_length": len(answer) >= 50,
        "no_placeholders": "[TODO]" not in answer and "..." not in answer,
        "proper_punctuation": answer.strip()[-1] in ".!?"
    }

    score = sum(validations.values()) / len(validations)

    return {
        "key": "format_validation",
        "score": score,
        "comment": f"Validations: {validations}"
    }

```text

### ğŸ“Š Dataset Management

#### Golden Datasets

**Estrutura recomendada** (JSON):

```json
{
  "examples": [
    {
      "id": "example_001",
      "inputs": {
        "question": "What is the capital of France?",
        "context": "France is a country in Europe. Its capital city is Paris."
      },
      "outputs": {
        "answer": "Paris",
        "confidence": 1.0
      },
      "metadata": {
        "category": "geography",
        "difficulty": "easy",
        "created_at": "2025-01-20"
      }
    }
  ]
}

```text

#### Synthetic Dataset Generation

```python
from openai import OpenAI

def generate_synthetic_examples(topic: str, count: int):
    """
    Gera exemplos sintÃ©ticos para evaluation.

    Args:
        topic: TÃ³pico dos exemplos
        count: Quantidade de exemplos

    Returns:
        list: Exemplos gerados
    """
    client = OpenAI()
    examples = []

    for i in range(count):
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{
                "role": "user",
                "content": f"Generate a question-answer pair about {topic}. Return JSON."
            }]
        )

        example = json.loads(response.choices[0].message.content)
        examples.append({
            "id": f"synthetic_{i:03d}",
            "inputs": {"question": example["question"]},
            "outputs": {"answer": example["answer"]},
            "metadata": {"source": "synthetic", "topic": topic}
        })

    return examples

```text

### ğŸ§ª Testing Evaluators

**Sempre testar evaluators com pytest**:

```python
import pytest
from evaluations.evaluators.hallucination_detector import hallucination_detector

def test_hallucination_detector_no_hallucination(mocker):
    """Deve retornar score alto para resposta sem alucinaÃ§Ã£o"""
    # Arrange
    mock_openai = mocker.patch('openai.OpenAI')
    mock_openai.return_value.chat.completions.create.return_value.choices = [
        mocker.MagicMock(
            message=mocker.MagicMock(
                content='{"score": 1.0, "reasoning": "No hallucination detected"}'
            )
        )
    ]

    run = mocker.MagicMock(outputs={"answer": "Paris is the capital of France"})
    example = mocker.MagicMock(
        inputs={"context": "France's capital is Paris"},
        outputs={"answer": "Paris"}
    )

    # Act
    result = hallucination_detector(run, example)

    # Assert
    assert result["score"] == 1.0
    assert "hallucination" in result["key"]

def test_hallucination_detector_with_hallucination(mocker):
    """Deve retornar score baixo para resposta alucinada"""
    # Similar ao anterior, mas com score baixo
    pass

```text

### ğŸ”„ CI/CD para Regression Testing

**GitHub Actions** (.github/workflows/eval_regression.yml):

```yaml
name: Evaluation Regression Tests

on:
  pull_request:
    branches: [main]
  push:
    branches: [main]

jobs:
  eval-regression:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Run evaluation suite
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          python scripts/run_eval.py --suite rag_eval_suite

      - name: Check regression
        run: |
          python scripts/check_regression.py --baseline baseline_scores.json

      - name: Upload results
        uses: actions/upload-artifact@v3
        with:
          name: eval-results
          path: eval_results.json

```text

### ğŸ“ˆ MÃ©tricas Recomendadas por Tipo de Sistema

#### RAG Systems
- âœ… **Hallucination Detection**: LLM-as-judge
- âœ… **Relevance**: LLM-as-judge ou semantic similarity
- âœ… **Faithfulness**: Verifica se output estÃ¡ no contexto
- âœ… **Context Precision**: Ranking de documentos recuperados
- âœ… **Answer Correctness**: Exact match ou similarity com golden answer

#### Conversational Agents
- âœ… **Coherence**: LLM-as-judge
- âœ… **Helpfulness**: LLM-as-judge
- âœ… **Safety**: Rule-based (detectar conteÃºdo imprÃ³prio)
- âœ… **Tool Usage**: Validar se tools corretos foram chamados
- âœ… **Multi-turn Consistency**: Verificar contexto entre turnos

#### Summarization
- âœ… **ROUGE**: N-gram overlap com summary esperado
- âœ… **Factuality**: LLM-as-judge comparando com fonte
- âœ… **Conciseness**: Length ratio
- âœ… **Coverage**: Verifica se pontos principais estÃ£o presentes

#### Code Generation
- âœ… **Syntax Validation**: AST parsing
- âœ… **Test Pass Rate**: Executar testes gerados
- âœ… **Code Similarity**: Diff com cÃ³digo esperado
- âœ… **Security**: Detectar vulnerabilidades

## ğŸ¯ Plugin LLM Eval Developer

Este projeto usa o plugin `llm-eval-developer` com os seguintes recursos:

**Comandos**:
- `/create-evaluator` - Gerar cÃ³digo de evaluator customizado
- `/create-eval-suite` - Scaffold completo de evaluation suite
- `/eval-metrics` - Documentar mÃ©tricas disponÃ­veis
- `/eval-patterns` - Mostrar padrÃµes de desenvolvimento

**Agente**:
- `eval-developer` - Especialista em criar cÃ³digo de evaluators

**Skill**:
- `evaluation-developer` - Auto-invocada para scaffolding de evaluations

**Frameworks Suportados**:
- OpenEvals (langchain-ai/openevals)
- LangSmith Evaluation
- Custom implementations

**MÃ©tricas**:
- LLM-as-judge (GPT-4, Claude, local models)
- Similarity-based (BLEU, ROUGE, BERTScore, cosine)
- Rule-based (regex, format, length, keywords)
- Composite (mÃºltiplas mÃ©tricas combinadas)


**Filosofia**: Measure > Guess | Automate > Manual | Regression Tests > Hope

```text

### 3. Adicionar Contexto do Projeto (Se Fornecido)

Se o usuÃ¡rio fornecer descriÃ§Ã£o do tipo de evaluation, adicionar seÃ§Ã£o customizada:

```markdown

## ğŸ“Š Contexto Deste Projeto

**Tipo de Sistema**: [RAG/Agent/Summarization/Code Gen]
**Framework de Evaluation**: [OpenEvals/LangSmith/Custom]

**Evaluators Recomendados**:
- [Hallucination detector para RAG]
- [Relevance evaluator para retrieval]
- [Safety checker para conversational]

**MÃ©tricas Principais**:
- [MÃ©tricas especÃ­ficas do tipo de sistema]

**Datasets NecessÃ¡rios**:
- Golden examples: [quantidade recomendada]
- Edge cases: [exemplos especÃ­ficos]
- Regression test cases: [baseline]

```text

### 4. Detectar Stack de Evaluation do Projeto

Analisar projeto para customizar instruÃ§Ãµes:

- Verificar `requirements.txt`, `pyproject.toml` para frameworks
- Detectar se usa OpenEvals, LangSmith, ou custom
- Identificar modelos LLM em uso (OpenAI, Anthropic, local)
- Localizar datasets existentes (JSON, CSV)
- Verificar se hÃ¡ evaluators implementados

**Adicionar ao CLAUDE.md**:

```markdown

## ğŸ”§ Stack de Evaluation Detectada

**Framework**: [OpenEvals/LangSmith/Custom]
**LLM Provider**: [OpenAI/Anthropic/Local]
**Dataset Format**: [JSON/CSV/HuggingFace]
**Testing**: [pytest/unittest]

**Evaluators Existentes**:
- [listar evaluators encontrados]

**Datasets Existentes**:
- [listar datasets encontrados]

```text

### 5. Confirmar com UsuÃ¡rio

Mostrar preview do que serÃ¡ adicionado:

```text

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“ SETUP LLM EVALUATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Arquivo: CLAUDE.md

AÃ§Ã£o: [CRIAR NOVO / ADICIONAR SEÃ‡ÃƒO]

Stack de Evaluation Detectada:
- Framework: [OpenEvals/LangSmith]
- LLM Provider: [OpenAI/Anthropic]
- Tipo de Sistema: [RAG/Agent/Summarization]

Evaluators Existentes:
- [X] evaluators detectados

Datasets Existentes:
- [Y] golden datasets encontrados

ConteÃºdo a ser adicionado:

[Preview das instruÃ§Ãµes]

Adicionar ao CLAUDE.md? (s/n)

```text

### 6. Criar/Atualizar Arquivo

Se usuÃ¡rio confirmar:
- Criar ou atualizar CLAUDE.md
- Adicionar instruÃ§Ãµes completas
- Preservar conteÃºdo existente (NUNCA sobrescrever)
- Validar que arquivo foi criado corretamente

```text

âœ… CLAUDE.md configurado com sucesso!

InstruÃ§Ãµes de LLM evaluation adicionadas.

Stack de Evaluation detectada:
- Framework: LangSmith
- LLM Provider: OpenAI
- Tipo de Sistema: RAG

Evaluators existentes:
- 3 evaluators encontrados

Datasets existentes:
- 2 golden datasets

PrÃ³ximos passos:
1. Revisar CLAUDE.md
2. Customizar evaluators (se necessÃ¡rio)
3. Executar: /create-evaluator
4. Criar eval suite: /create-eval-suite
5. Testar evaluators: pytest evaluations/tests/

Claude agora estÃ¡ orientado a:
âœ“ Desenvolver evaluators customizados
âœ“ Estruturar evaluation suites adequadamente
âœ“ Implementar mÃ©tricas corretas
âœ“ Gerenciar datasets de evaluation
âœ“ Testar evaluators com pytest
âœ“ Configurar CI/CD para regression testing

```text

## ğŸ“š Exemplos de Uso

### Exemplo 1: Novo Projeto RAG

```bash
/setup-project-eval "RAG system com LangSmith + OpenAI"

```text

**Resultado**:
- Cria `CLAUDE.md` na raiz do projeto
- Adiciona padrÃµes LangSmith + OpenAI
- Configura evaluators para RAG (hallucination, relevance, faithfulness)
- Define estrutura de golden datasets
- Orienta sobre LLM-as-judge

### Exemplo 2: Projeto Conversational Agent

```bash
/setup-project-eval "Conversational agent com OpenEvals"

```text

**Resultado**:
- Detecta `CLAUDE.md` existente
- Adiciona seÃ§Ã£o de evaluation ao final
- Preserva conteÃºdo existente
- Inclui evaluators de coherence, helpfulness, safety
- Configura multi-turn consistency testing

### Exemplo 3: Projeto Code Generation

```bash
/setup-project-eval "Code generation LLM"

```text

**Resultado**:
- Adiciona evaluators de syntax validation
- Configura test execution evaluation
- Orienta sobre security scanning
- Define mÃ©tricas de code similarity

## âš ï¸ Importante

### NÃ£o Sobrescrever ConteÃºdo Existente

Se `CLAUDE.md` jÃ¡ existe:
- âŒ NUNCA sobrescrever conteÃºdo
- âœ… SEMPRE adicionar ao final
- âœ… Usar separador claro: `---`

### Detectar Framework de Evaluation

Analisar projeto para customizar instruÃ§Ãµes:
- Verificar se usa OpenEvals, LangSmith, ou custom
- Detectar modelos LLM (OpenAI, Anthropic, local)
- Identificar tipo de sistema (RAG, agent, summarization)
- Localizar datasets e evaluators existentes

### Validar Sintaxe Markdown

ApÃ³s criar/atualizar:
- Verificar que markdown estÃ¡ vÃ¡lido
- Headers bem formatados
- Code blocks Python com syntax highlighting
- Links funcionando

## ğŸš€ ApÃ³s Executar Este Comando

O usuÃ¡rio terÃ¡:

1. âœ… `CLAUDE.md` configurado com padrÃµes de LLM evaluation
2. âœ… Claude orientado a desenvolver evaluators customizados
3. âœ… Estrutura de evaluation suite definida
4. âœ… MÃ©tricas recomendadas documentadas
5. âœ… PadrÃµes de testing de evaluators

**PrÃ³ximo passo**: Executar `/create-evaluator` para criar primeiro evaluator!

## ğŸ’¡ Dica

ApÃ³s configurar o projeto, sempre teste evaluators antes de usar em production:

```bash

# Criar evaluator customizado
/create-evaluator

# Testar evaluator
pytest evaluations/tests/

# Criar evaluation suite completa
/create-eval-suite

# Executar evaluation
python scripts/run_eval.py

```text

Isso garantirÃ¡ que evaluations sÃ£o confiÃ¡veis e detectam regressÃµes corretamente.
````
