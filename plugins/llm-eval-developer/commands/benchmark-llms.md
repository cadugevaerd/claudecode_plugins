---
description: Cria suite de benchmark comparativo para avaliar m√∫ltiplos LLMs usando LangChain/LangGraph e LangSmith para tracking autom√°tico de m√©tricas
---

# Benchmark LLMs - Comparative Evaluation com LangChain/LangGraph

Cria **suite completa de benchmarking comparativo** para avaliar m√∫ltiplos LLMs usando **LangChain/LangGraph** para execu√ß√£o e **LangSmith** para tracking autom√°tico de:
- **M√©tricas de qualidade** (accuracy, relevance, hallucination via LangSmith evaluators)
- **M√©tricas de performance** (latency P50/P95/P99, TTFT via callbacks)
- **M√©tricas de custo** (tracking autom√°tico via LangSmith)
- **M√©tricas de seguran√ßa** (bias detection, toxicity via LLM-as-judge)

## üéØ Objetivo

Gerar c√≥digo funcional completo usando **LangChain Expression Language (LCEL)** e **LangSmith** para executar benchmarks comparativos que permitam **decis√µes data-driven** sobre qual LLM usar.

## üìã Como Usar

```bash
/benchmark-llms
```

O agente perguntar√° interativamente:

### 1. **Quais LLMs comparar?**

**Exemplos**:
- `gpt-4o, claude-3.5-sonnet, gemini-1.5-pro`
- `gpt-4o-mini, claude-3-haiku, gemini-1.5-flash` (modelos menores)
- `gpt-4, gpt-3.5-turbo, claude-3-opus, claude-3-sonnet` (mix)

**Formato**: Lista separada por v√≠rgulas

### 2. **Qual dataset/benchmark usar?**

**Op√ß√µes padr√£o**:
- **LangSmith Dataset**: Nome do dataset j√° criado no LangSmith
- **MMLU**: Multi-task Q&A (57 subjects) - knowledge & reasoning
- **HumanEval**: Code generation (164 programming problems)
- **TruthfulQA**: Factual accuracy (817 questions)
- **Custom**: Seu pr√≥prio dataset local em JSON

**Ou fornecer caminho**:
- `datasets/my_benchmark.json` (custom dataset)

### 3. **Quais m√©tricas avaliar?**

**Categorias de m√©tricas**:

‚úÖ **Qualidade/Acur√°cia** (via LangSmith evaluators):
- `accuracy` - Respostas corretas (%)
- `relevance` - Relev√¢ncia das respostas (LLM-as-judge)
- `hallucination` - Taxa de alucina√ß√µes (LLM-as-judge)
- `coherence` - Coer√™ncia do texto
- `f1_score` - F1 score para classifica√ß√£o

‚úÖ **Performance/Velocidade** (via LangChain callbacks):
- `latency_p50` - Lat√™ncia mediana
- `latency_p95` - Lat√™ncia p95 (SLA)
- `latency_p99` - Lat√™ncia p99 (worst-case)
- `ttft` - Time to First Token (streaming)
- `throughput` - Tokens por segundo
- `tpot` - Time Per Output Token

‚úÖ **Custo** (via LangSmith automatic tracking):
- `cost_per_1k_tokens` - Custo por 1K tokens
- `total_cost` - Custo total do benchmark
- `cost_efficiency` - Accuracy per dollar

‚úÖ **Robustez**:
- `task_consistency` - Consist√™ncia entre diferentes tipos de tarefa
- `failure_rate` - Taxa de falha/timeout
- `robustness_score` - Score de robustez geral

‚úÖ **Seguran√ßa & Bias** (via LangSmith evaluators):
- `bias_detection` - Detec√ß√£o de vi√©s (BBQ benchmark)
- `toxicity` - Toxicidade (ToxiGen)
- `jailbreak_resistance` - Resist√™ncia a jailbreaks
- `pii_leakage` - Vazamento de PII

**Formato**: Lista separada por v√≠rgulas ou `all` para todas

### 4. **Formato de output?**

**Op√ß√µes**:
- `json` - Resultados estruturados em JSON
- `markdown` - Relat√≥rio em Markdown (human-readable)
- `html` - Dashboard HTML interativo
- `csv` - CSV para an√°lise em Excel/Pandas
- `langsmith` - View direto no LangSmith UI (padr√£o)
- `all` - Todos os formatos

### 5. **Execu√ß√£o paralela?**

**Op√ß√µes**:
- `yes` - Executa LLMs em paralelo usando LCEL batch (mais r√°pido, mais $$$)
- `no` - Executa sequencialmente (mais lento, mais barato)

**Recomenda√ß√£o**: `yes` para 2-3 modelos, `no` para 5+ modelos

## üîç Processo de Execu√ß√£o

### Passo 1: Coletar Informa√ß√µes

Pergunta interativa sobre:
- Modelos a comparar
- Dataset/benchmark
- M√©tricas desejadas
- Formato de output
- Paralleliza√ß√£o

### Passo 2: Gerar Estrutura de Benchmark

Cria estrutura completa:

```
benchmarks/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ benchmark_config.py          # Configura√ß√£o de modelos e m√©tricas
‚îÇ   ‚îî‚îÄ‚îÄ langsmith_config.py          # Config LangSmith project/dataset
‚îú‚îÄ‚îÄ datasets/
‚îÇ   ‚îú‚îÄ‚îÄ dataset_loader.py            # Carregador de datasets LangSmith
‚îÇ   ‚îî‚îÄ‚îÄ dataset_uploader.py          # Upload dataset para LangSmith
‚îú‚îÄ‚îÄ benchmarking/
‚îÇ   ‚îú‚îÄ‚îÄ langchain_benchmark.py       # Benchmark usando LCEL chains
‚îÇ   ‚îú‚îÄ‚îÄ langgraph_benchmark.py       # Benchmark usando LangGraph (parallel)
‚îÇ   ‚îú‚îÄ‚îÄ llm_clients.py               # Clientes LangChain por provider
‚îÇ   ‚îú‚îÄ‚îÄ callbacks/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ latency_callback.py      # Callback para lat√™ncia/TTFT
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ token_callback.py        # Callback para token tracking
‚îÇ   ‚îú‚îÄ‚îÄ evaluators/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ langsmith_evaluators.py  # LangSmith evaluators nativos
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ custom_evaluators.py     # Custom evaluators
‚îÇ   ‚îî‚îÄ‚îÄ reporters/
‚îÇ       ‚îú‚îÄ‚îÄ json_reporter.py         # JSON output
‚îÇ       ‚îú‚îÄ‚îÄ markdown_reporter.py     # Markdown report
‚îÇ       ‚îú‚îÄ‚îÄ html_reporter.py         # HTML dashboard
‚îÇ       ‚îî‚îÄ‚îÄ csv_reporter.py          # CSV export
‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_benchmark.py            # Testes unit√°rios
‚îÇ   ‚îî‚îÄ‚îÄ test_evaluators.py           # Testes de evaluators
‚îú‚îÄ‚îÄ run_benchmark.py                 # Script principal
‚îú‚îÄ‚îÄ analyze_results.py               # An√°lise via LangSmith API
‚îú‚îÄ‚îÄ requirements.txt                 # Depend√™ncias
‚îî‚îÄ‚îÄ README.md                        # Documenta√ß√£o
```

### Passo 3: Implementar C√≥digo com LangChain/LangGraph

Cada arquivo √© gerado com c√≥digo completo usando LangChain/LangGraph!

**Exemplo: `langchain_benchmark.py`** (Benchmark com LCEL)

```python
"""
LangChain Benchmark Suite
Usa LCEL chains e LangSmith para benchmark comparativo.
"""

from typing import List, Dict, Any
from dataclasses import dataclass
import asyncio
import numpy as np

from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableConfig
from langchain_core.callbacks import BaseCallbackHandler

from langsmith import Client
from langsmith.evaluation import evaluate, LangChainStringEvaluator

@dataclass
class BenchmarkConfig:
    """Configura√ß√£o de benchmark"""
    models: List[Dict[str, str]]  # [{"provider": "openai", "model": "gpt-4o"}]
    dataset_name: str  # Nome do dataset no LangSmith
    evaluators: List[str]  # Lista de evaluators
    project_name: str  # Projeto LangSmith
    parallel: bool = True
    max_concurrency: int = 10


class LatencyCallback(BaseCallbackHandler):
    """Callback para medir lat√™ncia e TTFT usando LangChain"""

    def __init__(self):
        self.latencies = []
        self.ttfts = []
        self.start_time = None
        self.first_token_time = None

    def on_llm_start(self, *args, **kwargs):
        """Marca in√≠cio da infer√™ncia"""
        import time
        self.start_time = time.perf_counter()
        self.first_token_time = None

    def on_llm_new_token(self, token: str, **kwargs):
        """Captura Time to First Token"""
        if self.first_token_time is None:
            import time
            self.first_token_time = time.perf_counter()
            ttft = (self.first_token_time - self.start_time) * 1000
            self.ttfts.append(ttft)

    def on_llm_end(self, *args, **kwargs):
        """Marca fim e calcula lat√™ncia total"""
        import time
        if self.start_time:
            latency = (time.perf_counter() - self.start_time) * 1000
            self.latencies.append(latency)

    def get_stats(self) -> Dict[str, float]:
        """Retorna estat√≠sticas de performance"""
        return {
            "avg_latency_ms": np.mean(self.latencies) if self.latencies else 0.0,
            "p50_latency_ms": np.percentile(self.latencies, 50) if self.latencies else 0.0,
            "p95_latency_ms": np.percentile(self.latencies, 95) if self.latencies else 0.0,
            "p99_latency_ms": np.percentile(self.latencies, 99) if self.latencies else 0.0,
            "avg_ttft_ms": np.mean(self.ttfts) if self.ttfts else 0.0,
        }


class LangChainBenchmark:
    """
    Benchmark de m√∫ltiplos LLMs usando LangChain LCEL e LangSmith.

    Usa:
    - LCEL chains para definir workflows
    - LangSmith evaluate() API para execu√ß√£o
    - LangSmith evaluators para m√©tricas
    - Custom callbacks para lat√™ncia/TTFT
    - Automatic cost tracking via LangSmith
    """

    def __init__(self, config: BenchmarkConfig):
        self.config = config
        self.client = Client()  # LangSmith client
        self.results = {}

    def create_chain(self, model_config: Dict[str, str]):
        """
        Cria chain LCEL com modelo espec√≠fico.

        Args:
            model_config: {"provider": "openai", "model": "gpt-4o", "temperature": 0}

        Returns:
            Runnable chain (prompt | llm | parser)
        """
        # Criar LLM baseado no provider
        if model_config["provider"] == "openai":
            llm = ChatOpenAI(
                model=model_config["model"],
                temperature=model_config.get("temperature", 0)
            )
        elif model_config["provider"] == "anthropic":
            llm = ChatAnthropic(
                model=model_config["model"],
                temperature=model_config.get("temperature", 0)
            )
        elif model_config["provider"] == "google":
            llm = ChatGoogleGenerativeAI(
                model=model_config["model"],
                temperature=model_config.get("temperature", 0)
            )
        else:
            raise ValueError(f"Provider {model_config['provider']} n√£o suportado")

        # Chain LCEL: prompt | llm | parser
        prompt = ChatPromptTemplate.from_messages([
            ("system", "You are a helpful assistant."),
            ("human", "{input}")
        ])

        chain = prompt | llm | StrOutputParser()
        return chain

    def create_langsmith_evaluators(self) -> List:
        """
        Cria evaluators do LangSmith para quality metrics.

        Returns:
            Lista de evaluators prontos para usar no evaluate()
        """
        evaluators = []

        for evaluator_name in self.config.evaluators:
            if evaluator_name == "qa":
                # Q&A correctness evaluator
                evaluators.append(LangChainStringEvaluator("qa"))

            elif evaluator_name == "context_qa":
                # Context-aware Q&A evaluator
                evaluators.append(LangChainStringEvaluator("context_qa"))

            elif evaluator_name == "helpfulness":
                # Helpfulness criteria evaluator
                evaluators.append(
                    LangChainStringEvaluator("criteria",
                        config={"criteria": "helpfulness"})
                )

            elif evaluator_name == "harmfulness":
                # Harmfulness criteria evaluator
                evaluators.append(
                    LangChainStringEvaluator("criteria",
                        config={"criteria": "harmfulness"})
                )

            elif evaluator_name == "relevance":
                # Custom LLM-as-judge for relevance
                from langchain_openai import ChatOpenAI
                evaluators.append(
                    LangChainStringEvaluator("labeled_criteria",
                        config={
                            "criteria": {
                                "relevance": "Is the answer relevant to the question?"
                            },
                            "llm": ChatOpenAI(model="gpt-4o-mini")
                        }
                    )
                )

        return evaluators

    async def run_benchmark(self) -> Dict[str, Dict]:
        """
        Executa benchmark em todos os modelos usando LangSmith evaluate().

        Returns:
            Dict com resultados por modelo
        """
        print(f"üöÄ Iniciando benchmark de {len(self.config.models)} modelos...")
        print(f"üìä Dataset: {self.config.dataset_name}")
        print(f"‚öôÔ∏è  Modo: {'Paralelo (LCEL batch)' if self.config.parallel else 'Sequencial'}")
        print(f"üîç LangSmith Project: {self.config.project_name}")

        # Criar evaluators do LangSmith
        evaluators = self.create_langsmith_evaluators()

        for model_config in self.config.models:
            model_name = f"{model_config['provider']}:{model_config['model']}"

            print(f"\nüìç Benchmarking {model_name}...")

            # Criar chain LCEL
            chain = self.create_chain(model_config)

            # Callback para lat√™ncia
            latency_callback = LatencyCallback()

            # Fun√ß√£o target para evaluate
            def predict(inputs: Dict) -> Dict:
                """Wrapper para chain.invoke com callbacks"""
                config = RunnableConfig(
                    callbacks=[latency_callback],
                    max_concurrency=self.config.max_concurrency if self.config.parallel else 1
                )
                result = chain.invoke(inputs, config=config)
                return {"output": result}

            # Executar evaluation com LangSmith
            # LangSmith automaticamente trackeia:
            # - Token usage
            # - Costs (se pricing configurado)
            # - Traces completos
            # - Evaluator scores
            results = evaluate(
                predict,
                data=self.config.dataset_name,
                evaluators=evaluators,
                experiment_prefix=f"benchmark-{model_name}",
                project_name=self.config.project_name,
                max_concurrency=self.config.max_concurrency if self.config.parallel else 1
            )

            # Agregar resultados de performance (via callbacks)
            perf_stats = latency_callback.get_stats()

            # Extrair custo total do LangSmith
            # LangSmith trackeia automaticamente se pricing estiver configurado
            total_cost = self._get_cost_from_langsmith(results)

            # Armazenar resultados
            self.results[model_name] = {
                # Quality metrics (do LangSmith evaluators)
                "evaluator_scores": {
                    evaluator_name: results.get(evaluator_name, {}).get("mean", 0.0)
                    for evaluator_name in self.config.evaluators
                },

                # Performance metrics (dos callbacks)
                **perf_stats,

                # Cost metrics (do LangSmith tracking)
                "total_cost_usd": total_cost,
                "cost_efficiency": (
                    results.get("qa", {}).get("mean", 0.0) / total_cost
                    if total_cost > 0 else 0.0
                ),

                # Metadata
                "experiment_url": results.experiment_url if hasattr(results, 'experiment_url') else None
            }

            print(f"‚úÖ {model_name} conclu√≠do!")
            print(f"   Avg Latency: {perf_stats['avg_latency_ms']:.0f}ms")
            print(f"   P95 Latency: {perf_stats['p95_latency_ms']:.0f}ms")
            print(f"   Total Cost: ${total_cost:.4f}")

        return self.results

    def _get_cost_from_langsmith(self, results) -> float:
        """
        Extrai custo total do LangSmith tracking.

        LangSmith automaticamente trackeia custos se pricing estiver configurado.

        Args:
            results: Resultado do evaluate()

        Returns:
            Custo total em USD
        """
        # LangSmith fornece custo automaticamente se configurado
        # Aqui voc√™ pode extrair do results ou consultar via API

        # Placeholder - substituir por extra√ß√£o real
        # do results ou query via LangSmith API
        return 0.0

    def generate_reports(self, output_formats: List[str]):
        """
        Gera relat√≥rios nos formatos especificados.

        Args:
            output_formats: ['json', 'markdown', 'html', 'csv', 'langsmith']
        """
        from benchmarking.reporters import (
            JSONReporter,
            MarkdownReporter,
            HTMLReporter,
            CSVReporter
        )

        reporters = {
            "json": JSONReporter(),
            "markdown": MarkdownReporter(),
            "html": HTMLReporter(),
            "csv": CSVReporter()
        }

        for format in output_formats:
            if format == "langsmith":
                print(f"üìä Resultados dispon√≠veis no LangSmith: {self.config.project_name}")
                continue

            reporter = reporters.get(format)
            if reporter:
                reporter.generate(self.results)
                print(f"üìÑ Relat√≥rio {format.upper()} gerado!")
```

**Exemplo: `langgraph_benchmark.py`** (Benchmark com LangGraph - Parallel)

```python
"""
LangGraph Benchmark Suite
Usa LangGraph para execu√ß√£o paralela de m√∫ltiplos LLMs.
"""

from typing import TypedDict, Annotated, Dict, List
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI

class BenchmarkState(TypedDict):
    """Estado do benchmark workflow"""
    input: str
    models: List[str]
    responses: Annotated[Dict[str, str], "Respostas de cada modelo"]
    latencies: Annotated[Dict[str, float], "Lat√™ncias por modelo"]
    costs: Annotated[Dict[str, float], "Custos por modelo"]
    metrics: Annotated[Dict[str, Dict], "M√©tricas por modelo"]
    comparison: str  # An√°lise comparativa final


def create_model_node(model_name: str, llm):
    """
    Cria node para chamar um modelo espec√≠fico.

    Args:
        model_name: Nome do modelo (ex: "gpt-4o")
        llm: Inst√¢ncia do LLM (ChatOpenAI, ChatAnthropic, etc.)

    Returns:
        Function node para o grafo
    """
    def node(state: BenchmarkState):
        import time

        # Timing
        start = time.perf_counter()

        # Invocar LLM
        response = llm.invoke(state["input"])

        # Lat√™ncia
        latency = (time.perf_counter() - start) * 1000

        # Atualizar estado
        return {
            "responses": {
                **state.get("responses", {}),
                model_name: response.content
            },
            "latencies": {
                **state.get("latencies", {}),
                model_name: latency
            }
        }

    return node


def evaluate_responses_node(state: BenchmarkState):
    """
    Node para avaliar todas as respostas usando LLM-as-judge.

    Args:
        state: Estado atual do grafo

    Returns:
        Estado atualizado com m√©tricas
    """
    from langchain_openai import ChatOpenAI

    evaluator_llm = ChatOpenAI(model="gpt-4o-mini")

    metrics = {}
    for model_name, response in state["responses"].items():
        # Avaliar qualidade usando LLM-as-judge
        eval_prompt = f"""
        Avalie a resposta abaixo em uma escala de 1-10:

        Input: {state['input']}
        Resposta: {response}

        Crit√©rios: acur√°cia, relev√¢ncia, completude, coer√™ncia

        Retorne apenas um JSON:
        {{"quality_score": N, "reasoning": "..."}}
        """

        eval_result = evaluator_llm.invoke(eval_prompt)

        # Parse score
        import json
        try:
            score_data = json.loads(eval_result.content)
            quality_score = score_data.get("quality_score", 5)
        except:
            quality_score = 5

        metrics[model_name] = {
            "quality_score": quality_score / 10.0,  # Normalizar 0-1
            "latency_ms": state["latencies"][model_name]
        }

    return {"metrics": metrics}


def create_benchmark_graph(models_config: List[Dict]):
    """
    Cria grafo LangGraph para benchmark paralelo.

    Args:
        models_config: Lista de configs [{"provider": "openai", "model": "gpt-4o"}]

    Returns:
        Compiled LangGraph
    """
    workflow = StateGraph(BenchmarkState)

    # Criar LLMs
    llms = {}
    for config in models_config:
        model_name = f"{config['provider']}:{config['model']}"

        if config["provider"] == "openai":
            llms[model_name] = ChatOpenAI(model=config["model"])
        elif config["provider"] == "anthropic":
            llms[model_name] = ChatAnthropic(model=config["model"])
        elif config["provider"] == "google":
            llms[model_name] = ChatGoogleGenerativeAI(model=config["model"])

    # Adicionar nodes para cada modelo (execu√ß√£o paralela)
    for model_name, llm in llms.items():
        workflow.add_node(
            model_name,
            create_model_node(model_name, llm)
        )

    # Node de avalia√ß√£o
    workflow.add_node("evaluate", evaluate_responses_node)

    # Entry point (start parallelization)
    first_model = list(llms.keys())[0]
    workflow.set_entry_point(first_model)

    # Conectar modelos em paralelo (todos executam concorrentemente)
    # e convergem para evaluate
    for model_name in llms.keys():
        workflow.add_edge(model_name, "evaluate")

    workflow.add_edge("evaluate", END)

    return workflow.compile()


# Uso
if __name__ == "__main__":
    models = [
        {"provider": "openai", "model": "gpt-4o"},
        {"provider": "anthropic", "model": "claude-3-5-sonnet-20241022"},
        {"provider": "google", "model": "gemini-1.5-pro"}
    ]

    graph = create_benchmark_graph(models)

    result = graph.invoke({
        "input": "Explique machine learning em 2 par√°grafos",
        "models": [f"{m['provider']}:{m['model']}" for m in models]
    })

    print("\nüìä Resultados:")
    for model, metrics in result["metrics"].items():
        print(f"\n{model}:")
        print(f"  Quality: {metrics['quality_score']:.2f}")
        print(f"  Latency: {metrics['latency_ms']:.0f}ms")
```

**Exemplo: `langsmith_evaluators.py`** (LangSmith Evaluators)

```python
"""
LangSmith Evaluators para Benchmark
Usa evaluators nativos e custom do LangSmith.
"""

from langsmith.evaluation import LangChainStringEvaluator
from langchain_openai import ChatOpenAI

def get_quality_evaluators():
    """
    Retorna evaluators de qualidade do LangSmith.

    Returns:
        Lista de evaluators prontos para usar
    """
    return [
        # Q&A correctness
        LangChainStringEvaluator("qa"),

        # Context-aware Q&A
        LangChainStringEvaluator("context_qa"),

        # Helpfulness
        LangChainStringEvaluator("criteria",
            config={"criteria": "helpfulness"}),

        # Harmfulness (safety)
        LangChainStringEvaluator("criteria",
            config={"criteria": "harmfulness"}),
    ]


def get_custom_evaluators():
    """
    Retorna custom evaluators usando LLM-as-judge.

    Returns:
        Lista de custom evaluators
    """
    return [
        # Relevance evaluator
        LangChainStringEvaluator("labeled_criteria",
            config={
                "criteria": {
                    "relevance": "Is the answer relevant to the question?"
                },
                "llm": ChatOpenAI(model="gpt-4o-mini")
            }
        ),

        # Accuracy evaluator
        LangChainStringEvaluator("labeled_criteria",
            config={
                "criteria": {
                    "accuracy": "Is the answer factually correct based on the context?"
                },
                "llm": ChatOpenAI(model="gpt-4o-mini")
            }
        ),

        # Coherence evaluator
        LangChainStringEvaluator("labeled_criteria",
            config={
                "criteria": {
                    "coherence": "Is the answer coherent and well-structured?"
                },
                "llm": ChatOpenAI(model="gpt-4o-mini")
            }
        ),
    ]


def get_all_evaluators():
    """
    Retorna todos os evaluators (nativos + custom).

    Returns:
        Lista completa de evaluators
    """
    return get_quality_evaluators() + get_custom_evaluators()
```

### Passo 4: Implementar Callbacks para M√©tricas

**Exemplo: `latency_callback.py`**

```python
"""
LangChain Callback para tracking de lat√™ncia e TTFT.
"""

from langchain_core.callbacks import BaseCallbackHandler
import time
from typing import List, Dict
import numpy as np


class LatencyTracker(BaseCallbackHandler):
    """
    Callback para medir lat√™ncia e Time to First Token.

    Usa hooks do LangChain:
    - on_llm_start: marca in√≠cio
    - on_llm_new_token: captura TTFT (primeiro token)
    - on_llm_end: calcula lat√™ncia total
    """

    def __init__(self):
        self.latencies: List[float] = []
        self.ttfts: List[float] = []
        self.current_start: float = None
        self.current_first_token: float = None

    def on_llm_start(self, serialized: Dict, prompts: List[str], **kwargs):
        """Marca in√≠cio da infer√™ncia"""
        self.current_start = time.perf_counter()
        self.current_first_token = None

    def on_llm_new_token(self, token: str, **kwargs):
        """Captura Time to First Token (streaming)"""
        if self.current_first_token is None and self.current_start:
            self.current_first_token = time.perf_counter()
            ttft = (self.current_first_token - self.current_start) * 1000
            self.ttfts.append(ttft)

    def on_llm_end(self, response, **kwargs):
        """Calcula lat√™ncia total"""
        if self.current_start:
            latency = (time.perf_counter() - self.current_start) * 1000
            self.latencies.append(latency)

    def on_llm_error(self, error: Exception, **kwargs):
        """Reset em caso de erro"""
        self.current_start = None
        self.current_first_token = None

    def get_statistics(self) -> Dict[str, float]:
        """
        Retorna estat√≠sticas de performance.

        Returns:
            Dict com m√©tricas de lat√™ncia e TTFT
        """
        if not self.latencies:
            return {
                "avg_latency_ms": 0.0,
                "p50_latency_ms": 0.0,
                "p95_latency_ms": 0.0,
                "p99_latency_ms": 0.0,
                "avg_ttft_ms": 0.0,
                "p95_ttft_ms": 0.0,
            }

        stats = {
            "avg_latency_ms": np.mean(self.latencies),
            "p50_latency_ms": np.percentile(self.latencies, 50),
            "p95_latency_ms": np.percentile(self.latencies, 95),
            "p99_latency_ms": np.percentile(self.latencies, 99),
            "min_latency_ms": np.min(self.latencies),
            "max_latency_ms": np.max(self.latencies),
        }

        if self.ttfts:
            stats.update({
                "avg_ttft_ms": np.mean(self.ttfts),
                "p50_ttft_ms": np.percentile(self.ttfts, 50),
                "p95_ttft_ms": np.percentile(self.ttfts, 95),
                "p99_ttft_ms": np.percentile(self.ttfts, 99),
            })
        else:
            stats.update({
                "avg_ttft_ms": 0.0,
                "p50_ttft_ms": 0.0,
                "p95_ttft_ms": 0.0,
                "p99_ttft_ms": 0.0,
            })

        return stats

    def reset(self):
        """Reset das m√©tricas"""
        self.latencies = []
        self.ttfts = []
        self.current_start = None
        self.current_first_token = None
```

### Passo 5: Script Principal

**`run_benchmark.py`**:

```python
"""
Script principal para executar benchmark usando LangChain/LangSmith.
"""

import asyncio
from benchmarking.langchain_benchmark import LangChainBenchmark, BenchmarkConfig

async def main():
    """Executa benchmark comparativo com LangChain/LangSmith"""

    # Configura√ß√£o
    config = BenchmarkConfig(
        models=[
            {"provider": "openai", "model": "gpt-4o", "temperature": 0},
            {"provider": "anthropic", "model": "claude-3-5-sonnet-20241022", "temperature": 0},
            {"provider": "google", "model": "gemini-1.5-pro", "temperature": 0}
        ],
        dataset_name="my-benchmark-dataset",  # Dataset no LangSmith
        evaluators=["qa", "context_qa", "helpfulness", "relevance"],
        project_name="llm-benchmark-comparison",  # Projeto LangSmith
        parallel=True,
        max_concurrency=10
    )

    # Executar benchmark
    benchmark = LangChainBenchmark(config)
    results = await benchmark.run_benchmark()

    # Gerar relat√≥rios
    benchmark.generate_reports(["json", "markdown", "html", "langsmith"])

    print("\n‚úÖ Benchmark conclu√≠do!")
    print(f"üìä Veja resultados no LangSmith: https://smith.langchain.com/o/your-org/projects/p/{config.project_name}")
    print("üìÑ Relat√≥rios gerados em results/")

if __name__ == "__main__":
    asyncio.run(main())
```

## üìä Exemplo de Output

### LangSmith UI

Todos os resultados s√£o automaticamente trackados no LangSmith:
- ‚úÖ Traces completos de cada infer√™ncia
- ‚úÖ Token usage autom√°tico
- ‚úÖ Costs autom√°ticos (se pricing configurado)
- ‚úÖ Evaluator scores
- ‚úÖ Comparison view entre modelos
- ‚úÖ Dataset annotations

### Markdown Report

```markdown
# üìä LLM Benchmark Report

## Summary
- **Models Evaluated**: 3 (gpt-4o, claude-3.5-sonnet, gemini-1.5-pro)
- **Dataset**: my-benchmark-dataset (500 examples)
- **LangSmith Project**: llm-benchmark-comparison
- **Duration**: 127 seconds
- **Total Cost**: $2.45

## üìà Comparison Tables

### Quality Metrics (LangSmith Evaluators)

| Model | QA Score | Helpfulness | Relevance | Harmfulness |
|-------|----------|-------------|-----------|-------------|
| gpt-4o | 0.872 | 0.91 | 0.89 | 0.02 |
| claude-3.5-sonnet | 0.858 | 0.89 | 0.91 | 0.01 |
| gemini-1.5-pro | 0.843 | 0.87 | 0.85 | 0.03 |

### Performance Metrics (LangChain Callbacks)

| Model | Avg Latency | P95 Latency | P99 Latency | TTFT (P95) |
|-------|-------------|-------------|-------------|------------|
| claude-3.5-sonnet | 432ms | 891ms | 1203ms | 187ms |
| gemini-1.5-pro | 521ms | 1034ms | 1421ms | 203ms |
| gpt-4o | 687ms | 1456ms | 2103ms | 312ms |

### Cost Metrics (LangSmith Tracking)

| Model | Total Cost | Cost/1K Tokens | Cost Efficiency |
|-------|------------|----------------|-----------------|
| gemini-1.5-pro | $0.43 | $0.001234 | 196.05 qa/$ |
| claude-3.5-sonnet | $0.87 | $0.002891 | 98.62 qa/$ |
| gpt-4o | $1.15 | $0.003421 | 75.83 qa/$ |

## üèÜ Winner Analysis

üéØ **Best Quality**: gpt-4o (QA Score: 0.872)
‚ö° **Fastest (P95)**: claude-3.5-sonnet (891ms)
üí∞ **Best Value**: gemini-1.5-pro (196.05 qa per $)
üõ°Ô∏è **Safest**: claude-3.5-sonnet (Harmfulness: 0.01)

## üí° Recommendations

### For Production Use
- **Best overall**: **claude-3.5-sonnet** (balanced quality, speed, safety)
- **Budget-conscious**: **gemini-1.5-pro** (best cost-efficiency)
- **Highest quality**: **gpt-4o** (best QA score)
```

## üéì Melhores Pr√°ticas

### 1. Use LangSmith para Tracking Autom√°tico

**LangSmith automaticamente trackeia**:
- Token usage por request
- Costs (se pricing configurado)
- Traces completos
- Evaluator scores
- Lat√™ncias (via UI)

**N√£o precisa calcular manualmente!**

### 2. Use LCEL Batch para Paraleliza√ß√£o

```python
# LCEL batch executa em paralelo automaticamente
results = chain.batch(
    inputs,
    config=RunnableConfig(max_concurrency=10)
)
```

### 3. Use LangSmith Evaluators Nativos

**Vantagens**:
- Implementa√ß√µes testadas
- Sem c√≥digo extra
- Tracking autom√°tico
- Consistency

### 4. Use Callbacks para M√©tricas Customizadas

**Para m√©tricas que LangSmith n√£o trackeia automaticamente**:
- TTFT (Time to First Token)
- Percentis de lat√™ncia (P95, P99)
- Custom performance metrics

### 5. Configure Pricing no LangSmith

**Para cost tracking autom√°tico**:

```python
# Via LangSmith UI ou API
client.update_pricing({
    "gpt-4o": {"input": 0.0025, "output": 0.01},
    "claude-3.5-sonnet": {"input": 0.003, "output": 0.015},
    # etc
})
```

## ‚öôÔ∏è Vari√°veis de Ambiente Necess√°rias

```bash
# API Keys dos LLMs
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
GOOGLE_API_KEY=...

# LangSmith (OBRIGAT√ìRIO para tracking)
LANGSMITH_API_KEY=lsv2_...
LANGSMITH_TRACING=true
LANGSMITH_PROJECT=llm-benchmark-comparison

# Opcional: Endpoint customizado
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
```

## üîó Integra√ß√£o com LangSmith

### Setup Inicial

```bash
# Instalar depend√™ncias
pip install langchain-openai langchain-anthropic langchain-google-genai
pip install langsmith langgraph
pip install numpy

# Configurar LangSmith
export LANGSMITH_API_KEY=lsv2_...
export LANGSMITH_TRACING=true
```

### Upload Dataset para LangSmith

```python
from langsmith import Client

client = Client()

# Upload dataset
dataset = client.create_dataset("my-benchmark-dataset")

# Adicionar exemplos
client.create_examples(
    inputs=[
        {"input": "What is the capital of France?"},
        {"input": "Explain quantum computing"},
        # ...
    ],
    outputs=[
        {"output": "Paris"},
        {"output": "..."},
        # ...
    ],
    dataset_id=dataset.id
)
```

## üö® Vantagens de Usar LangChain/LangSmith

‚úÖ **Automatic Tracking**: Custos, tokens, lat√™ncias trackados automaticamente
‚úÖ **Built-in Evaluators**: Evaluators prontos (qa, context_qa, criteria)
‚úÖ **LCEL Parallelization**: Batch processing otimizado
‚úÖ **Visualization**: UI do LangSmith para an√°lise visual
‚úÖ **Versioning**: Experiments versionados automaticamente
‚úÖ **Comparison View**: Compare modelos lado a lado no UI
‚úÖ **No Reinventar Roda**: Infraestrutura pronta de evaluation

## üìñ Refer√™ncias

### LangChain/LangSmith
- [LangSmith Evaluation Guide](https://docs.smith.langchain.com/evaluation)
- [LCEL Batch Processing](https://python.langchain.com/docs/how_to/parallel/)
- [LangChain Callbacks](https://python.langchain.com/docs/integrations/callbacks/)
- [LangGraph Parallel Workflows](https://langchain-ai.github.io/langgraph/how-tos/)

### Benchmarks
- [MMLU](https://arxiv.org/abs/2009.03300)
- [HumanEval](https://github.com/openai/human-eval)
- [BBQ Bias Benchmark](https://arxiv.org/abs/2110.08193)

---

**Criado com llm-eval-developer plugin usando LangChain/LangGraph/LangSmith** üöÄ
