# LangSmith Infrastructure for LLM-as-Judge

DocumentaÃ§Ã£o tÃ©cnica completa da infraestrutura LangSmith necessÃ¡ria para executar avaliaÃ§Ãµes offline usando LLM-as-Judge.

## ğŸ“¦ Componentes Fundamentais

Qualquer avaliaÃ§Ã£o offline no LangSmith, incluindo LLM-as-Judge, requer **trÃªs componentes fundamentais**:

1. **Dataset** - ColeÃ§Ã£o versionada de Examples
2. **Target Function** - AplicaÃ§Ã£o sob teste
3. **Evaluator(s)** - FunÃ§Ãµes de scoring (incluindo LLM-as-Judge)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          LangSmith Evaluation Pipeline          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  Dataset (Examples)                             â”‚
â”‚  â”œâ”€â”€ Example 1: {inputs, outputs}               â”‚
â”‚  â”œâ”€â”€ Example 2: {inputs, outputs}               â”‚
â”‚  â””â”€â”€ Example N: {inputs, outputs}               â”‚
â”‚           â†“                                     â”‚
â”‚  Target Function (Your App)                     â”‚
â”‚  â”œâ”€â”€ Receives: inputs                           â”‚
â”‚  â””â”€â”€ Returns: predictions                       â”‚
â”‚           â†“                                     â”‚
â”‚  Evaluators (LLM-as-Judge + others)             â”‚
â”‚  â”œâ”€â”€ Receives: inputs + outputs + predictions   â”‚
â”‚  â””â”€â”€ Returns: scores + comments                 â”‚
â”‚           â†“                                     â”‚
â”‚  Experiment Results (LangSmith UI)              â”‚
â”‚  â””â”€â”€ Heatmap, Traces, Metrics                   â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 1. Datasets - ColeÃ§Ã£o Versionada de Examples

### 1.1 O Que Ã© um Dataset LangSmith

**DefiniÃ§Ã£o**: Dataset Ã© uma coleÃ§Ã£o estÃ¡vel e versionada de **Examples** usada para avaliar aplicaÃ§Ãµes de forma reproduzÃ­vel.

**CaracterÃ­sticas:**
- âœ… Versionamento automÃ¡tico (imutabilidade)
- âœ… Armazenado no LangSmith (cloud)
- âœ… CompartilhÃ¡vel entre equipe
- âœ… RastreÃ¡vel via UI

### 1.2 Estrutura de um Example

Cada Example contÃ©m **dois componentes principais**:

```python
{
    "inputs": {
        # DicionÃ¡rio passado para Target Function
        "pergunta": "O que Ã© LangChain?",
        "contexto": "Framework Python para LLMs"
    },
    "outputs": {
        # Reference outputs (ground truth) - OPCIONAL mas CRÃTICO para LLM-as-Judge
        "resposta_esperada": "LangChain Ã© um framework Python...",
        "metadata": {"categoria": "definiÃ§Ã£o"}
    }
}
```

**Campos:**

| Campo | Tipo | ObrigatÃ³rio | Uso LLM-as-Judge |
|-------|------|-------------|------------------|
| `inputs` | dict | âœ… Sim | Passado para target function E prompt juiz |
| `outputs` | dict | âŒ NÃ£o | **âœ… CRÃTICO** - Ground truth para comparaÃ§Ã£o |

### 1.3 Reference Outputs (Ground Truth)

**âš ï¸ CRÃTICO para LLM-as-Judge**: Reference outputs sÃ£o a **resposta correta esperada** que o modelo juiz usa para avaliar a prediction.

**Sem reference outputs:**
- âŒ Judge nÃ£o tem baseline para comparaÃ§Ã£o
- âŒ AvaliaÃ§Ã£o de correÃ§Ã£o impossÃ­vel
- âŒ Apenas critÃ©rios sem ground truth (ex: concisÃ£o) funcionam

**Com reference outputs:**
- âœ… Judge compara prediction vs reference
- âœ… AvaliaÃ§Ã£o factual precisa
- âœ… CritÃ©rios como CORRECTNESS funcionam

**Exemplo comparativo:**

```python
# âŒ SEM reference - Judge nÃ£o sabe se estÃ¡ correto
{
    "inputs": {"pergunta": "Capital da FranÃ§a?"},
    "outputs": {}  # Vazio!
}

# âœ… COM reference - Judge pode avaliar correÃ§Ã£o
{
    "inputs": {"pergunta": "Capital da FranÃ§a?"},
    "outputs": {"resposta_esperada": "Paris"}
}
```

### 1.4 Versionamento de Datasets

**Por que versionar?**
- âœ… Reprodutibilidade: mesma versÃ£o = mesmos resultados
- âœ… Rastreabilidade: qual dataset gerou qual experiment
- âœ… EvoluÃ§Ã£o: adicionar examples sem quebrar histÃ³rico

**Como funciona:**
- Cada modificaÃ§Ã£o cria nova versÃ£o
- Experiments referenciam versÃ£o especÃ­fica
- UI mostra histÃ³rico de versÃµes

**Exemplo:**
```python
# VersÃ£o 1: 50 examples
dataset_v1 = client.create_dataset("qa-eval", examples=[...])

# VersÃ£o 2: 50 + 20 novos examples (automÃ¡tico)
client.create_examples(dataset_id=dataset_v1.id, examples=[...])
```

### 1.5 Criando Datasets via SDK

#### OpÃ§Ã£o 1: Criar dataset + examples juntos

```python
from langsmith import Client

client = Client()

examples = [
    {
        "inputs": {"pergunta": "O que Ã© Python?"},
        "outputs": {"resposta_esperada": "Python Ã© uma linguagem..."}
    },
    {
        "inputs": {"pergunta": "O que Ã© TypeScript?"},
        "outputs": {"resposta_esperada": "TypeScript Ã© um superset..."}
    }
]

dataset = client.create_dataset(
    dataset_name="qa-golden-dataset",
    description="Dataset curado para Q&A",
    examples=examples
)
```

#### OpÃ§Ã£o 2: Criar dataset vazio + adicionar examples depois

```python
# 1. Criar dataset
dataset = client.create_dataset(
    dataset_name="qa-golden-dataset",
    description="Dataset curado para Q&A"
)

# 2. Adicionar examples incrementalmente
client.create_examples(
    dataset_id=dataset.id,
    examples=[
        {
            "inputs": {"pergunta": "..."},
            "outputs": {"resposta_esperada": "..."}
        }
    ]
)
```

#### OpÃ§Ã£o 3: Criar dataset de production runs

```python
# Converter runs existentes em dataset
client.create_dataset_from_runs(
    dataset_name="production-dataset",
    run_ids=["run-id-1", "run-id-2", ...]
)
```

### 1.6 Golden Datasets vs Production Datasets

**Golden Datasets** (Curado):
- âœ… Examples cuidadosamente selecionados
- âœ… Reference outputs validados por humanos
- âœ… Cobertura de edge cases
- âœ… Tamanho menor (~50-200 examples)
- âœ… Uso: Quick Evals, regression testing

**Production Datasets** (Real):
- âœ… Examples de runs reais
- âœ… DistribuiÃ§Ã£o realista de inputs
- âœ… Sem reference outputs (geralmente)
- âœ… Tamanho maior (1000s+)
- âœ… Uso: A/B testing, backtesting

**Best Practice**: Usar ambos!
- Golden dataset para iteraÃ§Ã£o rÃ¡pida (< 5min)
- Production dataset para validaÃ§Ã£o final

### 1.7 EstratÃ©gias de Curation

**Abordagem 1: Manual Curation**
```python
# Criar examples Ã  mÃ£o
examples = [
    {"inputs": {...}, "outputs": {...}},
    # Curado por especialista de domÃ­nio
]
```

**Abordagem 2: Sample from Production**
```python
# Filtrar runs por critÃ©rio
runs = client.list_runs(
    project_name="production",
    filter="feedback.score > 0.8"
)

# Converter em dataset
dataset = client.create_dataset_from_runs(
    dataset_name="high-quality-subset",
    run_ids=[r.id for r in runs]
)
```

**Abordagem 3: Hybrid (Production + Human Annotation)**
```python
# 1. Capturar production runs
# 2. Humanos anotam reference outputs
# 3. Criar dataset com inputs (prod) + outputs (humanos)
```

## 2. Target Function - AplicaÃ§Ã£o Sob Teste

### 2.1 O Que Ã© Target Function

**DefiniÃ§Ã£o**: FunÃ§Ã£o Python `Callable` que representa sua aplicaÃ§Ã£o sob avaliaÃ§Ã£o.

**Contrato:**
- **Input**: DicionÃ¡rio de inputs (do dataset example)
- **Output**: DicionÃ¡rio de predictions

```python
def my_qa_app(inputs: dict) -> dict:
    """
    Target function para Q&A.

    Args:
        inputs: {"pergunta": "...", "contexto": "..."}

    Returns:
        {"resposta": "..."}
    """
    pergunta = inputs["pergunta"]
    # LÃ³gica da aplicaÃ§Ã£o (LLM call, retrieval, etc)
    resposta = call_llm(pergunta)
    return {"resposta": resposta}
```

### 2.2 Requisitos da Target Function

**ObrigatÃ³rios:**
- âœ… Aceita `dict` como input
- âœ… Retorna `dict` como output
- âœ… Ã‰ `Callable` (funÃ§Ã£o ou objeto com `__call__`)
- âœ… DeterminÃ­stica (mesmo input = mesmo output) - idealmente

**Opcionais mas recomendados:**
- âœ… Type hints para clareza
- âœ… Docstring explicando I/O
- âœ… Error handling robusto
- âœ… Logging para debugging

### 2.3 Tipos de Target Functions

#### FunÃ§Ã£o Simples
```python
def simple_app(inputs: dict) -> dict:
    return {"output": f"Processed: {inputs['input']}"}
```

#### LangChain Chain
```python
from langchain_core.runnables import RunnableLambda

chain = prompt | llm | output_parser

def chain_app(inputs: dict) -> dict:
    result = chain.invoke(inputs)
    return {"output": result}
```

#### LangGraph Graph
```python
from langgraph.graph import StateGraph

graph = StateGraph(...)

def graph_app(inputs: dict) -> dict:
    result = graph.invoke(inputs)
    return {"output": result["final_answer"]}
```

#### Classe com __call__
```python
class MyApp:
    def __init__(self, model):
        self.model = model

    def __call__(self, inputs: dict) -> dict:
        response = self.model.invoke(inputs["query"])
        return {"response": response}

app = MyApp(model=my_llm)
# app Ã© Callable!
```

### 2.4 Mapeamento: Dataset â†’ Target â†’ Judge

**Fluxo de dados:**

```python
# Example do dataset
{
    "inputs": {"pergunta": "Capital da FranÃ§a?"},
    "outputs": {"resposta_esperada": "Paris"}
}

# â†“ inputs passados para Target Function

def target(inputs: dict) -> dict:
    return {"resposta": "Paris"}  # prediction

# â†“ Judge recebe inputs + outputs + prediction

# Judge prompt (simplificado):
"""
Pergunta: {pergunta}                 # De inputs
Resposta Esperada: {resposta_esperada}  # De outputs
Resposta Gerada: {resposta}           # De prediction
"""
```

**Chaves mapeadas via create_llm_as_judge:**
```python
judge = create_llm_as_judge(
    input_keys=["pergunta"],              # inputs do dataset
    reference_output_keys=["resposta_esperada"],  # outputs do dataset
    prediction_key="resposta"             # output da target function
)
```

## 3. SDK Python - OrquestraÃ§Ã£o com langsmith.evaluate()

### 3.1 FunÃ§Ã£o Central: evaluate()

**Assinatura:**
```python
from langsmith.evaluation import evaluate

results = evaluate(
    target_function,     # Callable - AplicaÃ§Ã£o sob teste
    data,                # str (dataset name) ou Dataset object
    evaluators,          # List[Evaluator] - Incluindo LLM-as-Judge
    experiment_prefix,   # str - Nome do experiment
    max_concurrency=10   # int - ParalelizaÃ§Ã£o
)
```

### 3.2 Workflow Interno do evaluate()

```
1. Fetch dataset
   â””â”€> client.read_dataset(data)

2. For each example in dataset:
   a. Extract inputs
   b. Call target_function(inputs) â†’ prediction
   c. For each evaluator:
      - Call evaluator(inputs, outputs, prediction) â†’ score
   d. Log to LangSmith

3. Create Experiment
   â””â”€> Aggregate scores, visualizar UI

4. Return EvaluationResults
```

### 3.3 Exemplo Completo End-to-End

```python
from langsmith import Client
from langsmith.evaluation import evaluate, create_llm_as_judge

# 1. Setup client
client = Client()

# 2. Criar dataset (ou usar existente)
dataset = client.create_dataset(
    dataset_name="qa-eval",
    examples=[
        {
            "inputs": {"pergunta": "Capital da FranÃ§a?"},
            "outputs": {"resposta_esperada": "Paris"}
        },
        {
            "inputs": {"pergunta": "Maior planeta do sistema solar?"},
            "outputs": {"resposta_esperada": "JÃºpiter"}
        }
    ]
)

# 3. Definir target function
def my_qa_app(inputs: dict) -> dict:
    # SimulaÃ§Ã£o - na prÃ¡tica, chamar LLM
    return {"resposta": "Paris"}  # Hardcoded para exemplo

# 4. Criar LLM-as-Judge evaluator
judge = create_llm_as_judge(
    criteria="CORRECTNESS",
    model="openai:gpt-4o-mini",
    input_keys=["pergunta"],
    reference_output_keys=["resposta_esperada"],
    prediction_key="resposta"
)

# 5. Executar avaliaÃ§Ã£o
results = evaluate(
    target_function=my_qa_app,
    data="qa-eval",  # Nome do dataset
    evaluators=[judge],
    experiment_prefix="qa-eval-v1",
    max_concurrency=5
)

# 6. Resultados
print(f"Aggregate Score: {results.aggregate_score}")
print(f"Experiment URL: {results.experiment_url}")
```

### 3.4 ParalelizaÃ§Ã£o e Performance

**max_concurrency** controla execuÃ§Ãµes paralelas:

```python
# Slow (sequencial)
evaluate(..., max_concurrency=1)  # 1 example por vez

# Fast (paralelo)
evaluate(..., max_concurrency=10)  # 10 examples simultÃ¢neos
```

**Trade-offs:**
- âœ… Maior concurrency = mais rÃ¡pido
- âŒ Maior concurrency = mais custo (API calls simultÃ¢neas)
- âŒ Rate limits de modelos podem limitar

**RecomendaÃ§Ã£o**: max_concurrency=5-10 para balancear

### 3.5 Resultado da AvaliaÃ§Ã£o

**Objeto retornado**: `EvaluationResults`

**Campos principais:**
```python
results = evaluate(...)

# Aggregate metrics
results.aggregate_score  # float - Score mÃ©dio

# Individual results
for result in results.results:
    print(result.score)      # Score deste example
    print(result.comment)    # Justificativa do judge
    print(result.example_id)  # ID do example avaliado

# UI link
results.experiment_url  # Link direto para LangSmith UI
```

## 4. Versionamento e Reprodutibilidade

### 4.1 Por Que Versionamento Importa

**Problema sem versionamento:**
- âŒ Dataset muda, resultados nÃ£o comparÃ¡veis
- âŒ ImpossÃ­vel reproduzir experiment
- âŒ A/B testing invÃ¡lido

**SoluÃ§Ã£o com versionamento:**
- âœ… Dataset imutÃ¡vel por versÃ£o
- âœ… Experiment referencia versÃ£o especÃ­fica
- âœ… Resultados reproduzÃ­veis

### 4.2 Versionamento AutomÃ¡tico

LangSmith versiona automaticamente quando vocÃª:

1. Cria dataset inicial (versÃ£o 1)
2. Adiciona/modifica examples (versÃ£o 2, 3, ...)
3. Cada versÃ£o tem ID Ãºnico

```python
# VersÃ£o 1
dataset = client.create_dataset("my-dataset", examples=[...])

# VersÃ£o 2 (automÃ¡tico ao adicionar examples)
client.create_examples(dataset_id=dataset.id, examples=[...])
```

### 4.3 Experiment Linking

Cada experiment armazena:
- Dataset name
- Dataset version
- Timestamp
- Evaluator configs
- Results

**BenefÃ­cio**: Rastreabilidade completa

```python
# Experiment mostra no UI:
# - Dataset: qa-eval (versÃ£o 3)
# - Evaluators: CORRECTNESS judge
# - Date: 2025-01-15
# - Results: 0.85 avg score
```

## 5. Setup e ConfiguraÃ§Ã£o

### 5.1 InstalaÃ§Ã£o SDK

```bash
pip install langsmith
```

### 5.2 AutenticaÃ§Ã£o

**OpÃ§Ã£o 1: Environment variable**
```bash
export LANGSMITH_API_KEY="lsv2_pt_..."
```

**OpÃ§Ã£o 2: .env file**
```bash
# .env
LANGSMITH_API_KEY=lsv2_pt_...
```

**OpÃ§Ã£o 3: CÃ³digo**
```python
import os
os.environ["LANGSMITH_API_KEY"] = "lsv2_pt_..."
```

### 5.3 Client Configuration

```python
from langsmith import Client

# Default (usa LANGSMITH_API_KEY)
client = Client()

# Custom endpoint (para self-hosted)
client = Client(
    api_url="https://custom.langsmith.com",
    api_key="..."
)
```

### 5.4 Verificar ConexÃ£o

```python
# Listar datasets para validar conexÃ£o
datasets = list(client.list_datasets())
print(f"Conectado! {len(datasets)} datasets encontrados")
```

## 6. Casos de Uso AvanÃ§ados

### 6.1 Subset Evaluation (Quick Evals)

**Problema**: Dataset grande (1000s examples) = lento + caro

**SoluÃ§Ã£o**: Avaliar subset pequeno para iteraÃ§Ã£o rÃ¡pida

```python
# Criar subset de 20 examples do dataset original
subset_examples = client.list_examples(
    dataset_name="large-dataset",
    limit=20
)

# Criar dataset temporÃ¡rio
subset = client.create_dataset(
    dataset_name="quick-eval-subset",
    examples=[ex.inputs for ex in subset_examples]
)

# Avaliar subset rapidamente
evaluate(
    target_function=my_app,
    data="quick-eval-subset",
    evaluators=[judge]
)
```

### 6.2 Upload Results Only (No Re-execution)

**CenÃ¡rio**: VocÃª jÃ¡ tem predictions armazenadas, quer apenas avaliar

```python
# OpÃ§Ã£o: upload_results=False
results = evaluate(
    target_function=my_app,
    data="dataset",
    evaluators=[judge],
    upload_results=False  # NÃ£o envia para LangSmith (local apenas)
)
```

**Uso**: Desenvolvimento local rÃ¡pido sem custo de armazenamento

### 6.3 Incremental Dataset Building

**Pattern**: Adicionar examples gradualmente

```python
# Dia 1: 50 examples
dataset = client.create_dataset("growing-dataset", examples=[...])

# Dia 2: +20 examples
client.create_examples(dataset_id=dataset.id, examples=[...])

# Dia 3: +30 examples
client.create_examples(dataset_id=dataset.id, examples=[...])

# Total: 100 examples em versÃ£o 3
```

## 7. Troubleshooting

### Problema: Dataset nÃ£o encontrado
```python
# âŒ Erro
evaluate(data="non-existent-dataset", ...)

# âœ… SoluÃ§Ã£o: Verificar nome
datasets = client.list_datasets()
print([d.name for d in datasets])
```

### Problema: Target function nÃ£o retorna dict
```python
# âŒ Errado
def bad_target(inputs):
    return "string response"  # NÃ£o Ã© dict!

# âœ… Correto
def good_target(inputs):
    return {"response": "string response"}
```

### Problema: Chaves nÃ£o existem no example
```python
# âŒ Dataset
{"inputs": {"question": "..."}}  # "question"

# âŒ Judge config
judge = create_llm_as_judge(
    input_keys=["pergunta"]  # "pergunta" â‰  "question"!
)

# âœ… SoluÃ§Ã£o: Usar nomes exatos
judge = create_llm_as_judge(
    input_keys=["question"]
)
```

## 8. Checklist de Infraestrutura

**Antes de executar evaluate():**
- [ ] LangSmith API key configurada?
- [ ] Dataset criado e versionado?
- [ ] Examples tÃªm reference outputs (para CORRECTNESS)?
- [ ] Target function retorna dict?
- [ ] Chaves mapeadas corretamente (input_keys, reference_output_keys, prediction_key)?
- [ ] Evaluator (LLM-as-Judge) criado?
- [ ] ConexÃ£o com LangSmith validada?

**Para Quick Evals:**
- [ ] Subset dataset criado (< 50 examples)?
- [ ] upload_results=False configurado (opcional)?

**Para Reprodutibilidade:**
- [ ] Dataset versionado?
- [ ] Experiment prefix descritivo?
- [ ] Timestamp registrado?
