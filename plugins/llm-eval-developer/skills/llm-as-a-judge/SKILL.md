---
name: llm-as-a-judge
description: LLM-as-a-Judge evaluation with LangSmith - datasets, judge prompts, criteria, and LLMOps workflow. Use when implementing LLM evaluators, designing judge prompts, creating evaluation datasets, running offline evaluations, or measuring subjective quality metrics (correctness, relevance, coherence). Essential for LangSmith evaluate() workflows.
version: 1.0.0
allowed-tools:
  - Read
  - Grep
  - Bash
---

# LLM-as-a-Judge Evaluation with LangSmith

Conhecimento especializado para implementar avalia√ß√µes usando LLM como juiz (LLM-as-Judge) no ecossistema LangSmith, cobrindo infraestrutura de datasets, engenharia de prompts para ju√≠zes e integra√ß√£o com ciclo de vida LLMOps.

## üìã When to Use Me

Invoque esta skill automaticamente quando:

- **Implementar LLM-as-Judge evaluators** no LangSmith
- **Criar ou configurar datasets** de avalia√ß√£o com reference outputs
- **Desenhar prompts para modelos ju√≠zes** (crit√©rios, scoring, justificativas)
- **Mapear chaves de dados** (input_keys, reference_output_keys, prediction_key)
- **Executar avalia√ß√µes offline** usando `langsmith.evaluate()`
- **Avaliar crit√©rios subjetivos** (corre√ß√£o, relev√¢ncia, concis√£o, coer√™ncia)
- **Debugar avalia√ß√µes** usando LangSmith UI (heatmaps, traces)
- **Combinar m√©tricas qualitativas + quantitativas** (score + lat√™ncia + custo)
- **Configurar avalia√ß√£o cont√≠nua** em produ√ß√£o (online evaluators)

**Gatilhos espec√≠ficos:**

- "LLM as judge"
- "create_llm_as_judge"
- "evaluation criteria"
- "judge prompt"
- "offline evaluation LangSmith"
- "dataset reference outputs"

## üéì Core Knowledge

### 1. O Que √© LLM-as-a-Judge

**Defini√ß√£o**: T√©cnica de avalia√ß√£o h√≠brida onde um LLM atua como avaliador para crit√©rios **subjetivos** que n√£o podem ser capturados por regras determin√≠sticas.

**Quando usar LLM-as-Judge:**

- ‚úÖ Crit√©rios subjetivos: relev√¢ncia, coer√™ncia, tom, helpfulness
- ‚úÖ Avalia√ß√£o de qualidade factual (com ground truth)
- ‚úÖ Compara√ß√µes pairwise (qual resposta √© melhor?)
- ‚úÖ Safety e harmfulness detection

**Quando N√ÉO usar:**

- ‚ùå M√©tricas objetivas simples (lat√™ncia, custo, token count)
- ‚ùå Valida√ß√£o de formato estruturado (JSON schema)
- ‚ùå M√©tricas determin√≠sticas (exact match, regex)

### 2. Tr√™s Pilares Fundamentais

#### Pilar 1: Infraestrutura LangSmith

- **Datasets**: Cole√ß√£o versionada de Examples (inputs + reference outputs)
- **Target Function**: Aplica√ß√£o sob teste (Callable Python)
- **SDK Python**: `langsmith.evaluate()` orquestra dataset + target + evaluators

#### Pilar 2: Engenharia de Prompt Juiz

- **Crit√©rios de avalia√ß√£o**: CORRECTNESS, RELEVANCE, CONCISENESS, HARMFULNESS
- **Prompt estruturado**: Papel do juiz + dados de entrada + formato de sa√≠da (JSON)
- **Mapeamento de chaves**: input_keys, reference_output_keys, prediction_key
- **Modelo juiz**: Sele√ß√£o do LLM (ex: `openai:gpt-4o-mini`)

#### Pilar 3: LLMOps Workflow

- **Avalia√ß√£o offline**: Final de sprint/itera√ß√£o
- **Experiments**: Compara√ß√µes A/B de prompts/modelos
- **Debugging**: Heatmap + Traces para diagnosticar scores baixos
- **M√©tricas combinadas**: Score qualitativo + lat√™ncia + custo
- **Avalia√ß√£o online**: Monitoramento cont√≠nuo em produ√ß√£o

### 3. Estrutura de um Dataset LangSmith

```python
# Example structure
{
    "inputs": {"pergunta": "O que √© LangChain?"},
    "outputs": {"resposta_esperada": "LangChain √© um framework..."}  # Reference (ground truth)
}
```

**Caracter√≠sticas cr√≠ticas:**

- ‚úÖ Versionamento autom√°tico pelo LangSmith
- ‚úÖ Reference outputs s√£o cruciais para LLM-as-Judge
- ‚úÖ Inputs mapeados para aplica√ß√£o alvo
- ‚úÖ Outputs comparados pelo modelo juiz

### 4. Fluxo de Avalia√ß√£o Offline

```
1. Dataset (inputs + references)
   ‚Üì
2. Target Function (sua aplica√ß√£o)
   ‚Üì
3. Predictions (outputs gerados)
   ‚Üì
4. LLM Judge (compara prediction vs reference)
   ‚Üì
5. Score + Justificativa
   ‚Üì
6. Experiment Results (LangSmith UI)
```

### 5. Crit√©rios Pr√©-Constru√≠dos do LangSmith

| Crit√©rio | O Que Avalia | Exemplo de Uso |
|----------|--------------|----------------|
| `CORRECTNESS` | Precis√£o factual | Verificar se resposta est√° correta |
| `RELEVANCE` | Alinhamento com pergunta | Avaliar se resposta √© relevante |
| `CONCISENESS` | Brevidade e objetividade | Medir se resposta √© concisa |
| `HARMFULNESS` | Conte√∫do prejudicial | Testes de safety/guardrails |

### 6. Fun√ß√£o create_llm_as_judge

**Fun√ß√£o principal**: `create_llm_as_judge()` do SDK LangSmith

**Par√¢metros cr√≠ticos:**

- `criteria`: Crit√©rio de avalia√ß√£o (ex: "CORRECTNESS")
- `model`: Modelo juiz (ex: "openai:gpt-4o-mini")
- `input_keys`: Chaves do dataset inputs (ex: ["pergunta"])
- `reference_output_keys`: Chaves do ground truth (ex: ["resposta_esperada"])
- `prediction_key`: Chave da sa√≠da gerada (ex: "resposta")
- `prompt`: Template do prompt juiz (opcional, usa default se n√£o fornecido)

**Exemplo b√°sico:**

```python
from langsmith.evaluation import create_llm_as_judge

judge = create_llm_as_judge(
    criteria="CORRECTNESS",
    model="openai:gpt-4o-mini",
    input_keys=["pergunta"],
    reference_output_keys=["resposta_esperada"],
    prediction_key="resposta"
)
```

### 7. Anatomia de um Prompt Juiz Eficaz

**3 Componentes Obrigat√≥rios:**

1. **Papel do Juiz**: "Voc√™ √© um avaliador especialista"
1. **Dados de Entrada**: Quais elementos considerar
   - Input original (pergunta)
   - Reference output (ground truth)
   - Prediction (resposta gerada)
1. **Formato de Sa√≠da**: JSON estruturado
   ```json
   {
     "score": 1,  // 1 = correto, 0 = incorreto
     "comment": "Justificativa detalhada"
   }
   ```

**Padr√µes avan√ßados:**

- ‚úÖ Few-shot examples para calibrar juiz
- ‚úÖ Chain-of-thought para racioc√≠nio expl√≠cito
- ‚úÖ Rubrica detalhada (escala 1-5 com descri√ß√µes)
- ‚úÖ Bias correction via human feedback

### 8. Mapeamento de Chaves (Critical!)

**Problema**: Dataset tem chaves diferentes da sua aplica√ß√£o

**Solu√ß√£o**: Mapear explicitamente no `create_llm_as_judge`

**Exemplo:**

```python
# Dataset Example
{
    "inputs": {"question": "What is AI?"},
    "outputs": {"expected_answer": "AI is..."}
}

# Target Function Output
{
    "answer": "AI stands for..."
}

# Mapeamento correto
judge = create_llm_as_judge(
    input_keys=["question"],              # Do dataset inputs
    reference_output_keys=["expected_answer"],  # Do dataset outputs
    prediction_key="answer"               # Da sua aplica√ß√£o
)
```

### 9. Execu√ß√£o com langsmith.evaluate()

**Fun√ß√£o orquestradora**: `langsmith.evaluate()`

**Workflow:**

1. Busca dataset do LangSmith
1. Executa target function para cada example
1. Aplica evaluators (incluindo LLM-as-Judge)
1. Registra resultados em Experiment

**Exemplo:**

```python
from langsmith import Client
from langsmith.evaluation import evaluate

client = Client()

results = evaluate(
    target_function=my_app,
    data="my-dataset-name",
    evaluators=[judge],
    experiment_prefix="eval-v1"
)
```

### 10. M√©tricas Combinadas (ROI Decision)

**Best Practice**: Combinar qualidade + performance + custo

**F√≥rmula exemplo:**

```python
weighted_score = (
    0.6 * quality_score +    # LLM-as-Judge
    0.3 * (1 - latency_normalized) +
    0.1 * (1 - cost_normalized)
)
```

**Uso**: Decis√µes A/B informadas (n√£o apenas qualidade!)

## üìö Reference Files

Para conhecimento detalhado, consulte:

- **INFRASTRUCTURE.md** - LangSmith datasets, target functions, SDK Python, versionamento
- **PROMPT_ENGINEERING.md** - Design de prompts juiz, crit√©rios, few-shot, bias correction
- **LLMOPS.md** - Ciclo de vida completo, Experiments, debugging, avalia√ß√£o online

## üí° Quick Examples

### Exemplo 1: Avaliador Simples de Corre√ß√£o

```python
from langsmith.evaluation import create_llm_as_judge, evaluate

# 1. Criar judge para corre√ß√£o factual
correctness_judge = create_llm_as_judge(
    criteria="CORRECTNESS",
    model="openai:gpt-4o-mini",
    input_keys=["pergunta"],
    reference_output_keys=["resposta_esperada"],
    prediction_key="resposta"
)

# 2. Executar avalia√ß√£o
results = evaluate(
    target_function=my_qa_app,
    data="qa-golden-dataset",
    evaluators=[correctness_judge],
    experiment_prefix="qa-eval"
)

# 3. Ver resultados no LangSmith UI
# Navigate to Experiments ‚Üí qa-eval ‚Üí Heatmap
```

### Exemplo 2: Judge Customizado com Prompt

```python
custom_prompt = """
Voc√™ √© um avaliador especialista em respostas de chatbots.

Entrada:
- Pergunta: {pergunta}
- Resposta Esperada: {resposta_esperada}
- Resposta Gerada: {resposta}

Avalie se a Resposta Gerada est√° factualmente correta comparada √† Resposta Esperada.

Retorne JSON:
{{
  "score": 1 se correta, 0 se incorreta,
  "comment": "Justificativa detalhada"
}}
"""

judge = create_llm_as_judge(
    criteria="custom",
    model="openai:gpt-4o-mini",
    input_keys=["pergunta"],
    reference_output_keys=["resposta_esperada"],
    prediction_key="resposta",
    prompt=custom_prompt
)
```

### Exemplo 3: M√∫ltiplos Avaliadores

```python
# Avaliar corre√ß√£o + relev√¢ncia + concis√£o
judges = [
    create_llm_as_judge(criteria="CORRECTNESS", ...),
    create_llm_as_judge(criteria="RELEVANCE", ...),
    create_llm_as_judge(criteria="CONCISENESS", ...)
]

results = evaluate(
    target_function=my_app,
    data="dataset",
    evaluators=judges  # Lista de evaluators
)
```

## ‚úÖ Checklist R√°pido

**Antes de criar LLM-as-Judge:**

- [ ] Dataset tem reference outputs (ground truth)?
- [ ] Crit√©rio √© subjetivo (n√£o pode ser regex/exato)?
- [ ] Modelo juiz selecionado (ex: gpt-4o-mini)?
- [ ] Chaves mapeadas corretamente (input_keys, reference_output_keys, prediction_key)?
- [ ] Prompt juiz define formato de sa√≠da (JSON com score + comment)?

**Ao executar avalia√ß√£o:**

- [ ] Target function retorna dicion√°rio com prediction_key?
- [ ] Dataset versionado e est√°vel?
- [ ] `evaluate()` configurado com experiment_prefix?
- [ ] LangSmith API key configurada?

**Ao analisar resultados:**

- [ ] Heatmap mostra distribui√ß√£o de scores?
- [ ] Traces revelam por que score baixo?
- [ ] M√©tricas combinadas (score + lat√™ncia + custo)?
- [ ] Compara√ß√£o A/B entre experiments?

## üéØ Regras de Ouro

1. **Reference outputs s√£o cruciais** - LLM-as-Judge precisa de ground truth
1. **Prompt juiz √© cr√≠tico** - Invista tempo em engenharia de prompt
1. **Combine m√©tricas** - N√£o avalie apenas qualidade (lat√™ncia + custo importam)
1. **Itere com human feedback** - Alinhe juiz com prefer√™ncias humanas
1. **Use few-shot examples** - Calibre comportamento do juiz
1. **Versione datasets** - Reprodutibilidade √© essencial
1. **Debug com Traces** - LangSmith UI mostra reasoning do juiz

## üîç Troubleshooting Comum

**Problema**: Judge sempre d√° score m√°ximo (bias positivo)

- ‚úÖ Adicionar few-shot examples com casos negativos
- ‚úÖ Revisar prompt para ser mais cr√≠tico
- ‚úÖ Usar human corrections no LangSmith

**Problema**: Chaves n√£o mapeiam corretamente

- ‚úÖ Verificar nomes exatos em dataset examples
- ‚úÖ Validar output da target function
- ‚úÖ Usar `print()` para debug durante desenvolvimento

**Problema**: Avalia√ß√£o muito lenta

- ‚úÖ Usar modelo juiz mais r√°pido (gpt-4o-mini vs gpt-4)
- ‚úÖ Reduzir tamanho do dataset para itera√ß√£o r√°pida
- ‚úÖ Executar em paralelo (LangSmith faz automaticamente)

**Problema**: Scores inconsistentes

- ‚úÖ Definir rubrica clara no prompt
- ‚úÖ Usar chain-of-thought para reasoning expl√≠cito
- ‚úÖ Calibrar com human feedback

## üöÄ Next Steps

Ap√≥s dominar conceitos b√°sicos:

1. Consulte **PROMPT_ENGINEERING.md** para padr√µes avan√ßados de prompts
1. Veja **INFRASTRUCTURE.md** para setup completo de datasets
1. Leia **LLMOPS.md** para integra√ß√£o em ciclo de desenvolvimento
