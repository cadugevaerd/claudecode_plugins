# Prompt Engineering for LLM-as-Judge

Guia completo de engenharia de prompts para modelos ju√≠zes no LangSmith, cobrindo design de crit√©rios, estrutura de prompts, calibra√ß√£o via few-shot, e corre√ß√£o de bias.

## üéØ Anatomia de um Prompt Juiz Eficaz

Todo prompt LLM-as-Judge deve conter **3 componentes obrigat√≥rios**:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Prompt Anatomy (3 Parts)         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                        ‚îÇ
‚îÇ  1. ROLE: Quem √© o juiz?               ‚îÇ
‚îÇ     "Voc√™ √© um avaliador especialista" ‚îÇ
‚îÇ                                        ‚îÇ
‚îÇ  2. DATA: Quais dados considerar?      ‚îÇ
‚îÇ     - Input (pergunta, contexto)       ‚îÇ
‚îÇ     - Reference (resposta esperada)    ‚îÇ
‚îÇ     - Prediction (resposta gerada)     ‚îÇ
‚îÇ                                        ‚îÇ
‚îÇ  3. OUTPUT FORMAT: Como responder?     ‚îÇ
‚îÇ     JSON: {score: X, comment: "..."}   ‚îÇ
‚îÇ                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Template Base Recomendado

```python
base_prompt = """
# 1. ROLE
Voc√™ √© um avaliador especialista em {dom√≠nio}.

# 2. DATA
Analise os seguintes dados:
- Input: {input_field}
- Reference Output: {reference_field}
- Prediction: {prediction_field}

# 3. OUTPUT FORMAT
Avalie se a Prediction est√° {crit√©rio}.

Retorne APENAS um JSON v√°lido:
{{
  "score": 1 (se {crit√©rio}) ou 0 (se n√£o),
  "comment": "Justificativa detalhada em 1-2 frases"
}}
"""
```

## 1. Definindo Crit√©rios de Avalia√ß√£o

### 1.1 Crit√©rios Pr√©-Constru√≠dos do LangSmith

LangSmith oferece crit√©rios prontos via constantes:

| Crit√©rio | Constante | O Que Avalia | Requer Reference? |
|----------|-----------|--------------|-------------------|
| Corre√ß√£o | `CORRECTNESS` | Precis√£o factual | ‚úÖ Sim |
| Relev√¢ncia | `RELEVANCE` | Alinhamento com pergunta | ‚ùå N√£o |
| Concis√£o | `CONCISENESS` | Brevidade | ‚ùå N√£o |
| Harmfulness | `HARMFULNESS` | Conte√∫do prejudicial | ‚ùå N√£o |
| Helpfulness | `HELPFULNESS` | Utilidade da resposta | ‚ùå N√£o |

**Uso:**
```python
from langsmith.evaluation import create_llm_as_judge

judge = create_llm_as_judge(
    criteria="CORRECTNESS",  # Usa prompt pr√©-constru√≠do
    ...
)
```

### 1.2 Crit√©rios Customizados

**Quando customizar:**
- ‚úÖ Dom√≠nio espec√≠fico (m√©dico, legal, t√©cnico)
- ‚úÖ Rubrica complexa (escala 1-5 com descri√ß√µes)
- ‚úÖ M√∫ltiplas dimens√µes (tom + precis√£o + estrutura)

**Como customizar:**
```python
custom_criteria = """
Avalie se a resposta √© tecnicamente precisa E apropriada para p√∫blico leigo.

Crit√©rios:
1. Corre√ß√£o t√©cnica (sem erros factuais)
2. Linguagem acess√≠vel (sem jarg√£o excessivo)
3. Exemplos pr√°ticos (quando relevante)

Score:
- 1: Atende todos os crit√©rios
- 0: Falha em qualquer crit√©rio
"""

judge = create_llm_as_judge(
    criteria=custom_criteria,  # String customizada
    ...
)
```

### 1.3 Rubrica de Scoring

**Escalas comuns:**

**Binary (0/1):**
```
score: 1 se correto, 0 se incorreto
```

**Likert (1-5):**
```
score:
  5 - Excelente (completo e preciso)
  4 - Bom (pequenas omiss√µes)
  3 - Aceit√°vel (parcialmente correto)
  2 - Insatisfat√≥rio (erros significativos)
  1 - Ruim (completamente incorreto)
```

**Percentage (0-100):**
```
score: 0-100 (% de informa√ß√£o correta)
```

**Recomenda√ß√£o**: Binary (0/1) para itera√ß√£o r√°pida, Likert (1-5) para nuance.

## 2. Estrutura de Prompts por N√≠vel

### 2.1 N√≠vel 1: Prompt B√°sico (Zero-Shot)

**Caracter√≠sticas:**
- Sem exemplos
- Instru√ß√£o direta
- Formato de sa√≠da claro

```python
basic_prompt = """
Voc√™ √© um avaliador de respostas de Q&A.

Dados:
- Pergunta: {pergunta}
- Resposta Esperada: {resposta_esperada}
- Resposta Gerada: {resposta}

Avalie se a Resposta Gerada est√° factualmente correta comparada √† Resposta Esperada.

Retorne JSON:
{{
  "score": 1 se correta, 0 se incorreta,
  "comment": "Justificativa"
}}
"""
```

**Quando usar:**
- ‚úÖ Crit√©rios simples (corre√ß√£o bin√°ria)
- ‚úÖ Dom√≠nio geral (n√£o especializado)
- ‚úÖ Itera√ß√£o r√°pida (sem calibra√ß√£o)

### 2.2 N√≠vel 2: Prompt com Rubrica Detalhada

**Caracter√≠sticas:**
- Rubrica expl√≠cita
- M√∫ltiplos aspectos
- Escala clara

```python
rubric_prompt = """
Voc√™ √© um avaliador especialista em respostas t√©cnicas.

Dados:
- Pergunta: {pergunta}
- Resposta Esperada: {resposta_esperada}
- Resposta Gerada: {resposta}

Rubrica de Avalia√ß√£o:
1. Corre√ß√£o Factual (50%)
   - Todos os fatos corretos? (+1)
   - Algum erro factual? (-1)

2. Completude (30%)
   - Cobre todos os pontos da refer√™ncia? (+0.6)
   - Omite informa√ß√µes cr√≠ticas? (-0.6)

3. Clareza (20%)
   - Explica√ß√£o clara e organizada? (+0.4)
   - Confusa ou amb√≠gua? (-0.4)

Score Final: Soma dos componentes (0-2 normalizado para 0-1)

Retorne JSON:
{{
  "score": 0.0-1.0,
  "comment": "Breakdown: Corre√ß√£o=X, Completude=Y, Clareza=Z"
}}
"""
```

**Quando usar:**
- ‚úÖ Avalia√ß√£o multidimensional
- ‚úÖ Feedback detalhado necess√°rio
- ‚úÖ Dom√≠nio com m√∫ltiplos aspectos

### 2.3 N√≠vel 3: Prompt com Chain-of-Thought (CoT)

**Caracter√≠sticas:**
- Racioc√≠nio expl√≠cito
- Passo a passo
- Conclus√£o justificada

```python
cot_prompt = """
Voc√™ √© um avaliador especialista. Siga um processo de racioc√≠nio passo a passo.

Dados:
- Pergunta: {pergunta}
- Resposta Esperada: {resposta_esperada}
- Resposta Gerada: {resposta}

Processo de Avalia√ß√£o:

1. AN√ÅLISE FACTUAL
   - Liste os fatos na Resposta Esperada
   - Liste os fatos na Resposta Gerada
   - Compare: quais est√£o corretos, incorretos ou omitidos?

2. AVALIA√á√ÉO DE COMPLETUDE
   - A resposta gerada cobre todos os pontos essenciais?
   - H√° informa√ß√µes extras (b√¥nus) ou irrelevantes (ru√≠do)?

3. DECIS√ÉO FINAL
   - Com base na an√°lise acima, a resposta √© correta?
   - Justificativa: por que sim ou n√£o?

Retorne JSON:
{{
  "reasoning": "Passo 1: ... Passo 2: ... Passo 3: ...",
  "score": 1 ou 0,
  "comment": "Decis√£o final e justificativa"
}}
"""
```

**Quando usar:**
- ‚úÖ Alta precis√£o necess√°ria
- ‚úÖ Debugging de decis√µes do juiz
- ‚úÖ Dom√≠nio complexo (m√©dico, legal)

**‚ö†Ô∏è Trade-off**: Mais tokens (mais caro + mais lento)

### 2.4 N√≠vel 4: Prompt com Few-Shot Examples

**Caracter√≠sticas:**
- Exemplos de julgamentos corretos
- Calibra√ß√£o autom√°tica
- Alinhamento com prefer√™ncias

```python
few_shot_prompt = """
Voc√™ √© um avaliador de respostas t√©cnicas.

# EXEMPLOS DE JULGAMENTO:

Exemplo 1:
Pergunta: "O que √© Python?"
Esperada: "Python √© uma linguagem de programa√ß√£o interpretada"
Gerada: "Python √© uma linguagem interpretada criada por Guido van Rossum"
Score: 1 (Correta - inclui fato essencial + informa√ß√£o extra v√°lida)

Exemplo 2:
Pergunta: "O que √© Python?"
Esperada: "Python √© uma linguagem de programa√ß√£o interpretada"
Gerada: "Python √© uma cobra venenosa"
Score: 0 (Incorreta - contexto errado)

Exemplo 3:
Pergunta: "O que √© Python?"
Esperada: "Python √© uma linguagem de programa√ß√£o interpretada"
Gerada: "Python √© uma linguagem compilada"
Score: 0 (Incorreta - erro factual)

# AGORA AVALIE:

Pergunta: {pergunta}
Esperada: {resposta_esperada}
Gerada: {resposta}

Retorne JSON:
{{
  "score": 1 ou 0,
  "comment": "Justificativa baseada nos exemplos"
}}
"""
```

**Quando usar:**
- ‚úÖ Calibra√ß√£o necess√°ria
- ‚úÖ Crit√©rios subjetivos
- ‚úÖ Alinhamento com humanos

**Best Practice**: 3-5 exemplos (n√£o mais!)

## 3. Mapeamento de Chaves de Dados

### 3.1 O Problema do Mapeamento

**Cen√°rio comum:**
- Dataset usa chaves: `{"question", "answer"}`
- Target function retorna: `{"response"}`
- Prompt usa vari√°veis: `{pergunta}`, `{resposta}`

**Solu√ß√£o**: Mapear explicitamente via `create_llm_as_judge`

### 3.2 Par√¢metros de Mapeamento

```python
judge = create_llm_as_judge(
    criteria="CORRECTNESS",
    model="openai:gpt-4o-mini",

    # Mapeamento cr√≠tico:
    input_keys=["question"],           # Do dataset.inputs
    reference_output_keys=["answer"],  # Do dataset.outputs
    prediction_key="response"          # Da target function output
)
```

**Vari√°veis no prompt:**
- `{question}` ‚Üê dataset.inputs["question"]
- `{answer}` ‚Üê dataset.outputs["answer"]
- `{response}` ‚Üê target_output["response"]

### 3.3 Exemplo Completo de Mapeamento

```python
# Dataset Example
{
    "inputs": {
        "user_query": "Capital da Fran√ßa?",
        "context": "Geografia europeia"
    },
    "outputs": {
        "expected_response": "Paris"
    }
}

# Target Function
def my_app(inputs: dict) -> dict:
    return {"generated_answer": "Paris"}

# Prompt Template
custom_prompt = """
Query: {user_query}
Context: {context}
Expected: {expected_response}
Generated: {generated_answer}

Avalie corre√ß√£o.
"""

# Mapeamento
judge = create_llm_as_judge(
    prompt=custom_prompt,
    input_keys=["user_query", "context"],      # 2 inputs!
    reference_output_keys=["expected_response"],
    prediction_key="generated_answer"
)
```

### 3.4 Debugging Mapeamento

**Problema**: KeyError ou vari√°veis vazias no prompt

**Solu√ß√£o**: Validar chaves antes de criar judge

```python
# 1. Inspecionar dataset example
example = client.read_example(example_id="...")
print("Inputs keys:", example.inputs.keys())
print("Outputs keys:", example.outputs.keys())

# 2. Testar target function
test_output = my_app(example.inputs)
print("Prediction keys:", test_output.keys())

# 3. Confirmar mapeamento
# input_keys deve estar em example.inputs.keys()
# reference_output_keys deve estar em example.outputs.keys()
# prediction_key deve estar em test_output.keys()
```

## 4. Sele√ß√£o do Modelo Juiz

### 4.1 Modelos Dispon√≠veis

**OpenAI:**
- `openai:gpt-4o` - Mais preciso, mais lento, mais caro
- `openai:gpt-4o-mini` - Balanceado (recomendado)
- `openai:gpt-3.5-turbo` - Mais r√°pido, menos preciso

**Anthropic:**
- `anthropic:claude-3-5-sonnet-20250219` - Alta qualidade
- `anthropic:claude-3-haiku-20240307` - R√°pido e econ√¥mico

**Outros:**
- `fireworks_ai:accounts/...` - Modelos open-source

### 4.2 Trade-offs de Modelo

| Modelo | Precis√£o | Velocidade | Custo | Caso de Uso |
|--------|----------|------------|-------|-------------|
| GPT-4o | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | üí∞üí∞üí∞ | Avalia√ß√£o final, alta precis√£o |
| GPT-4o-mini | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | üí∞üí∞ | Itera√ß√£o r√°pida, balanceado |
| GPT-3.5-turbo | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | üí∞ | Quick evals, screening |

**Recomenda√ß√£o**: Iniciar com `gpt-4o-mini`, escalar para `gpt-4o` em produ√ß√£o.

### 4.3 Configura√ß√£o do Modelo

```python
judge = create_llm_as_judge(
    criteria="CORRECTNESS",
    model="openai:gpt-4o-mini",  # Formato: provider:model
    # Par√¢metros opcionais:
    # temperature=0.0,  # Determin√≠stico
    # max_tokens=500    # Limite de resposta
)
```

## 5. Calibra√ß√£o via Few-Shot Examples

### 5.1 O Que √© Calibra√ß√£o

**Problema**: Judge "out-of-the-box" pode n√£o alinhar com suas prefer√™ncias

**Solu√ß√£o**: Few-shot examples que mostram julgamentos desejados

### 5.2 LangSmith Human Corrections

**Workflow automatizado:**

1. **Executar avalia√ß√£o inicial**
   ```python
   results = evaluate(data="dataset", evaluators=[judge])
   ```

2. **Humanos corrigem scores no LangSmith UI**
   - Navegar para Experiment
   - Clicar em example
   - Modificar score (ex: 0 ‚Üí 1)
   - Adicionar feedback

3. **LangSmith auto-insere corre√ß√µes como few-shot**
   - Corre√ß√µes viram exemplos no prompt
   - Judge aprende com feedback humano

**Benef√≠cio**: Calibra√ß√£o autom√°tica sem reescrever prompts!

### 5.3 Few-Shot Manual (via Prompt)

**Alternativa**: Incluir examples diretamente no prompt

```python
few_shot_template = """
Voc√™ √© avaliador de Q&A t√©cnico.

# EXEMPLOS DE CALIBRA√á√ÉO:

[Exemplo 1]
Q: "O que √© Docker?"
Esperada: "Docker √© plataforma de containeriza√ß√£o"
Gerada: "Docker √© uma ferramenta de virtualiza√ß√£o"
Score: 0.5 (Parcialmente correta - conceito relacionado mas n√£o exato)
Feedback: "Virtualiza√ß√£o ‚â† Containeriza√ß√£o"

[Exemplo 2]
Q: "O que √© Kubernetes?"
Esperada: "Kubernetes orquestra containers"
Gerada: "Kubernetes gerencia Docker containers em cluster"
Score: 1 (Correta - adiciona detalhes v√°lidos)
Feedback: "Inclui informa√ß√£o extra relevante"

# AGORA AVALIE:
Q: {pergunta}
Esperada: {resposta_esperada}
Gerada: {resposta}

Retorne JSON: {{"score": ..., "comment": ...}}
"""
```

**Quantos exemplos?**
- ‚úÖ 3-5 exemplos = Sweet spot
- ‚ùå < 3 = Pouco contexto
- ‚ùå > 10 = Token waste + confusion

### 5.4 Estrat√©gias de Sele√ß√£o de Few-Shot

**Op√ß√£o 1: Edge Cases**
- Casos dif√≠ceis onde judge errou
- Exemplos amb√≠guos

**Op√ß√£o 2: Distribui√ß√£o Balanceada**
- 50% positivos (score=1)
- 50% negativos (score=0)

**Op√ß√£o 3: Coverage de Tipos de Erro**
- Erro factual
- Omiss√£o de informa√ß√£o
- Informa√ß√£o extra irrelevante
- Formato incorreto

## 6. Corre√ß√£o de Bias

### 6.1 Tipos Comuns de Bias

**1. Positivity Bias**
- Judge tende a dar scores altos
- Problema: N√£o detecta erros sutis

**2. Length Bias**
- Respostas longas recebem scores melhores
- Problema: Verbosidade ‚â† Qualidade

**3. Format Bias**
- Respostas bem formatadas (markdown, listas) pontuam alto
- Problema: Formato ‚â† Corre√ß√£o

**4. Echo Chamber**
- LLM avalia outro LLM favoravelmente
- Problema: Bias sist√™mico

### 6.2 Detectando Bias

**An√°lise estat√≠stica:**
```python
# Calcular distribui√ß√£o de scores
scores = [r.score for r in results.results]
mean_score = sum(scores) / len(scores)

# Red flags:
# - mean_score > 0.9 (positivity bias?)
# - mean_score < 0.1 (negativity bias?)
# - std_dev < 0.1 (todos scores iguais - n√£o discrimina)
```

**Compara√ß√£o com human judgments:**
```python
# Correla√ß√£o judge vs human
# Se correlation < 0.7 ‚Üí bias ou crit√©rios mal definidos
```

### 6.3 T√©cnicas de Corre√ß√£o

**T√©cnica 1: Prompt Engineering**
```python
# Adicionar instru√ß√£o expl√≠cita
bias_aware_prompt = """
IMPORTANTE: Seja cr√≠tico. Penalize erros sutis.
N√£o favore√ßa respostas longas ou bem formatadas automaticamente.
Foque apenas em corre√ß√£o factual.

{resto do prompt}
"""
```

**T√©cnica 2: Few-Shot com Casos Negativos**
```python
# Incluir exemplos de respostas longas mas incorretas
negative_example = """
Gerada: "Python √© uma linguagem compilada criada em 1991 por Guido van Rossum..."
(200 palavras de texto bem formatado)
Score: 0 (Erro factual: Python √© INTERPRETADA, n√£o compilada)
"""
```

**T√©cnica 3: Human Corrections (LangSmith)**
- Corrigir false positives no UI
- LangSmith usa corre√ß√µes como few-shot
- Bias reduz gradualmente

**T√©cnica 4: Dual Judging**
```python
# Usar 2 judges independentes
judge_1 = create_llm_as_judge(model="openai:gpt-4o-mini", ...)
judge_2 = create_llm_as_judge(model="anthropic:claude-3-5-sonnet", ...)

# Average ou majority vote
final_score = (score_1 + score_2) / 2
```

### 6.4 Valida√ß√£o de Calibra√ß√£o

**Checklist:**
- [ ] Distribui√ß√£o de scores razo√°vel (n√£o s√≥ 0 ou 1)?
- [ ] Correla√ß√£o com human judgments > 0.7?
- [ ] Respostas incorretas √≥bvias recebem score 0?
- [ ] Respostas longas mas erradas n√£o pontuam alto?
- [ ] Few-shot examples incluem edge cases?

## 7. Padr√µes Avan√ßados

### 7.1 Multi-Aspect Evaluation

**Cen√°rio**: Avaliar m√∫ltiplas dimens√µes separadamente

```python
# Judge 1: Corre√ß√£o
correctness_judge = create_llm_as_judge(criteria="CORRECTNESS", ...)

# Judge 2: Concis√£o
conciseness_judge = create_llm_as_judge(criteria="CONCISENESS", ...)

# Judge 3: Tom
tone_judge = create_llm_as_judge(
    criteria="Avalie se o tom √© profissional e amig√°vel",
    ...
)

# Combinar
results = evaluate(
    data="dataset",
    evaluators=[correctness_judge, conciseness_judge, tone_judge]
)

# Resultado: 3 scores por example
```

### 7.2 Pairwise Comparison

**Cen√°rio**: Comparar 2 respostas (A vs B)

```python
pairwise_prompt = """
Voc√™ √© um avaliador comparativo.

Pergunta: {pergunta}

Resposta A: {resposta_a}
Resposta B: {resposta_b}

Qual resposta √© melhor? Considere:
- Corre√ß√£o factual
- Completude
- Clareza

Retorne JSON:
{{
  "winner": "A" ou "B",
  "score_a": 0-1,
  "score_b": 0-1,
  "comment": "Justificativa da escolha"
}}
"""

pairwise_judge = create_llm_as_judge(
    prompt=pairwise_prompt,
    input_keys=["pergunta", "resposta_a", "resposta_b"],
    prediction_key="comparison"  # Dummy (n√£o usado)
)
```

### 7.3 Adaptive Rubric

**Cen√°rio**: Rubrica muda baseado no tipo de pergunta

```python
adaptive_prompt = """
Voc√™ √© avaliador adaptativo.

Tipo de Pergunta: {question_type}

Se {question_type} == "factual":
    Rubrica: Corre√ß√£o bin√°ria (0 ou 1)
Sen√£o se {question_type} == "opini√£o":
    Rubrica: Fundamenta√ß√£o (argumentos v√°lidos = 1)
Sen√£o:
    Rubrica: Relev√¢ncia (alinhamento com pergunta)

{dados para avaliar}

Retorne JSON: {{"score": ..., "comment": ...}}
"""
```

## 8. Best Practices Resumidas

### Checklist de Design de Prompt

**Estrutura:**
- [ ] Role claro (especialista em X)?
- [ ] Dados mapeados corretamente (input, reference, prediction)?
- [ ] Formato de sa√≠da expl√≠cito (JSON com score + comment)?

**Crit√©rios:**
- [ ] Crit√©rio bem definido (n√£o vago)?
- [ ] Escala clara (0/1 ou 1-5 com descri√ß√µes)?
- [ ] Rubrica expl√≠cita (quando usar escala num√©rica)?

**Calibra√ß√£o:**
- [ ] Few-shot examples inclu√≠dos (3-5)?
- [ ] Edge cases cobertos?
- [ ] Distribui√ß√£o balanceada (positivos + negativos)?

**Bias:**
- [ ] Instru√ß√£o para ser cr√≠tico?
- [ ] Exemplos de respostas longas mas incorretas?
- [ ] Human corrections planejadas?

**Performance:**
- [ ] Modelo apropriado (gpt-4o-mini para itera√ß√£o)?
- [ ] Prompt conciso (< 1000 tokens)?
- [ ] Chain-of-thought apenas quando necess√°rio?

## 9. Troubleshooting de Prompts

**Problema**: Judge sempre d√° score m√°ximo
```python
# Diagn√≥stico: Positivity bias
# Solu√ß√£o: Adicionar few-shot com casos negativos + instru√ß√£o cr√≠tica
```

**Problema**: Scores inconsistentes
```python
# Diagn√≥stico: Rubrica vaga ou modelo fraco
# Solu√ß√£o: Definir rubrica detalhada OU usar modelo melhor (gpt-4o)
```

**Problema**: Judge n√£o usa reference output
```python
# Diagn√≥stico: Prompt n√£o menciona reference explicitamente
# Solu√ß√£o: Adicionar "Compare Prediction vs Reference Output"
```

**Problema**: JSON inv√°lido retornado
```python
# Diagn√≥stico: Modelo n√£o segue formato
# Solu√ß√£o: Enfatizar "Retorne APENAS JSON v√°lido, sem texto adicional"
```

**Problema**: Coment√°rios vazios ou gen√©ricos
```python
# Diagn√≥stico: Prompt n√£o exige justificativa espec√≠fica
# Solu√ß√£o: "comment deve mencionar QUAL fato est√° incorreto e POR QU√ä"
```

## 10. Exemplos Completos

### Exemplo 1: Judge B√°sico (Zero-Shot)
```python
from langsmith.evaluation import create_llm_as_judge

basic_judge = create_llm_as_judge(
    criteria="CORRECTNESS",
    model="openai:gpt-4o-mini",
    input_keys=["question"],
    reference_output_keys=["answer"],
    prediction_key="response"
)
```

### Exemplo 2: Judge Customizado (Rubrica Detalhada)
```python
rubric = """
Avalie resposta t√©cnica em escala 1-5:
5 - Completa e precisa
4 - Pequenas omiss√µes
3 - Parcialmente correta
2 - Erros significativos
1 - Completamente incorreta

Retorne JSON: {{"score": 1-5, "comment": "..."}}
"""

custom_judge = create_llm_as_judge(
    criteria=rubric,
    model="openai:gpt-4o",
    input_keys=["question"],
    reference_output_keys=["expected_answer"],
    prediction_key="generated_answer"
)
```

### Exemplo 3: Judge com Few-Shot e CoT
```python
few_shot_cot = """
Voc√™ √© avaliador com racioc√≠nio expl√≠cito.

EXEMPLOS:
[3 exemplos de julgamento com reasoning]

PROCESSO:
1. Analisar fatos
2. Comparar com refer√™ncia
3. Decidir score

AGORA AVALIE:
Q: {question}
Ref: {reference}
Pred: {prediction}

Retorne JSON:
{{
  "reasoning": "Passo 1: ... Passo 2: ... Passo 3: ...",
  "score": 0 ou 1,
  "comment": "Decis√£o final"
}}
"""

advanced_judge = create_llm_as_judge(
    prompt=few_shot_cot,
    model="openai:gpt-4o",
    input_keys=["question"],
    reference_output_keys=["reference"],
    prediction_key="prediction"
)
```
