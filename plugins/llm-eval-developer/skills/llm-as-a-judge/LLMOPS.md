# LLMOps Workflow for LLM-as-Judge

Guia completo de integraÃ§Ã£o do LLM-as-Judge no ciclo de vida de desenvolvimento (LLMOps), cobrindo avaliaÃ§Ã£o offline/online, experiments, debugging, mÃ©tricas combinadas e decisÃµes de deployment.

## ğŸ”„ Ciclo de Vida LLMOps com LLM-as-Judge

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          LLMOps Development Cycle                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  1. DEVELOP                                             â”‚
â”‚     â”œâ”€â”€ Iterar prompts/chains                           â”‚
â”‚     â””â”€â”€ Quick Evals (subset dataset, < 5min)            â”‚
â”‚              â†“                                          â”‚
â”‚  2. EVALUATE (Offline - End of Sprint)                  â”‚
â”‚     â”œâ”€â”€ Full dataset evaluation                         â”‚
â”‚     â”œâ”€â”€ LLM-as-Judge + metrics (latency, cost)          â”‚
â”‚     â””â”€â”€ Create Experiment in LangSmith                  â”‚
â”‚              â†“                                          â”‚
â”‚  3. COMPARE (A/B Testing)                               â”‚
â”‚     â”œâ”€â”€ Compare Experiments (v1 vs v2)                  â”‚
â”‚     â”œâ”€â”€ Heatmap analysis                                â”‚
â”‚     â””â”€â”€ Decide: deploy, iterate, or rollback            â”‚
â”‚              â†“                                          â”‚
â”‚  4. DEBUG (If Needed)                                   â”‚
â”‚     â”œâ”€â”€ Traces: why low score?                          â”‚
â”‚     â”œâ”€â”€ Fix prompt/logic                                â”‚
â”‚     â””â”€â”€ Re-evaluate (back to step 2)                    â”‚
â”‚              â†“                                          â”‚
â”‚  5. DEPLOY                                              â”‚
â”‚     â”œâ”€â”€ Promote to production                           â”‚
â”‚     â””â”€â”€ Enable online evaluators                        â”‚
â”‚              â†“                                          â”‚
â”‚  6. MONITOR (Continuous)                                â”‚
â”‚     â”œâ”€â”€ Online LLM-as-Judge in real-time                â”‚
â”‚     â”œâ”€â”€ Alerts on quality degradation                   â”‚
â”‚     â””â”€â”€ Feedback loop (back to step 1)                  â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 1. AvaliaÃ§Ã£o Offline (End of Sprint)

### 1.1 O Que Ã© AvaliaÃ§Ã£o Offline

**DefiniÃ§Ã£o**: AvaliaÃ§Ã£o executada em dataset estÃ¡tico **antes** de deploy em produÃ§Ã£o.

**CaracterÃ­sticas:**

- âœ… Dataset versionado e imutÃ¡vel
- âœ… ExecuÃ§Ã£o batch (nÃ£o real-time)
- âœ… Resultados armazenados em Experiment
- âœ… ComparÃ¡vel entre versÃµes

**Quando executar:**

- ğŸ”„ Final de cada sprint/iteraÃ§Ã£o
- ğŸ”„ Antes de merge de PR
- ğŸ”„ Antes de deploy em staging
- ğŸ”„ Quando mudanÃ§a significativa no prompt/modelo

### 1.2 Setup de AvaliaÃ§Ã£o Formal

**Workflow recomendado:**

```python
from langsmith import Client
from langsmith.evaluation import evaluate, create_llm_as_judge

# 1. Setup
client = Client()

# 2. Criar/usar dataset estÃ¡vel
dataset = client.read_dataset(dataset_name="golden-qa-eval")

# 3. Definir evaluators
judge = create_llm_as_judge(
    criteria="CORRECTNESS",
    model="openai:gpt-4o-mini",
    input_keys=["question"],
    reference_output_keys=["expected_answer"],
    prediction_key="answer"
)

# Evaluators adicionais (latÃªncia, custo)
from langsmith.evaluation import metrics

latency_evaluator = metrics.latency()
cost_evaluator = metrics.cost()

# 4. Executar avaliaÃ§Ã£o formal
results = evaluate(
    target_function=my_qa_app,
    data="golden-qa-eval",
    evaluators=[judge, latency_evaluator, cost_evaluator],
    experiment_prefix="sprint-5-final",
    max_concurrency=10
)

# 5. Revisar resultados
print(f"Quality Score: {results.aggregate_score}")
print(f"Experiment URL: {results.experiment_url}")
```

### 1.3 MÃ©tricas a Capturar

**Qualidade (LLM-as-Judge):**

- Score mÃ©dio (0-1)
- DistribuiÃ§Ã£o de scores
- Taxa de falha (score = 0)

**Performance:**

- LatÃªncia mÃ©dia (segundos)
- P95 latency
- Timeout rate

**Custo:**

- Custo por query (USD)
- Custo total do experiment
- Token usage (input + output)

**Exemplo de log:**

```python
# Resultados do experiment
{
    "quality": {
        "mean_score": 0.85,
        "score_distribution": {"0": 5, "1": 35},  # 5 falhas, 35 sucessos
        "failure_rate": 0.125  # 12.5%
    },
    "performance": {
        "mean_latency_ms": 450,
        "p95_latency_ms": 780,
        "timeout_rate": 0.02
    },
    "cost": {
        "total_usd": 0.42,
        "per_query_usd": 0.0105,
        "total_tokens": 15000
    }
}
```

### 1.4 Quick Evals vs Full Evals

**Quick Evals** (IteraÃ§Ã£o RÃ¡pida):

- âœ… Subset pequeno (20-50 examples)
- âœ… Executado frequentemente (a cada mudanÃ§a)
- âœ… Tempo: < 5 minutos
- âœ… Custo: < $0.10

```python
# Quick eval setup
quick_eval = evaluate(
    data="golden-qa-eval",
    evaluators=[judge],
    max_concurrency=10,
    limit=20  # Apenas 20 examples!
)
```

**Full Evals** (ValidaÃ§Ã£o Formal):

- âœ… Dataset completo (100-1000s examples)
- âœ… Executado no final do sprint
- âœ… Tempo: 10-60 minutos
- âœ… Custo: $1-10

```python
# Full eval setup
full_eval = evaluate(
    data="golden-qa-eval",  # Todos examples
    evaluators=[judge, latency_evaluator, cost_evaluator],
    max_concurrency=10
)
```

**EstratÃ©gia recomendada:**

```
Durante desenvolvimento:
â”œâ”€â”€ Quick evals (frequentes)
â””â”€â”€ Feedback rÃ¡pido (< 5min)

Final do sprint:
â”œâ”€â”€ Full eval (validaÃ§Ã£o)
â””â”€â”€ DecisÃ£o de deploy
```

## 2. Experiments - ComparaÃ§Ã£o A/B

### 2.1 O Que Ã© um Experiment

**DefiniÃ§Ã£o**: Registro de uma execuÃ§Ã£o de avaliaÃ§Ã£o, incluindo dataset, evaluators, target function e resultados.

**Metadados armazenados:**

- Dataset name + version
- Evaluator configs
- Timestamp
- Git commit (se configurado)
- Results (scores, latencies, custos)

### 2.2 Criando Experiments ComparÃ¡veis

**Regra de ouro**: Mudar APENAS 1 variÃ¡vel por experiment

**Exemplo: Comparar 2 prompts**

```python
# Experiment 1: Prompt V1
results_v1 = evaluate(
    target_function=app_with_prompt_v1,
    data="qa-eval",
    evaluators=[judge],
    experiment_prefix="prompt-v1"
)

# Experiment 2: Prompt V2 (MESMOS dataset + evaluators!)
results_v2 = evaluate(
    target_function=app_with_prompt_v2,
    data="qa-eval",  # MESMO dataset
    evaluators=[judge],  # MESMO evaluator
    experiment_prefix="prompt-v2"
)

# Comparar no LangSmith UI:
# Navigate to Experiments â†’ Compare (prompt-v1 vs prompt-v2)
```

**VariÃ¡veis que podem ser testadas:**

- Prompt template
- Modelo LLM (gpt-4 vs gpt-3.5)
- Temperatura
- Chain logic
- Retrieval strategy (RAG)

### 2.3 AnÃ¡lise Comparativa

**Heatmap Comparison:**

- Visualizar scores lado a lado
- Identificar examples onde v2 > v1
- Identificar regressions (v2 < v1)

**Score delta:**

```python
# Calcular diferenÃ§a
delta = results_v2.aggregate_score - results_v1.aggregate_score

if delta > 0.05:  # Melhoria significativa
    print("âœ… Deploy v2")
elif delta < -0.05:  # RegressÃ£o
    print("âŒ Rollback, keep v1")
else:  # Neutro
    print("âš ï¸ Avaliar outros fatores (latÃªncia, custo)")
```

**ROI Analysis:**

```python
# Combinar qualidade + custo
roi_v1 = results_v1.aggregate_score / results_v1.total_cost
roi_v2 = results_v2.aggregate_score / results_v2.total_cost

if roi_v2 > roi_v1:
    print("âœ… v2 tem melhor ROI")
```

### 2.4 DecisÃ£o de Deploy

**Framework de decisÃ£o:**

| Score Î” | Latency Î” | Cost Î” | DecisÃ£o |
|---------|-----------|--------|---------|
| +10% | 0% | 0% | âœ… Deploy (win claro) |
| +5% | +20% | +50% | âš ï¸ Avaliar trade-off |
| -5% | -50% | -70% | âš ï¸ Avaliar se performance vale regressÃ£o |
| -10% | 0% | 0% | âŒ NÃ£o deploy (regressÃ£o) |

**Checklist de decisÃ£o:**

- [ ] Quality score melhorou OU manteve?
- [ ] NÃ£o hÃ¡ regressions crÃ­ticas (ex: 0% â†’ 100% failure em categoria especÃ­fica)?
- [ ] LatÃªncia estÃ¡ dentro de SLA (ex: < 1s)?
- [ ] Custo estÃ¡ dentro de budget?
- [ ] Stakeholders aprovaram?

## 3. Debugging com LangSmith UI

### 3.1 Ferramentas de Debug

**Heatmap:**

- VisualizaÃ§Ã£o de scores por example
- Cores: verde (1.0), amarelo (0.5), vermelho (0.0)
- Identificar patterns de falha

**Traces:**

- Detalhamento de cada run
- Input â†’ LLM calls â†’ Output
- Tempo de cada step
- Prompt enviado ao modelo

**Comments:**

- Justificativas do LLM-as-Judge
- Por que score foi X?

### 3.2 Workflow de Debug

**CenÃ¡rio**: Aggregate score = 0.7 (esperava > 0.85)

**Passo 1: Identificar failures no Heatmap**

```
Navigate to: Experiment â†’ Heatmap
Filter: score < 0.5
Result: 10 examples com score baixo
```

**Passo 2: Analisar Traces de failures**

```
Click em example com score = 0
Ver Trace:
- Input: {"question": "..."}
- Target output: {"answer": "..."}
- Judge output: {"score": 0, "comment": "Erro factual: ..."}
```

**Passo 3: Categorizar erros**

```
Erro 1: Prompt ambÃ­guo (5 cases)
Erro 2: Retrieval falhou (3 cases)
Erro 3: Modelo LLM alucinando (2 cases)
```

**Passo 4: Priorizar fixes**

```
Fix 1: Clarificar prompt (impacto: +12% score)
Fix 2: Melhorar retrieval (impacto: +7% score)
Fix 3: Adicionar guardrails anti-alucinaÃ§Ã£o (impacto: +5% score)
```

**Passo 5: Implementar + Re-avaliar**

```python
# Aplicar fix 1
fixed_app = update_prompt(...)

# Re-avaliar
results_fixed = evaluate(
    target_function=fixed_app,
    data="qa-eval",
    evaluators=[judge],
    experiment_prefix="post-fix-1"
)

# Comparar: original (0.70) vs fixed (0.82) âœ…
```

### 3.3 Debugging do Judge Itself

**Problema**: Judge parece errado (ex: dando score alto para resposta claramente incorreta)

**Debug do judge:**

**Passo 1: Ver prompt enviado ao judge**

```
Trace â†’ Judge LLM call â†’ Input (prompt)
```

**Passo 2: Verificar mapeamento de chaves**

```
# Esperado:
{question}: "Capital da FranÃ§a?"
{expected_answer}: "Paris"
{generated_answer}: "Londres"  # âŒ Incorreto

# Se judge deu score = 1, investigar:
# - Prompt nÃ£o menciona "compare com expected_answer"?
# - Judge tem positivity bias?
```

**Passo 3: Testar judge isoladamente**

```python
# Teste unitÃ¡rio do judge
test_input = {"question": "Capital da FranÃ§a?"}
test_reference = {"expected_answer": "Paris"}
test_prediction = {"generated_answer": "Londres"}  # Incorreta

judge_result = judge(
    inputs=test_input,
    outputs=test_reference,
    prediction=test_prediction
)

# Esperado: score = 0
# Se score = 1: judge estÃ¡ quebrado!
```

**Passo 4: Calibrar judge**

- Adicionar few-shot examples
- Revisar prompt para ser mais crÃ­tico
- Usar human corrections no LangSmith

## 4. MÃ©tricas Combinadas (Weighted Score)

### 4.1 Por Que Combinar MÃ©tricas

**Problema**: Score alto de qualidade pode vir com:

- âŒ LatÃªncia inaceitÃ¡vel (5s+ por query)
- âŒ Custo proibitivo ($1+ por query)

**SoluÃ§Ã£o**: Weighted score que balanceia qualidade + performance + custo

### 4.2 FÃ³rmula de Weighted Score

```python
# Normalizar mÃ©tricas (0-1)
quality_norm = quality_score  # JÃ¡ estÃ¡ 0-1
latency_norm = 1 - (latency_ms / max_acceptable_latency_ms)
cost_norm = 1 - (cost_usd / max_acceptable_cost_usd)

# Weighted average
weighted_score = (
    0.6 * quality_norm +      # 60% peso em qualidade
    0.3 * latency_norm +      # 30% peso em performance
    0.1 * cost_norm           # 10% peso em custo
)
```

**Ajustar pesos por prioridade do negÃ³cio:**

| CenÃ¡rio | Quality | Latency | Cost |
|---------|---------|---------|------|
| MÃ©dico/Legal | 0.8 | 0.1 | 0.1 | (Qualidade crÃ­tica) |
| Chatbot | 0.5 | 0.4 | 0.1 | (LatÃªncia importante) |
| AnÃ¡lise batch | 0.7 | 0.1 | 0.2 | (Custo importante) |

### 4.3 ImplementaÃ§Ã£o

```python
def calculate_weighted_score(
    quality: float,
    latency_ms: float,
    cost_usd: float,
    quality_weight: float = 0.6,
    latency_weight: float = 0.3,
    cost_weight: float = 0.1,
    max_latency_ms: float = 1000,
    max_cost_usd: float = 0.05
) -> float:
    """
    Calcular score ponderado combinando qualidade + performance + custo.
    """
    # Normalizar (0-1)
    quality_norm = quality
    latency_norm = max(0, 1 - (latency_ms / max_latency_ms))
    cost_norm = max(0, 1 - (cost_usd / max_cost_usd))

    # Weighted average
    weighted = (
        quality_weight * quality_norm +
        latency_weight * latency_norm +
        cost_weight * cost_norm
    )

    return weighted

# Uso
weighted = calculate_weighted_score(
    quality=0.85,
    latency_ms=450,
    cost_usd=0.01
)
# Resultado: 0.82 (bom!)
```

### 4.4 DecisÃ£o Informada

**CenÃ¡rio: Comparar 2 versÃµes**

```python
# V1: Alta qualidade, lenta, cara
v1_weighted = calculate_weighted_score(
    quality=0.90, latency_ms=1200, cost_usd=0.08
)  # 0.70

# V2: Qualidade razoÃ¡vel, rÃ¡pida, barata
v2_weighted = calculate_weighted_score(
    quality=0.80, latency_ms=300, cost_usd=0.01
)  # 0.85

# DecisÃ£o: V2 wins (melhor balanceamento)
```

## 5. AvaliaÃ§Ã£o Online (ProduÃ§Ã£o)

### 5.1 O Que Ã© AvaliaÃ§Ã£o Online

**DefiniÃ§Ã£o**: AvaliaÃ§Ã£o executada em **tempo real** em produÃ§Ã£o, aplicada a cada request de usuÃ¡rio.

**CaracterÃ­sticas:**

- âœ… Aplica evaluators a production runs
- âœ… Sem reference outputs (geralmente)
- âœ… Detecta degradaÃ§Ã£o de qualidade
- âœ… Permite HITL (Human-in-the-Loop)

**DiferenÃ§a de Offline:**

| Aspecto | Offline | Online |
|---------|---------|--------|
| Quando | PrÃ©-deploy | ProduÃ§Ã£o (real-time) |
| Dataset | EstÃ¡tico | Production runs |
| Reference | DisponÃ­vel | Geralmente nÃ£o |
| LatÃªncia | NÃ£o importa | CrÃ­tica (< 100ms overhead) |

### 5.2 Configurando Online Evaluators

**Setup no LangSmith:**

```python
from langsmith import Client

client = Client()

# 1. Criar online evaluator
client.create_online_evaluator(
    name="production-relevance-judge",
    evaluator=create_llm_as_judge(
        criteria="RELEVANCE",  # NÃ£o requer reference
        model="openai:gpt-4o-mini",
        input_keys=["query"],
        prediction_key="response"
    ),
    project_name="production-chatbot",  # Aplicar a este projeto
    sampling_rate=0.1  # Avaliar 10% dos runs (economizar custo)
)
```

**Sampling rate trade-off:**

- 1.0 (100%): MÃ¡xima cobertura, alto custo
- 0.1 (10%): Amostra representativa, custo controlado
- 0.01 (1%): Monitoramento bÃ¡sico, muito econÃ´mico

### 5.3 Casos de Uso de Online Eval

**1. Quality Monitoring**

- Detectar degradaÃ§Ã£o gradual
- Alertar se score mÃ©dio cai abaixo de threshold

**2. A/B Testing em ProduÃ§Ã£o**

- 50% trÃ¡fego â†’ v1
- 50% trÃ¡fego â†’ v2
- Comparar scores online

**3. Human-in-the-Loop (HITL)**

- LLM-as-Judge avalia automaticamente
- Humanos revisam casos de baixo score
- Feedback humano melhora judge

**4. Safety Guardrails**

- Online judge de harmfulness
- Bloquear resposta se score alto de harmful
- Log para revisÃ£o

### 5.4 Alerting

**Setup de alerts:**

```python
# Pseudo-cÃ³digo (via LangSmith UI ou API)
client.create_alert(
    name="quality-degradation",
    condition="avg_score < 0.7",  # Threshold
    window="1 hour",  # Janela de tempo
    action="send_email",  # email, slack, webhook
    recipients=["team@example.com"]
)
```

**Tipos de alerts:**

- Quality degradation (score mÃ©dio caindo)
- High failure rate (% de score = 0)
- Latency spike (P95 > threshold)
- Cost spike (custo/query > budget)

## 6. IntegraÃ§Ã£o com CI/CD

### 6.1 Eval como Gate de CI/CD

**Workflow:**

```yaml
# .github/workflows/eval.yml
name: LangSmith Evaluation

on:
  pull_request:
    branches: [main]

jobs:
  evaluate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2

      - name: Run LangSmith Eval
        env:
          LANGSMITH_API_KEY: ${{ secrets.LANGSMITH_API_KEY }}
        run: |
          python scripts/run_eval.py

      - name: Check Score Threshold
        run: |
          SCORE=$(cat eval_results.json | jq '.aggregate_score')
          if (( $(echo "$SCORE < 0.80" | bc -l) )); then
            echo "âŒ Score $SCORE below threshold 0.80"
            exit 1
          fi
          echo "âœ… Score $SCORE passes threshold"
```

**BenefÃ­cio**: Bloqueia merge se qualidade regressar

### 6.2 Script de AvaliaÃ§Ã£o

```python
# scripts/run_eval.py
import json
from langsmith import Client
from langsmith.evaluation import evaluate, create_llm_as_judge

def main():
    # Setup
    judge = create_llm_as_judge(...)

    # Executar eval
    results = evaluate(
        target_function=my_app,
        data="ci-dataset",  # Dataset pequeno para CI (fast)
        evaluators=[judge],
        experiment_prefix=f"ci-pr-{os.getenv('PR_NUMBER')}"
    )

    # Salvar resultados
    with open("eval_results.json", "w") as f:
        json.dump({
            "aggregate_score": results.aggregate_score,
            "experiment_url": results.experiment_url
        }, f)

    # Exit code baseado em threshold
    if results.aggregate_score < 0.80:
        print(f"âŒ Failed: {results.aggregate_score}")
        exit(1)
    else:
        print(f"âœ… Passed: {results.aggregate_score}")
        exit(0)

if __name__ == "__main__":
    main()
```

### 6.3 Regression Testing

**Pattern**: Rodar eval antes de cada release

```python
# Pre-release checklist
def pre_release_eval():
    # 1. Full eval em dataset golden
    results = evaluate(data="golden-dataset", ...)

    # 2. Comparar com baseline (Ãºltima release)
    baseline_score = get_last_release_score()
    current_score = results.aggregate_score

    # 3. Bloquear se regressÃ£o
    if current_score < baseline_score - 0.05:  # 5% tolerance
        raise Exception("Quality regression detected!")

    # 4. Atualizar baseline
    update_baseline(current_score)
```

## 7. Best Practices LLMOps

### 7.1 Checklist de IntegraÃ§Ã£o

**Durante Desenvolvimento:**

- [ ] Quick evals configurados (< 5min)?
- [ ] Feedback rÃ¡pido a cada mudanÃ§a?

**Final de Sprint:**

- [ ] Full eval executado em dataset golden?
- [ ] Experiment criado com nome descritivo?
- [ ] MÃ©tricas combinadas calculadas (quality + latency + cost)?
- [ ] ComparaÃ§Ã£o A/B com versÃ£o anterior?

**Antes de Deploy:**

- [ ] Score acima de threshold (ex: > 0.80)?
- [ ] Sem regressions crÃ­ticas?
- [ ] Stakeholders aprovaram?
- [ ] LatÃªncia e custo aceitÃ¡veis?

**PÃ³s-Deploy:**

- [ ] Online evaluators habilitados?
- [ ] Alerts configurados (quality, latency, cost)?
- [ ] Sampling rate apropriado (10-20%)?
- [ ] HITL planejado para casos de baixo score?

### 7.2 Versionamento de Artifacts

**O que versionar:**

- âœ… Datasets (automÃ¡tico no LangSmith)
- âœ… Prompt templates (Git)
- âœ… Evaluator configs (Git)
- âœ… Experiment results (LangSmith)

**Exemplo de estrutura Git:**

```
repo/
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ v1.txt
â”‚   â””â”€â”€ v2.txt
â”œâ”€â”€ evaluators/
â”‚   â””â”€â”€ correctness_judge.py
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ golden_qa_v3.json  # Backup local
â””â”€â”€ experiments/
    â””â”€â”€ sprint_5_results.json
```

### 7.3 Evitando Overfitting ao Dataset

**Problema**: Otimizar demais para dataset golden â†’ nÃ£o generaliza em produÃ§Ã£o

**SoluÃ§Ãµes:**

**1. MÃºltiplos datasets:**

```python
# Dataset 1: Curado (golden)
eval_golden = evaluate(data="golden-dataset", ...)

# Dataset 2: Production sample
eval_prod = evaluate(data="production-sample", ...)

# Ambos devem passar threshold!
```

**2. Holdout set:**

```python
# 80% training/dev (iteraÃ§Ã£o)
# 20% holdout (validaÃ§Ã£o final)

# Iterar em dev set
dev_results = evaluate(data="dev-set", ...)

# Validar em holdout (apenas 1x antes de deploy!)
holdout_results = evaluate(data="holdout-set", ...)
```

**3. Refresh periÃ³dico:**

```
A cada 3-6 meses:
â”œâ”€â”€ Adicionar novos examples (edge cases de produÃ§Ã£o)
â”œâ”€â”€ Remover examples obsoletos
â””â”€â”€ Re-validar todos prompts
```

## 8. Troubleshooting LLMOps

**Problema**: Eval passa em CI mas falha em produÃ§Ã£o

- âœ… CI dataset muito fÃ¡cil (nÃ£o representa produÃ§Ã£o)
- âœ… SoluÃ§Ã£o: Usar production sample como dataset CI

**Problema**: Online evals aumentam latÃªncia

- âœ… Sampling rate muito alto (100%)
- âœ… SoluÃ§Ã£o: Reduzir para 10-20% ou async evaluation

**Problema**: Alerts falsos (muito ruÃ­do)

- âœ… Threshold muito sensÃ­vel
- âœ… SoluÃ§Ã£o: Aumentar janela de tempo (1h â†’ 4h)

**Problema**: Experiments nÃ£o comparÃ¡veis

- âœ… Dataset mudou entre experiments
- âœ… SoluÃ§Ã£o: Pin dataset version nos experiments

## 9. MÃ©tricas de Sucesso LLMOps

**Velocidade de IteraÃ§Ã£o:**

- Tempo de feedback: < 5min (quick eval)
- Frequency de evals: diÃ¡ria (durante dev)

**Qualidade:**

- Aggregate score: > 0.80 (threshold)
- Regression rate: < 5% (comparado a baseline)

**EficiÃªncia:**

- Custo de eval: < 10% do custo de produÃ§Ã£o
- Tempo de debug: < 2h (de identificaÃ§Ã£o a fix)

**Confiabilidade:**

- False positive rate (bad deploy): < 2%
- Alert noise: < 10% (alertas falsos)

## 10. Roadmap de AdoÃ§Ã£o

**Semana 1-2: Setup BÃ¡sico**

- [ ] Criar dataset golden (50 examples)
- [ ] Implementar primeiro LLM-as-Judge
- [ ] Executar eval manual

**Semana 3-4: AutomaÃ§Ã£o**

- [ ] Integrar eval em CI/CD
- [ ] Configurar quick evals para dev
- [ ] Estabelecer threshold de qualidade

**Semana 5-6: Refinamento**

- [ ] Adicionar mÃ©tricas combinadas (weighted score)
- [ ] Implementar A/B testing workflow
- [ ] Calibrar judge com few-shot/human feedback

**Semana 7-8: ProduÃ§Ã£o**

- [ ] Habilitar online evaluators
- [ ] Configurar alerts de qualidade
- [ ] HITL para casos de baixo score

**MÃªs 3+: OtimizaÃ§Ã£o**

- [ ] Expandir coverage de dataset (100+ examples)
- [ ] MÃºltiplos judges (multi-aspect eval)
- [ ] Regression testing automatizado
