---
name: evaluation-developer
description: Desenvolve c√≥digo de evaluators para LLMs (OpenEvals, LangSmith, BLEU, ROUGE, LLM-as-judge). Use quando criar evaluators, m√©tricas customizadas, evaluation suites, ou testar LLMs.
allowed-tools: Read, Write, Bash, Grep, Glob
---

# Evaluation Developer Skill

## Instructions

Skill especializada em **desenvolver c√≥digo de evaluation** para LLMs e agentes. Foco em **criar**, n√£o executar.

### Step 1: Identify Evaluation Need

Quando usu√°rio menciona termos como:
- "criar evaluator", "desenvolver evaluation"
- "m√©tricas para LLM", "testar meu chatbot"
- "hallucination detection", "relevance evaluation"
- "BLEU score", "ROUGE", "LLM-as-judge"
- "evaluation suite", "evaluation dataset"
- "como avaliar meu LLM"

### Step 2: Determine Type of Evaluation

Identificar tipo de evaluator necess√°rio:

**LLM-as-Judge**:
- Para crit√©rios subjetivos (relevance, tone, coherence)
- Quando regras s√£o complexas demais para codificar
- Exemplo: Detectar alucina√ß√µes, avaliar qualidade

**Similarity-based**:
- Para comparar com ground truth
- BLEU (translation), ROUGE (summarization), embeddings
- Exemplo: Avaliar precis√£o de summary

**Rule-based**:
- Para valida√ß√µes exatas
- Regex, exact match, custom logic
- Exemplo: Validar formato de email

**Composite**:
- Combina m√∫ltiplas m√©tricas
- Quando precisa avaliar v√°rios aspectos
- Exemplo: Accuracy + Relevance + Response Time

### Step 3: Generate Evaluator Code

Gerar c√≥digo completo incluindo:

1. **Imports necess√°rios**
2. **Fun√ß√£o do evaluator** com:
   - Docstring clara
   - Type hints
   - Error handling
   - Retorno padronizado
3. **Testes unit√°rios** com pytest
4. **Exemplo de uso**
5. **Explica√ß√£o de quando usar**

#### Template para OpenEvals LLM-as-Judge:

```python
from openevals.llm import create_llm_as_judge

CUSTOM_PROMPT = """
[Instru√ß√µes claras do que avaliar]

<context_or_inputs>
{inputs}
</context_or_inputs>

<output_to_evaluate>
{outputs}
</output_to_evaluate>

[Crit√©rios de avalia√ß√£o]
[Escala de score]

Retorne apenas o score num√©rico.
"""

evaluator_name = create_llm_as_judge(
    prompt=CUSTOM_PROMPT,
    model="openai:gpt-4o-mini",
)
```

#### Template para LangSmith Custom Evaluator:

```python
from langsmith.evaluation import evaluator

@evaluator
def custom_evaluator(outputs: dict, inputs: dict = None, reference_outputs: dict = None) -> dict:
    """
    [Descri√ß√£o do que avalia]

    Args:
        outputs: [Explicar estrutura esperada]
        inputs: [Explicar se usado]
        reference_outputs: [Explicar se usado]

    Returns:
        dict: Score (0-1) e coment√°rio
    """
    # Implementa√ß√£o

    return {
        "score": score,
        "comment": "explica√ß√£o"
    }
```

#### Template para Similarity Metric:

```python
from [library] import [metric]

def metric_evaluator(outputs: dict, reference_outputs: dict) -> dict:
    """
    Calcula [m√©trica] entre output e refer√™ncia.

    [Explica√ß√£o da m√©trica]
    """
    output = outputs.get("field", "")
    reference = reference_outputs.get("field", "")

    # Calcular m√©trica
    score = calculate_metric(output, reference)

    return {
        "metric_name": score,
        "comment": f"Score: {score:.3f}"
    }
```

### Step 4: Include Testing Patterns

SEMPRE incluir testes unit√°rios:

```python
import pytest

def test_evaluator_positive_case():
    """Testa caso que deve ter score alto."""
    result = evaluator(
        outputs={"answer": "good answer"},
        inputs={"question": "question"}
    )
    assert result["score"] >= 0.8

def test_evaluator_negative_case():
    """Testa caso que deve ter score baixo."""
    result = evaluator(
        outputs={"answer": "bad answer"},
        inputs={"question": "question"}
    )
    assert result["score"] <= 0.3

@pytest.mark.parametrize("input,expected_min", [
    ("test1", 0.8),
    ("test2", 0.5),
])
def test_evaluator_various(input, expected_min):
    """Testa m√∫ltiplos casos."""
    result = evaluator(outputs={"answer": input})
    assert result["score"] >= expected_min
```

### Step 5: Explain Trade-offs

Para cada evaluator, explicar:

**Quando usar**:
- ‚úÖ Cen√°rio 1
- ‚úÖ Cen√°rio 2

**Quando N√ÉO usar**:
- ‚ùå Limita√ß√£o 1
- ‚ùå Limita√ß√£o 2

**Trade-offs**:
- Custo vs Precis√£o
- Velocidade vs Qualidade
- Determinismo vs Flexibilidade

### Step 6: Provide Integration Code

Mostrar como integrar em evaluation pipeline:

```python
from langsmith import evaluate

results = evaluate(
    your_app,
    data="dataset-name",
    evaluators=[evaluator1, evaluator2],
    experiment_prefix="experiment-name"
)
```

## When to Use

Esta skill √© invocada automaticamente quando:

- Usu√°rio quer **criar** evaluator customizado
- Precisa implementar m√©tricas (BLEU, ROUGE, custom)
- Quer scaffolding de evaluation suite
- Pergunta sobre **como avaliar** LLM
- Menciona OpenEvals, LangSmith, evaluation
- Precisa de **c√≥digo** de evaluation (n√£o execu√ß√£o)

**Trigger terms**:
- "criar evaluator", "desenvolver evaluation"
- "como avaliar", "m√©tricas para"
- "hallucination detection", "relevance scoring"
- "BLEU", "ROUGE", "LLM-as-judge"
- "evaluation suite", "dataset de evaluation"
- "testar meu LLM", "avaliar chatbot"

## Examples

### Example 1: Create Hallucination Detector

**User says**: "Preciso detectar alucina√ß√µes no meu RAG system"

**Skill generates**:

```python
"""
Hallucination Detector para RAG Systems
Usa LLM-as-judge para verificar se output √© suportado pelo contexto.
"""

from langsmith.evaluation import evaluator
from openai import OpenAI

@evaluator
def hallucination_detector(outputs: dict, inputs: dict) -> dict:
    """
    Detecta alucina√ß√µes comparando resposta com contexto.

    Args:
        outputs: Dict com "answer" - resposta do RAG
        inputs: Dict com "context" - contexto usado pelo RAG

    Returns:
        dict: Score 0-1 (1=factual, 0=alucinado)
    """
    answer = outputs.get("answer", "")
    context = inputs.get("context", "")

    prompt = f"""
Verifique se a RESPOSTA cont√©m APENAS informa√ß√µes do CONTEXTO.

CONTEXTO:
{context}

RESPOSTA:
{answer}

Retorne JSON:
{{
  "is_factual": true/false,
  "score": 0.0-1.0,
  "reason": "explica√ß√£o"
}}
"""

    client = OpenAI()
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"},
        temperature=0.0
    )

    result = eval(response.choices[0].message.content)

    return {
        "score": result["score"],
        "comment": result["reason"]
    }


# === TESTES ===

def test_hallucination_detector_factual():
    """Deve dar score alto para resposta factual."""
    result = hallucination_detector(
        outputs={"answer": "Paris is the capital."},
        inputs={"context": "Paris is France's capital."}
    )
    assert result["score"] >= 0.8

def test_hallucination_detector_hallucinated():
    """Deve dar score baixo para alucina√ß√£o."""
    result = hallucination_detector(
        outputs={"answer": "London is the capital."},
        inputs={"context": "Paris is France's capital."}
    )
    assert result["score"] <= 0.3


# === USO ===

from langsmith import evaluate

results = evaluate(
    your_rag_app,
    data="rag-dataset",
    evaluators=[hallucination_detector]
)
```

**E explica**:
‚úÖ Use para: RAG systems, Q&A factual
‚úÖ Vantagens: Detecta alucina√ß√µes sem√¢nticas
‚ùå Limita√ß√µes: Custo de API, lat√™ncia
üí° Trade-off: Precis√£o alta vs custo

---

### Example 2: Implement ROUGE Score

**User says**: "Como implemento ROUGE para meu summarizer?"

**Skill generates**:

```python
"""
ROUGE Score Evaluator
M√©trica padr√£o para avaliar summarization.
"""

from rouge import Rouge
from typing import Dict

def rouge_evaluator(outputs: Dict, reference_outputs: Dict) -> Dict:
    """
    Calcula ROUGE scores para summarization.

    ROUGE mede overlap de n-gramas entre summaries.

    Args:
        outputs: {"summary": "generated summary"}
        reference_outputs: {"summary": "reference summary"}

    Returns:
        dict: ROUGE-1, ROUGE-2, ROUGE-L F1 scores
    """
    generated = outputs.get("summary", "")
    reference = reference_outputs.get("summary", "")

    if not generated or not reference:
        return {
            "rouge_1_f1": 0.0,
            "rouge_2_f1": 0.0,
            "rouge_l_f1": 0.0
        }

    rouge = Rouge()
    scores = rouge.get_scores(generated, reference)[0]

    return {
        "rouge_1_f1": scores["rouge-1"]["f"],
        "rouge_2_f1": scores["rouge-2"]["f"],
        "rouge_l_f1": scores["rouge-l"]["f"],
    }


# === TESTES ===

def test_rouge_perfect_match():
    """Score perfeito para match exato."""
    result = rouge_evaluator(
        outputs={"summary": "Paris is capital."},
        reference_outputs={"summary": "Paris is capital."}
    )
    assert result["rouge_1_f1"] == 1.0

def test_rouge_partial():
    """Score parcial para overlap parcial."""
    result = rouge_evaluator(
        outputs={"summary": "Paris is beautiful."},
        reference_outputs={"summary": "Paris is capital."}
    )
    assert 0.3 < result["rouge_1_f1"] < 0.8


# === QUANDO USAR ===

"""
‚úÖ Use ROUGE para:
  - Summarization tasks
  - Comparar com ground truth
  - Avaliar overlap de conte√∫do

‚ùå N√ÉO use para:
  - Par√°frase (palavras diferentes, mesmo sentido)
  - Avaliar coer√™ncia/flu√™ncia
  - Quando n√£o h√° refer√™ncia exata

üí° Trade-offs:
  - R√°pido e determin√≠stico
  - Mas ignora sem√¢ntica
  - Combine com LLM-as-judge para melhor avalia√ß√£o
"""
```

---

### Example 3: Create Evaluation Suite

**User says**: "Preciso de uma evaluation suite completa para meu chatbot"

**Skill generates**:

```
evaluations/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ eval_config.py           # Thresholds e configura√ß√£o
‚îú‚îÄ‚îÄ datasets/
‚îÇ   ‚îú‚îÄ‚îÄ golden_dataset.json      # Dataset anotado
‚îÇ   ‚îî‚îÄ‚îÄ dataset_generator.py     # Gerador sint√©tico
‚îú‚îÄ‚îÄ evaluators/
‚îÇ   ‚îú‚îÄ‚îÄ accuracy_evaluator.py
‚îÇ   ‚îú‚îÄ‚îÄ relevance_evaluator.py
‚îÇ   ‚îî‚îÄ‚îÄ response_time_evaluator.py
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_evaluators.py       # Testes unit√°rios
‚îÇ   ‚îî‚îÄ‚îÄ test_integration.py      # Testes end-to-end
‚îú‚îÄ‚îÄ run_evaluation.py             # Script principal
‚îî‚îÄ‚îÄ README.md
```

**E gera c√≥digo funcional para cada arquivo**, incluindo:
- Evaluators implementados
- Dataset de exemplo
- Testes completos
- Script de execu√ß√£o
- Documenta√ß√£o

## Key Patterns

### Pattern 1: Always Include Tests

Todo evaluator DEVE ter testes unit√°rios:
- Casos positivos (score alto esperado)
- Casos negativos (score baixo esperado)
- Edge cases (empty, null, etc.)
- Parametrized tests para m√∫ltiplos casos

### Pattern 2: Explain Trade-offs

Sempre explicar:
- ‚úÖ Quando usar
- ‚ùå Limita√ß√µes
- üí° Trade-offs (custo, lat√™ncia, precis√£o)

### Pattern 3: Provide Integration Code

Mostrar como usar em:
- LangSmith evaluate()
- OpenEvals evaluation pipeline
- Pytest integration
- CI/CD (GitHub Actions)

### Pattern 4: Mock for Testing

Ensinar patterns de mock para:
- Evitar custos de API em testes
- Testes determin√≠sticos
- Testes offline

### Pattern 5: Documentation

Incluir:
- Docstrings completas
- Type hints
- Exemplos de uso
- Refer√™ncias (papers, docs)

## References

- OpenEvals: https://github.com/langchain-ai/openevals
- LangSmith Evaluation: https://docs.smith.langchain.com/evaluation
- BLEU/ROUGE: https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation
- LLM-as-Judge: https://www.evidentlyai.com/llm-guide/llm-as-a-judge
- Pytest: https://docs.pytest.org/